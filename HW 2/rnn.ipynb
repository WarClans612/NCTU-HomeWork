{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "HIDDEN_SIZE = 32\n",
    "EMBEDDING_LEN = 300\n",
    "EPOCH = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   name  label\n",
      "0     Unsupervised Representation Learning by Predic...      1\n",
      "1     Emergent Communication in a Multi-Modal, Multi...      1\n",
      "2     FastGCN: Fast Learning with Graph Convolutiona...      1\n",
      "3     Emergent Translation in Multi-Agent Communication      1\n",
      "4     An efficient framework for learning sentence r...      1\n",
      "5     NerveNet: Learning Structured Policy with Grap...      1\n",
      "6     Learning Latent Representations in Neural Netw...      1\n",
      "7                    Adversarial Dropout Regularization      1\n",
      "8                                 Demystifying MMD GANs      1\n",
      "9     Smooth Loss Functions for Deep Top-k Classific...      1\n",
      "10    Deep Learning as a Mixed Convex-Combinatorial ...      1\n",
      "11    Learning Approximate Inference Networks for St...      1\n",
      "12    LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYIN...      1\n",
      "13    Model compression via distillation and quantiz...      1\n",
      "14    Variational Message Passing with Structured In...      1\n",
      "15    Action-dependent Control Variates for Policy O...      1\n",
      "16    Variational image compression with a scale hyp...      1\n",
      "17    Variational Inference of Disentangled Latent C...      1\n",
      "18    Flipout: Efficient Pseudo-Independent Weight P...      1\n",
      "19                Kernel Implicit Variational Inference      1\n",
      "20    A Scalable Laplace Approximation for Neural Ne...      1\n",
      "21    The High-Dimensional Geometry of Binary Neural...      1\n",
      "22    Apprentice: Using Knowledge Distillation Techn...      1\n",
      "23            Distributed Prioritized Experience Replay      1\n",
      "24    Learning from Between-class Examples for Deep ...      1\n",
      "25    Training Confidence-calibrated Classifiers for...      1\n",
      "26    VoiceLoop: Voice Fitting and Synthesis via a P...      1\n",
      "27    Large scale distributed neural network trainin...      1\n",
      "28    Learning Differentially Private Recurrent Lang...      1\n",
      "29    Mastering the Dungeon: Grounded Language Learn...      1\n",
      "...                                                 ...    ...\n",
      "1205  Compact Embedding of Binary-coded Inputs and O...      0\n",
      "1206  A Deep Learning Approach for Joint Video Frame...      0\n",
      "1207  Pedestrian Detection Based On Fast R-CNN and B...      0\n",
      "1208  An Actor-critic Algorithm for Learning Rate Le...      0\n",
      "1209  Training Group Orthogonal Neural Networks with...      0\n",
      "1210  Progressive Attention Networks for Visual Attr...      0\n",
      "1211                  A Neural Knowledge Language Model      0\n",
      "1212  Fuzzy paraphrases in learning word representat...      0\n",
      "1213  What Is the Best Practice for CNNs Applied to ...      0\n",
      "1214         Rectified Factor Networks for Biclustering      0\n",
      "1215                       Sampling Generative Networks      0\n",
      "1216  Learning Locomotion Skills Using DeepRL: Does ...      0\n",
      "1217       Iterative Refinement for Machine Translation      0\n",
      "1218  Semantic Noise Modeling for Better Representat...      0\n",
      "1219  Ternary Weight Decomposition and Binary Activa...      0\n",
      "1220  Vocabulary Selection Strategies for Neural Mac...      0\n",
      "1221  Efficient Communications in Training Large Sca...      0\n",
      "1222                           Dynamic Partition Models      0\n",
      "1223  Learning Efficient Algorithms with Hierarchica...      0\n",
      "1224             Revisiting Distributed Synchronous SGD      0\n",
      "1225  New Learning Approach By Genetic Algorithm In ...      0\n",
      "1226  PREDICTION OF POTENTIAL HUMAN INTENTION USING ...      0\n",
      "1227  Conditional Image Synthesis With Auxiliary Cla...      0\n",
      "1228      Multi-label learning with semantic embeddings      0\n",
      "1229  Significance of Softmax-Based Features over Me...      0\n",
      "1230  Improving Sampling from Generative Autoencoder...      0\n",
      "1231  Learning to Protect Communications with Advers...      0\n",
      "1232  Deep unsupervised learning through spatial con...      0\n",
      "1233  SoftTarget Regularization: An Effective Techni...      0\n",
      "1234    Surprisal-Driven Feedback in Recurrent Networks      0\n",
      "\n",
      "[1235 rows x 2 columns]\n",
      "                                                 name  label\n",
      "0   Minimal-Entropy Correlation Alignment for Unsu...      1\n",
      "1   Large Scale Optimal Transport and Mapping Esti...      1\n",
      "2   TRUNCATED HORIZON POLICY SEARCH: COMBINING REI...      1\n",
      "3     Model-Ensemble Trust-Region Policy Optimization      1\n",
      "4          A Neural Representation of Sketch Drawings      1\n",
      "5           Deep Learning with Logged Bandit Feedback      1\n",
      "6   Learning Latent Permutations with Gumbel-Sinkh...      1\n",
      "7   Learning an Embedding Space for Transferable R...      1\n",
      "8   Unsupervised Learning of Goal Spaces for Intri...      1\n",
      "9   Multi-View Data Generation Without View Superv...      1\n",
      "10  Deep Bayesian Bandits Showdown:  An Empirical ...      1\n",
      "11          Semantic Interpolation in Implicit Models      1\n",
      "12                         Fidelity-Weighted Learning      1\n",
      "13  Latent Space Oddity: on the Curvature of Deep ...      1\n",
      "14  Imitation Learning from Visual Data with Multi...      1\n",
      "15   Hyperparameter optimization: a spectral approach      1\n",
      "16  Leveraging Grammar and Reinforcement Learning ...      1\n",
      "17  Efficient Sparse-Winograd Convolutional Neural...      1\n",
      "18  Espresso: Efficient Forward Propagation for Bi...      1\n",
      "19  Auto-Conditioned Recurrent Networks for Extend...      1\n",
      "20         Decoupling the Layers in Residual Networks      1\n",
      "21                         Polar Transformer Networks      1\n",
      "22  Enhancing The Reliability of Out-of-distributi...      1\n",
      "23  Stabilizing Adversarial Nets with Prediction M...      1\n",
      "24                           Graph Attention Networks      1\n",
      "25  Minimax Curriculum Learning: Machine Teaching ...      1\n",
      "26  Generalizing Hamiltonian Monte Carlo with Neur...      1\n",
      "27  An Online Learning Approach to Generative Adve...      1\n",
      "28             Improving GANs Using Optimal Transport      1\n",
      "29  The Kanerva Machine: A Generative Distributed ...      1\n",
      "..                                                ...    ...\n",
      "70                 Pointing Out SQL Queries From Text      0\n",
      "71    Achieving morphological agreement with Concorde      0\n",
      "72  Generative Models for Alignment and Data Effic...      0\n",
      "73  Estimation of cross-lingual news similarities ...      0\n",
      "74  Jiffy: A Convolutional Approach to Learning Ti...      0\n",
      "75  Optimizing the Latent Space of Generative Netw...      0\n",
      "76  Real-valued (Medical) Time Series Generation w...      0\n",
      "77  Statestream: A toolbox to explore layerwise-pa...      0\n",
      "78  BinaryFlex: On-the-Fly Kernel Generation in Bi...      0\n",
      "79          ResBinNet: Residual Binary Neural Network      0\n",
      "80                                           No Title      0\n",
      "81  Generalization of Learning using Reservoir Com...      0\n",
      "82  Interpreting Deep Classification Models With B...      0\n",
      "83  Evaluation of generative networks through thei...      0\n",
      "84                                           No Title      0\n",
      "85                                           No Title      0\n",
      "86  MACH: Embarrassingly parallel $K$-class classi...      0\n",
      "87        Code Synthesis with Priority Queue Training      0\n",
      "88                Composable Planning with Attributes      0\n",
      "89                        Neural Task Graph Execution      0\n",
      "90                                           No Title      0\n",
      "91  Realtime query completion via deep language mo...      0\n",
      "92         Learning what to learn in a neural program      0\n",
      "93  Discovery of Predictive Representations With a...      0\n",
      "94  Autostacker: an Automatic Evolutionary Hierarc...      0\n",
      "95              Lifelong Learning with Output Kernels      0\n",
      "96  Network Iterative Learning for Dynamic Deep Ne...      0\n",
      "97  Adversarial Learning for Semi-Supervised Seman...      0\n",
      "98        Deep Asymmetric Multi-task Feature Learning      0\n",
      "99              Feature Map Variational Auto-Encoders      0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "accepted = pd.read_excel('ICLR_accepted.xlsx', index_col=[0])\n",
    "rejected = pd.read_excel('ICLR_rejected.xlsx', index_col=[0])\n",
    "accepted.rename({0: 'name'}, axis = 1, inplace=True)\n",
    "rejected.rename({0: 'name'}, axis = 1, inplace=True)\n",
    "accepted['label'] = 1\n",
    "rejected['label'] = 0\n",
    "\n",
    "df_train = pd.concat([accepted[50:], rejected[50:]]).reset_index().drop('index', axis=1)\n",
    "df_test = pd.concat([accepted[:50], rejected[:50]]).reset_index().drop('index', axis=1)\n",
    "\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "\n",
    "df_train.to_csv('ICLR_train.csv', encoding='utf-8')\n",
    "df_test.to_csv('ICLR_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_only(text):\n",
    "    return \"\".join((char if char.isalpha() else \" \") for char in text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 2422\n",
      "Vector size of Text Vocabulary:  torch.Size([2422, 300])\n",
      "Label Length: 2\n",
      "(tensor([[ 351, 2104,  179,   19,    1,    1,    1,    1,    1,    1],\n",
      "        [  11, 1135, 1281,   10,   98, 2026,    8,  130,  185,  867],\n",
      "        [1467,  688,   43,   27,    2, 1491,   39,    3,    1,    1],\n",
      "        [1040,   46,    8,    5,  123,    1,    1,    1,    1,    1],\n",
      "        [1471,   41,   63,    8,  290,  260,   22,   32,   75,    1],\n",
      "        [  47,   35,    7,  555,   23,    5,    4,    1,    1,    1],\n",
      "        [  53,  375,  620,   19,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,   48,   53,   28,    7,   11,    6, 1162,   64,  241],\n",
      "        [2384,  873, 1419,    9,  228,   26,   10, 1908,  716,    1],\n",
      "        [ 896,   34, 1136,   15,   14,    4,    2,  120,  151, 2105],\n",
      "        [ 328,  131,    2,    6,  318,  181,    2,  834, 1718,    1],\n",
      "        [ 747,    5,    4,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  10,  614,  818,  870,    8,   29,   81, 1323, 1009,    9],\n",
      "        [ 553,  493, 1084,  670,  679,    2,  112,   65,    1,    1],\n",
      "        [  17,    3,   40,  400,  922,  192,   21,   10,  686,    1],\n",
      "        [   3,   37,  393,    7,   57,  235,   45,    1,    1,    1],\n",
      "        [ 182,  997, 1944,    8,  869, 1636, 1926,    7,    6,   20],\n",
      "        [  32, 1712, 1885,    7,   86,   42,    4,    1,    1,    1],\n",
      "        [  10,  452,    5,  246,   27,   24,  817,    1,    1,    1],\n",
      "        [1970,  238,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 191,  403,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 877,    9, 2406,  168,  443,    5,    4,  177, 1651, 2040],\n",
      "        [2175,  196,    2,   37,   27, 2364,   21,   76,    1,    1],\n",
      "        [ 366,    9, 1214, 2147,    9,    5,    4,    1,    1,    1],\n",
      "        [  85,    3,   26,  272,    6,   16, 1593,    1,    1,    1],\n",
      "        [  51,   59,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 132,    5,  104,  141,    2,   30,  159,    1,    1,    1],\n",
      "        [ 290, 1166,   13,  518,  101,   18,   36,    2,  371,  231],\n",
      "        [1654,  335,   13, 2180,   10,  633,    1,    1,    1,    1],\n",
      "        [ 156,  242, 2340, 2264,    1,    1,    1,    1,    1,    1],\n",
      "        [   6,  294,   39,    3,    2,  690, 2285,   35,    1,    1],\n",
      "        [  21,  140,  245,    9,  290,    5,  123,    1,    1,    1],\n",
      "        [2393, 1161,   37,  699,   13,  367,  376,    1,    1,    1],\n",
      "        [ 442,  277,    8,   20,    5,    4,    1,    1,    1,    1],\n",
      "        [  18,  386,   93,    4,    2, 1231,   22,   10,  533, 1038],\n",
      "        [1302,   89,  296, 1311,    6,    5,    4,    1,    1,    1],\n",
      "        [  14,   50,    2,   79,   30,   35,  138,    1,    1,    1],\n",
      "        [2310,   48,  195,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 860,   49,  381,    9,   49, 1098,   10,  108,  107,    2],\n",
      "        [2323,  172,    5,    4,    2,    3,   11,  178, 2322,   12],\n",
      "        [ 487,    9,  133,    3,    8,   49,   52,    1,    1,    1],\n",
      "        [ 286,  623,    8,    6,   19,    2,    3,   56,  569,    1],\n",
      "        [  14,   18,  122,    2,   85,   81,   35,    1,    1,    1],\n",
      "        [ 654,   11, 2308,    8,   14,   50,    7,  110, 2015,   33],\n",
      "        [  17,    3,   26, 2039,  833,    8,  358,  725,    2,   11],\n",
      "        [1420, 2418, 1097,   11,  724,  380,    8,   18,   11, 1614],\n",
      "        [   3, 1193,   58,    7,  116,    5,    4,    1,    1,    1],\n",
      "        [2291,  748,   67,    9,  205,   42,    2,  649,  368,  186],\n",
      "        [  57,    9,  152,  823,  559,   12,  109,   17,    3,    1],\n",
      "        [   6, 1028,   43,    2,    5,  431, 1882,    1,    1,    1],\n",
      "        [  16, 2134,   25,   38,   39,    8,  921, 1694, 1213,    3],\n",
      "        [2335,   80,  269,   11,  678,    2,   55,   63,    1,    1],\n",
      "        [1168, 1822,  868,   22,  191,  642,    1,    1,    1,    1],\n",
      "        [   3,   13,  489,    5,  123,    1,    1,    1,    1,    1],\n",
      "        [2293,   11,  387,   95,    7,   20,  327,    4,    1,    1],\n",
      "        [ 120, 2408,   24, 2220,  135,  285,    1,    1,    1,    1],\n",
      "        [ 180,    3,  360,   87,    7, 1526,   83,    1,    1,    1],\n",
      "        [  51,   59,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  10,  172,  215,  527,    9, 1158,    1,    1,    1,    1],\n",
      "        [ 947,  400, 1737,   18,    2,   89,    5,    4,    1,    1],\n",
      "        [ 619,  447,    2,   37,   42,    9,  166,  119,    1,    1],\n",
      "        [ 395, 1755,  184,    2,   20,    5,    4,    1,    1,    1],\n",
      "        [  90,  780,    4,    2,  133,    3,   21, 1837,    1,    1],\n",
      "        [  18,   71,    7, 1847,    1,    1,    1,    1,    1,    1],\n",
      "        [ 635,    5,   16,  115,   22,   17,    3,    1,    1,    1],\n",
      "        [   3,   13, 2316, 2106,    7, 1765,   90,  278,    6,   20],\n",
      "        [   3,  338,   28,    8, 1922, 1194,    1,    1,    1,    1],\n",
      "        [  32,   20,   14,    6,  106,   87,    1,    1,    1,    1],\n",
      "        [   6,   29,    3,   40,  367, 1152,    1,    1,    1,    1],\n",
      "        [ 853,  354,   31,    3,    2,  126,  382,    1,    1,    1],\n",
      "        [  10,   23,  651,   27,    2,    5,   31,   73,    1,    1],\n",
      "        [  66,   11,  261,   18,    8,  191,  449,   10,  617,  372],\n",
      "        [  68,  178,  341,   12,    5,    4,   24,  108,  840,  247],\n",
      "        [ 160, 1151,  881,  361,    4,    1,    1,    1,    1,    1],\n",
      "        [  21,   11,  434,  407,  409,    9, 1393, 1892,   10,  194],\n",
      "        [ 495,   44,   33,   83,    7,  828, 1118,    1,    1,    1],\n",
      "        [  10, 2091,   45,   16,    1,    1,    1,    1,    1,    1],\n",
      "        [ 182, 1425,    2,  898,  595,    7,    5,    4,    9, 2155],\n",
      "        [1520,   10,  172,    5,  104,   13, 2188,  237,   19,    1],\n",
      "        [  15,  251,    8,  225,  476,  327, 1869,    1,    1,    1],\n",
      "        [ 580, 1557,   36,  512,    2,  830,  258,    1,    1,    1],\n",
      "        [  11,  364,    8, 1720,  310,  150,   12,   29,    3,    8],\n",
      "        [  41,   34,  236,  549, 2261,    3,   13, 1361,  167,  519],\n",
      "        [ 690, 1041,  585,   12,   23,    5,    4,    1,    1,    1],\n",
      "        [ 272,    3,   74,   24,  683,  161,   12,   10,   23,    5],\n",
      "        [ 698,    8,    5,   16,  212, 1434,    1,    1,    1,    1],\n",
      "        [  10,  114, 1058, 2289,  583,    2,  102,   52,    1,    1],\n",
      "        [1074, 1746,  435,  884,    2, 1290,   39,    3,    1,    1],\n",
      "        [1630,   51, 2292,    3,   13, 2043,    2,  808,    9,  404],\n",
      "        [   3,  693,   84,    7,   14,  123,    2,  347, 1531,    1],\n",
      "        [ 822,  841,   60,    7, 1123, 2062,    1,    1,    1,    1],\n",
      "        [  89,  835,  673,    4,    1,    1,    1,    1,    1,    1],\n",
      "        [  68,   38,  345,    7,  453,   28,    1,    1,    1,    1],\n",
      "        [2162,  134,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,   13, 2037,  669, 1466,  143, 2038,    2,    6,   17],\n",
      "        [   3,   13,  206,  365,   25,  110,   40, 1564,   18,    1],\n",
      "        [   3,   20, 2173,   28,    2,  664,  111,  127,    1,    1],\n",
      "        [ 292,  155,    2,   15,   14,    4,    1,    1,    1,    1],\n",
      "        [  51,   59,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  14,   78,    3,    1,    1,    1,    1,    1,    1,    1]]), tensor([ 4, 10,  8,  5,  9,  7,  4, 10,  9, 10,  9,  3, 10,  8,  9,  7, 10,  7,\n",
      "         7,  2,  2, 10,  8,  7,  7,  2,  7, 10,  6,  4,  8,  7,  7,  6, 10,  7,\n",
      "         7,  3, 10, 10,  7,  9,  7, 10, 10, 10,  7, 10,  9,  7, 10,  8,  6,  5,\n",
      "         8,  6,  7,  2,  6,  8,  8,  7,  8,  4,  7, 10,  6,  6,  6,  7,  8, 10,\n",
      "        10,  5, 10,  7,  4, 10,  9,  7,  7, 10, 10,  7, 10,  6,  8,  8, 10,  9,\n",
      "         6,  4,  6,  2, 10,  9,  8,  6,  2,  3]))\n"
     ]
    }
   ],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=10)\n",
    "LABEL = data.LabelField()\n",
    "trn_datafields = [(\"id\", None),\n",
    "                    (\"name\", TEXT),\n",
    "                    (\"label\", LABEL)]\n",
    "\n",
    "train_data = TabularDataset(\n",
    "               path=\"./ICLR_train.csv\",\n",
    "               format='csv',\n",
    "               skip_header=True,\n",
    "               fields=trn_datafields)\n",
    "\n",
    "tst_datafields = [(\"id\", None), \n",
    "                 (\"name\", TEXT),\n",
    "                 (\"label\", LABEL)]\n",
    "\n",
    "test_data = TabularDataset(path = \"./ICLR_test.csv\",\n",
    "                    format='csv',\n",
    "                    skip_header=True,\n",
    "                    fields=tst_datafields)\n",
    "#train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=EMBEDDING_LEN))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "train_iter = data.BucketIterator(train_data, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "test_iter = data.BucketIterator(test_data, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text), repeat=False, shuffle=False)\n",
    "\n",
    "print(next(iter(train_iter)).name)\n",
    "vocab_size = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_length, hidden_size, num_layers=10, bidirectional=True)\n",
    "        \n",
    "        self.label = nn.Linear(20*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):    \n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        h_0 = torch.zeros(20, input_sentences.size()[0], self.hidden_size).cuda()\n",
    "        output, h_n = self.rnn(input, h_0)\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2)\n",
    "        h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "        logits = self.label(h_n)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "net = RNN(2, HIDDEN_SIZE, vocab_size, EMBEDDING_LEN, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00005, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_data = 0\n",
    "    net.cuda()\n",
    "    steps = 0\n",
    "    net.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.name[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(text)\n",
    "        loss = criterion(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = num_corrects\n",
    "        total_data += len(batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()*len(batch)\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/total_data, 100*total_epoch_acc/total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_data = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.name[0]\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = net(text)\n",
    "            loss = criterion(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = num_corrects\n",
    "            total_data += len(batch)\n",
    "            \n",
    "            total_epoch_loss += loss.item()*len(batch)\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/total_data, 100*total_epoch_acc/total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with loss of 0.680188023609671 training acc of 57.24696356275304 testing acc of 53.0\n",
      "Epoch 1 with loss of 0.6775208137295989 training acc of 57.16599190283401 testing acc of 51.0\n",
      "Epoch 2 with loss of 0.6745541119865077 training acc of 57.48987854251012 testing acc of 52.0\n",
      "Epoch 3 with loss of 0.672290388389155 training acc of 58.13765182186235 testing acc of 53.0\n",
      "Epoch 4 with loss of 0.6701920100069239 training acc of 59.02834008097166 testing acc of 52.0\n",
      "Epoch 5 with loss of 0.6680148678269946 training acc of 58.8663967611336 testing acc of 51.0\n",
      "Epoch 6 with loss of 0.6666460112038894 training acc of 58.13765182186235 testing acc of 51.0\n",
      "Epoch 7 with loss of 0.6639819277925529 training acc of 59.19028340080972 testing acc of 51.0\n",
      "Epoch 8 with loss of 0.661611995957641 training acc of 59.67611336032389 testing acc of 52.0\n",
      "Epoch 9 with loss of 0.6598271201496665 training acc of 59.67611336032389 testing acc of 51.0\n",
      "Epoch 10 with loss of 0.6568525341840891 training acc of 61.05263157894737 testing acc of 50.0\n",
      "Epoch 11 with loss of 0.6546085216255806 training acc of 61.1336032388664 testing acc of 51.0\n",
      "Epoch 12 with loss of 0.6519129889214087 training acc of 60.64777327935223 testing acc of 52.0\n",
      "Epoch 13 with loss of 0.6490635879126637 training acc of 61.53846153846154 testing acc of 53.0\n",
      "Epoch 14 with loss of 0.6460749958208215 training acc of 62.42914979757085 testing acc of 53.0\n",
      "Epoch 15 with loss of 0.6430499942196526 training acc of 62.51012145748988 testing acc of 54.0\n",
      "Epoch 16 with loss of 0.6397189061168717 training acc of 62.59109311740891 testing acc of 54.0\n",
      "Epoch 17 with loss of 0.6364828326441498 training acc of 63.31983805668016 testing acc of 52.0\n",
      "Epoch 18 with loss of 0.6328871609710971 training acc of 63.7246963562753 testing acc of 52.0\n",
      "Epoch 19 with loss of 0.6285123745439506 training acc of 64.37246963562752 testing acc of 52.0\n",
      "Epoch 20 with loss of 0.6243409919835295 training acc of 65.4251012145749 testing acc of 52.0\n",
      "Epoch 21 with loss of 0.620044164812034 training acc of 65.58704453441295 testing acc of 51.0\n",
      "Epoch 22 with loss of 0.6152378578900326 training acc of 66.47773279352226 testing acc of 49.0\n",
      "Epoch 23 with loss of 0.6104360015286125 training acc of 67.1255060728745 testing acc of 50.0\n",
      "Epoch 24 with loss of 0.6050890230456827 training acc of 67.53036437246963 testing acc of 50.0\n",
      "Epoch 25 with loss of 0.6006919871940304 training acc of 67.1255060728745 testing acc of 50.0\n",
      "Epoch 26 with loss of 0.593842131164875 training acc of 68.66396761133603 testing acc of 50.0\n",
      "Epoch 27 with loss of 0.5887080812261172 training acc of 69.95951417004049 testing acc of 50.0\n",
      "Epoch 28 with loss of 0.5824997738788003 training acc of 69.95951417004049 testing acc of 48.0\n",
      "Epoch 29 with loss of 0.5759319894709568 training acc of 71.17408906882591 testing acc of 50.0\n",
      "Epoch 30 with loss of 0.5695805202128916 training acc of 71.74089068825911 testing acc of 50.0\n",
      "Epoch 31 with loss of 0.563058811160717 training acc of 71.82186234817814 testing acc of 50.0\n",
      "Epoch 32 with loss of 0.555618557852772 training acc of 72.38866396761134 testing acc of 50.0\n",
      "Epoch 33 with loss of 0.5495734333026747 training acc of 73.19838056680162 testing acc of 51.0\n",
      "Epoch 34 with loss of 0.5415139637495342 training acc of 73.92712550607287 testing acc of 52.0\n",
      "Epoch 35 with loss of 0.5359154485980508 training acc of 74.08906882591093 testing acc of 52.0\n",
      "Epoch 36 with loss of 0.5286755361537702 training acc of 74.5748987854251 testing acc of 51.0\n",
      "Epoch 37 with loss of 0.520253286187948 training acc of 75.30364372469636 testing acc of 51.0\n",
      "Epoch 38 with loss of 0.514870827255944 training acc of 75.7085020242915 testing acc of 50.0\n",
      "Epoch 39 with loss of 0.5047239955620244 training acc of 76.19433198380567 testing acc of 50.0\n",
      "Epoch 40 with loss of 0.4978132800534669 training acc of 76.43724696356276 testing acc of 51.0\n",
      "Epoch 41 with loss of 0.4891686373152714 training acc of 76.92307692307692 testing acc of 51.0\n",
      "Epoch 42 with loss of 0.48044882008903905 training acc of 77.81376518218623 testing acc of 50.0\n",
      "Epoch 43 with loss of 0.47217984624237186 training acc of 78.46153846153847 testing acc of 50.0\n",
      "Epoch 44 with loss of 0.46422748056500546 training acc of 78.13765182186235 testing acc of 48.0\n",
      "Epoch 45 with loss of 0.4536676795376457 training acc of 80.0 testing acc of 50.0\n",
      "Epoch 46 with loss of 0.4466680676830925 training acc of 79.51417004048584 testing acc of 49.0\n",
      "Epoch 47 with loss of 0.43727065097947837 training acc of 80.80971659919028 testing acc of 50.0\n",
      "Epoch 48 with loss of 0.42789077879446236 training acc of 80.80971659919028 testing acc of 48.0\n",
      "Epoch 49 with loss of 0.4176942385160006 training acc of 81.05263157894737 testing acc of 49.0\n",
      "Epoch 50 with loss of 0.40881751821591306 training acc of 82.5910931174089 testing acc of 49.0\n",
      "Epoch 51 with loss of 0.3984378881541341 training acc of 83.64372469635627 testing acc of 52.0\n",
      "Epoch 52 with loss of 0.38982713512080885 training acc of 84.21052631578948 testing acc of 52.0\n",
      "Epoch 53 with loss of 0.3811713502957271 training acc of 84.2914979757085 testing acc of 50.0\n",
      "Epoch 54 with loss of 0.3691427513414066 training acc of 85.02024291497976 testing acc of 48.0\n",
      "Epoch 55 with loss of 0.36358644038076826 training acc of 86.23481781376518 testing acc of 51.0\n",
      "Epoch 56 with loss of 0.35274883573837124 training acc of 85.50607287449392 testing acc of 48.0\n",
      "Epoch 57 with loss of 0.3407189703663351 training acc of 87.20647773279352 testing acc of 53.0\n",
      "Epoch 58 with loss of 0.32848502291358916 training acc of 87.77327935222672 testing acc of 49.0\n",
      "Epoch 59 with loss of 0.3212152425094172 training acc of 88.34008097165992 testing acc of 50.0\n",
      "Epoch 60 with loss of 0.31030780861252233 training acc of 88.50202429149797 testing acc of 50.0\n",
      "Epoch 61 with loss of 0.3001634968437164 training acc of 89.23076923076923 testing acc of 50.0\n",
      "Epoch 62 with loss of 0.2906983920678436 training acc of 90.20242914979757 testing acc of 49.0\n",
      "Epoch 63 with loss of 0.2799879666040783 training acc of 90.44534412955466 testing acc of 52.0\n",
      "Epoch 64 with loss of 0.2732236812471861 training acc of 90.93117408906883 testing acc of 50.0\n",
      "Epoch 65 with loss of 0.2618011719422784 training acc of 91.417004048583 testing acc of 52.0\n",
      "Epoch 66 with loss of 0.2528033596180711 training acc of 91.82186234817814 testing acc of 51.0\n",
      "Epoch 67 with loss of 0.24313247734718477 training acc of 92.5506072874494 testing acc of 51.0\n",
      "Epoch 68 with loss of 0.2331412656587145 training acc of 92.95546558704453 testing acc of 52.0\n",
      "Epoch 69 with loss of 0.22627368412519755 training acc of 92.79352226720648 testing acc of 50.0\n",
      "Epoch 70 with loss of 0.216468411780562 training acc of 93.36032388663968 testing acc of 50.0\n",
      "Epoch 71 with loss of 0.20838827956543277 training acc of 94.08906882591093 testing acc of 51.0\n",
      "Epoch 72 with loss of 0.20080006351837745 training acc of 94.33198380566802 testing acc of 51.0\n",
      "Epoch 73 with loss of 0.19971376241218705 training acc of 94.08906882591093 testing acc of 51.0\n",
      "Epoch 74 with loss of 0.18673523047916318 training acc of 94.5748987854251 testing acc of 52.0\n",
      "Epoch 75 with loss of 0.17896871612622187 training acc of 94.49392712550608 testing acc of 51.0\n",
      "Epoch 76 with loss of 0.16994665213200727 training acc of 95.38461538461539 testing acc of 51.0\n",
      "Epoch 77 with loss of 0.16156578308295624 training acc of 95.62753036437248 testing acc of 50.0\n",
      "Epoch 78 with loss of 0.1557837891795857 training acc of 95.62753036437248 testing acc of 51.0\n",
      "Epoch 79 with loss of 0.14938576990052274 training acc of 96.03238866396761 testing acc of 51.0\n",
      "Epoch 80 with loss of 0.14083837337701427 training acc of 96.35627530364373 testing acc of 52.0\n",
      "Epoch 81 with loss of 0.13706716159094684 training acc of 95.95141700404858 testing acc of 51.0\n",
      "Epoch 82 with loss of 0.13019123697570462 training acc of 96.51821862348179 testing acc of 51.0\n",
      "Epoch 83 with loss of 0.12553746756995737 training acc of 96.76113360323886 testing acc of 51.0\n",
      "Epoch 84 with loss of 0.11953475290102515 training acc of 96.92307692307692 testing acc of 53.0\n",
      "Epoch 85 with loss of 0.1167068463950022 training acc of 96.84210526315789 testing acc of 51.0\n",
      "Epoch 86 with loss of 0.10999530122468346 training acc of 97.4089068825911 testing acc of 51.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 with loss of 0.10500785672230277 training acc of 97.4089068825911 testing acc of 50.0\n",
      "Epoch 88 with loss of 0.09953794592184577 training acc of 97.4089068825911 testing acc of 51.0\n",
      "Epoch 89 with loss of 0.09700511920789957 training acc of 97.48987854251013 testing acc of 52.0\n",
      "Epoch 90 with loss of 0.09664650255369271 training acc of 97.97570850202429 testing acc of 50.0\n",
      "Epoch 91 with loss of 0.088878869090365 training acc of 97.97570850202429 testing acc of 51.0\n",
      "Epoch 92 with loss of 0.08583864307234644 training acc of 98.13765182186235 testing acc of 51.0\n",
      "Epoch 93 with loss of 0.0833572111873009 training acc of 98.29959514170041 testing acc of 52.0\n",
      "Epoch 94 with loss of 0.07973132502694845 training acc of 98.29959514170041 testing acc of 52.0\n",
      "Epoch 95 with loss of 0.07552736125856276 training acc of 98.70445344129554 testing acc of 51.0\n",
      "Epoch 96 with loss of 0.07312957357298508 training acc of 98.70445344129554 testing acc of 52.0\n",
      "Epoch 97 with loss of 0.07064438517759686 training acc of 98.78542510121457 testing acc of 52.0\n",
      "Epoch 98 with loss of 0.06775945599986474 training acc of 99.02834008097166 testing acc of 54.0\n",
      "Epoch 99 with loss of 0.06798411432186119 training acc of 98.70445344129554 testing acc of 51.0\n",
      "Epoch 100 with loss of 0.06443983940822393 training acc of 98.78542510121457 testing acc of 52.0\n",
      "Epoch 101 with loss of 0.061700218662559266 training acc of 99.02834008097166 testing acc of 52.0\n",
      "Epoch 102 with loss of 0.05815048548977385 training acc of 99.19028340080972 testing acc of 52.0\n",
      "Epoch 103 with loss of 0.056761217850301915 training acc of 99.10931174089069 testing acc of 52.0\n",
      "Epoch 104 with loss of 0.054446973132821715 training acc of 99.19028340080972 testing acc of 53.0\n",
      "Epoch 105 with loss of 0.05304254558284273 training acc of 99.19028340080972 testing acc of 52.0\n",
      "Epoch 106 with loss of 0.05118331705269061 training acc of 99.02834008097166 testing acc of 53.0\n",
      "Epoch 107 with loss of 0.04970620203808493 training acc of 99.27125506072875 testing acc of 52.0\n",
      "Epoch 108 with loss of 0.047625570946376816 training acc of 99.10931174089069 testing acc of 52.0\n",
      "Epoch 109 with loss of 0.04572636413157952 training acc of 99.27125506072875 testing acc of 52.0\n",
      "Epoch 110 with loss of 0.045451980823206034 training acc of 99.35222672064778 testing acc of 55.0\n",
      "Epoch 111 with loss of 0.044427618904695335 training acc of 99.19028340080972 testing acc of 52.0\n",
      "Epoch 112 with loss of 0.04154940839359152 training acc of 99.35222672064778 testing acc of 54.0\n",
      "Epoch 113 with loss of 0.04091166450004829 training acc of 99.43319838056681 testing acc of 54.0\n",
      "Epoch 114 with loss of 0.03883496627422721 training acc of 99.43319838056681 testing acc of 54.0\n",
      "Epoch 115 with loss of 0.03755711123105968 training acc of 99.27125506072875 testing acc of 55.0\n",
      "Epoch 116 with loss of 0.03642670859993711 training acc of 99.35222672064778 testing acc of 55.0\n",
      "Epoch 117 with loss of 0.03423979122810036 training acc of 99.43319838056681 testing acc of 56.0\n",
      "Epoch 118 with loss of 0.033735336155181955 training acc of 99.35222672064778 testing acc of 54.0\n",
      "Epoch 119 with loss of 0.03319255113300042 training acc of 99.35222672064778 testing acc of 55.0\n",
      "Epoch 120 with loss of 0.03457759587689933 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 121 with loss of 0.030856358679199992 training acc of 99.43319838056681 testing acc of 56.0\n",
      "Epoch 122 with loss of 0.029539229672568047 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 123 with loss of 0.029339638821448875 training acc of 99.35222672064778 testing acc of 55.0\n",
      "Epoch 124 with loss of 0.02778097959273016 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 125 with loss of 0.02707162988089357 training acc of 99.51417004048584 testing acc of 56.0\n",
      "Epoch 126 with loss of 0.026444644316548277 training acc of 99.43319838056681 testing acc of 56.0\n",
      "Epoch 127 with loss of 0.027442999598890662 training acc of 99.51417004048584 testing acc of 56.0\n",
      "Epoch 128 with loss of 0.024658954449691753 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 129 with loss of 0.024696421790641812 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 130 with loss of 0.02285472153938613 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 131 with loss of 0.02379535108442731 training acc of 99.27125506072875 testing acc of 55.0\n",
      "Epoch 132 with loss of 0.02235164340992688 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 133 with loss of 0.0204324053547643 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 134 with loss of 0.021042440840710514 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 135 with loss of 0.020922471154556584 training acc of 99.51417004048584 testing acc of 56.0\n",
      "Epoch 136 with loss of 0.02012285719878278 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 137 with loss of 0.02078157598492105 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 138 with loss of 0.01918289245592679 training acc of 99.67611336032388 testing acc of 58.0\n",
      "Epoch 139 with loss of 0.018583957392435808 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 140 with loss of 0.0187994917884831 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 141 with loss of 0.021754098663988867 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 142 with loss of 0.017958777998140465 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 143 with loss of 0.01862796534018719 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 144 with loss of 0.017303107751284533 training acc of 99.67611336032388 testing acc of 57.0\n",
      "Epoch 145 with loss of 0.01740330219389456 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 146 with loss of 0.016968716267752745 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 147 with loss of 0.017289097632021314 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 148 with loss of 0.018077961655582495 training acc of 99.51417004048584 testing acc of 52.0\n",
      "Epoch 149 with loss of 0.017005501759921007 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 150 with loss of 0.014992737052924478 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 151 with loss of 0.01476873069255036 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 152 with loss of 0.014200745768814917 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 153 with loss of 0.01397639076010539 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 154 with loss of 0.015074187161981577 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 155 with loss of 0.012971732754124563 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 156 with loss of 0.01359144356177162 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 157 with loss of 0.013336823255894517 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 158 with loss of 0.013813697427571544 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 159 with loss of 0.02023087013211207 training acc of 99.51417004048584 testing acc of 57.0\n",
      "Epoch 160 with loss of 0.01653906542370435 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 161 with loss of 0.015282900828626836 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 162 with loss of 0.012953050475122655 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 163 with loss of 0.01344623871388947 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 164 with loss of 0.012176810454592289 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 165 with loss of 0.011419227889735206 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 166 with loss of 0.012873563881556273 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 167 with loss of 0.012304020263179232 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 168 with loss of 0.011461042196712332 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 169 with loss of 0.011679617567402632 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 170 with loss of 0.012604451647805057 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 171 with loss of 0.012249603090800254 training acc of 99.67611336032388 testing acc of 56.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 with loss of 0.012757071774633911 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 173 with loss of 0.013760352030260723 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 174 with loss of 0.011535739507299927 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 175 with loss of 0.010985723553368678 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 176 with loss of 0.012180941927846264 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 177 with loss of 0.012326514628350072 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 178 with loss of 0.011235777814665183 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 179 with loss of 0.010874786088997234 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 180 with loss of 0.010920358921668911 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 181 with loss of 0.010555540530109092 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 182 with loss of 0.013470604404927748 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 183 with loss of 0.010718523246624511 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 184 with loss of 0.010282746634940025 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 185 with loss of 0.010271603207628013 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 186 with loss of 0.010924846044202385 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 187 with loss of 0.010464101442539318 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 188 with loss of 0.009530952886531227 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 189 with loss of 0.01010459860662368 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 190 with loss of 0.010022738841946731 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 191 with loss of 0.010150985290797858 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 192 with loss of 0.009903138848436386 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 193 with loss of 0.011939152249470654 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 194 with loss of 0.009710255188316952 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 195 with loss of 0.010622623766798843 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 196 with loss of 0.010215122525123993 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 197 with loss of 0.009678939300841889 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 198 with loss of 0.009692197993137546 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 199 with loss of 0.009620780129973524 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 200 with loss of 0.010544860543238247 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 201 with loss of 0.00905878271394775 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 202 with loss of 0.009388209437962758 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 203 with loss of 0.010087706938551867 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 204 with loss of 0.011255263454593748 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 205 with loss of 0.013025391015491867 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 206 with loss of 0.011971222598724037 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 207 with loss of 0.011093624264212997 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 208 with loss of 0.009602096144803743 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 209 with loss of 0.009837315783568239 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 210 with loss of 0.008802356372817385 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 211 with loss of 0.010432407603059945 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 212 with loss of 0.010160264589254432 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 213 with loss of 0.008841412564219251 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 214 with loss of 0.0094259497728425 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 215 with loss of 0.009342055441623153 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 216 with loss of 0.01101379363839081 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 217 with loss of 0.00940923901041027 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 218 with loss of 0.009811250271411378 training acc of 99.67611336032388 testing acc of 57.0\n",
      "Epoch 219 with loss of 0.01066239884213699 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 220 with loss of 0.008583695546682426 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 221 with loss of 0.008635418353836063 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 222 with loss of 0.010341367697501594 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 223 with loss of 0.00948798359345328 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 224 with loss of 0.011303378476455867 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 225 with loss of 0.009128584458609583 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 226 with loss of 0.00860485524412335 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 227 with loss of 0.009594460825816642 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 228 with loss of 0.00860738379481109 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 229 with loss of 0.010003564677574494 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 230 with loss of 0.00993859420270331 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 231 with loss of 0.012107896325385824 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 232 with loss of 0.01122030998940244 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 233 with loss of 0.007948983806611494 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 234 with loss of 0.009516558110778058 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 235 with loss of 0.008900075014548656 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 236 with loss of 0.008961670500365828 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 237 with loss of 0.008300021588651516 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 238 with loss of 0.00901262866034312 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 239 with loss of 0.009645803380938556 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 240 with loss of 0.00792946095525585 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 241 with loss of 0.009144986859006197 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 242 with loss of 0.009007291027499356 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 243 with loss of 0.012427004667372265 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 244 with loss of 0.011079971922697265 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 245 with loss of 0.0133994945800045 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 246 with loss of 0.011654609191205036 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 247 with loss of 0.009386555404828386 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 248 with loss of 0.008780892338860131 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 249 with loss of 0.008182054525052728 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 250 with loss of 0.00966585988908448 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 251 with loss of 0.008615954630623613 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 252 with loss of 0.008392283605290991 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 253 with loss of 0.009241158101681303 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 254 with loss of 0.010143352219993287 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 255 with loss of 0.00926805747416519 training acc of 99.75708502024291 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 with loss of 0.009789058711447637 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 257 with loss of 0.008452013279179391 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 258 with loss of 0.009455367598516739 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 259 with loss of 0.00922975393939718 training acc of 99.59514170040485 testing acc of 57.0\n",
      "Epoch 260 with loss of 0.008993718871631874 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 261 with loss of 0.008311191042093371 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 262 with loss of 0.009045454870008294 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 263 with loss of 0.007954398434051135 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 264 with loss of 0.008207608872849253 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 265 with loss of 0.01019999522975974 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 266 with loss of 0.00872621244962067 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 267 with loss of 0.008777779734388053 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 268 with loss of 0.008043399665067312 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 269 with loss of 0.010558816462921106 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 270 with loss of 0.008303081666718911 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 271 with loss of 0.008851372094839991 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 272 with loss of 0.008716346842127472 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 273 with loss of 0.010590926256701954 training acc of 99.51417004048584 testing acc of 55.0\n",
      "Epoch 274 with loss of 0.008356799467689233 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 275 with loss of 0.008043165998307136 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 276 with loss of 0.008368108901364902 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 277 with loss of 0.008005572521120219 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 278 with loss of 0.008614853742769976 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 279 with loss of 0.009062755941545311 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 280 with loss of 0.007853188484564032 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 281 with loss of 0.008541525154457491 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 282 with loss of 0.00809095782767001 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 283 with loss of 0.007933021335474211 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 284 with loss of 0.007886718804495568 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 285 with loss of 0.008906880447354455 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 286 with loss of 0.007058277632852617 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 287 with loss of 0.009192821909560396 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 288 with loss of 0.009599401887510017 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 289 with loss of 0.0076569967746719534 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 290 with loss of 0.00851854185624659 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 291 with loss of 0.009366222641591985 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 292 with loss of 0.009513861781810053 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 293 with loss of 0.009132734383456409 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 294 with loss of 0.007872389056361638 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 295 with loss of 0.008664829603694229 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 296 with loss of 0.008098374736431217 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 297 with loss of 0.00882812635454647 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 298 with loss of 0.008086577403549526 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 299 with loss of 0.008048901578547199 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 300 with loss of 0.008770851414605433 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 301 with loss of 0.010760741962873101 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 302 with loss of 0.008435127035024678 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 303 with loss of 0.007789648945352839 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 304 with loss of 0.009117090816304147 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 305 with loss of 0.007415635767675605 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 306 with loss of 0.008478405248648997 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 307 with loss of 0.007605534983866731 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 308 with loss of 0.007555862667174776 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 309 with loss of 0.00863318981862955 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 310 with loss of 0.00791529111169667 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 311 with loss of 0.007624518109281898 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 312 with loss of 0.009061131153636525 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 313 with loss of 0.01050302463202824 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 314 with loss of 0.010459792481637314 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 315 with loss of 0.01011820471263962 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 316 with loss of 0.007965247251417654 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 317 with loss of 0.008591250622049275 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 318 with loss of 0.010112999882812985 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 319 with loss of 0.009489988932680143 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 320 with loss of 0.007317676799002326 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 321 with loss of 0.011404948616362655 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 322 with loss of 0.010768976448544939 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 323 with loss of 0.00786716470246011 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 324 with loss of 0.007694679282479335 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 325 with loss of 0.00824147526661183 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 326 with loss of 0.008064482895551846 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 327 with loss of 0.007631433237841738 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 328 with loss of 0.009676904026381218 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 329 with loss of 0.007438587507200974 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 330 with loss of 0.007768335541510419 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 331 with loss of 0.007537576718480225 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 332 with loss of 0.0077455003443268325 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 333 with loss of 0.007466967248966774 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 334 with loss of 0.007991122125236973 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 335 with loss of 0.009880666297705851 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 336 with loss of 0.008559256846741263 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 337 with loss of 0.00847617887961584 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 338 with loss of 0.008579673625430778 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 339 with loss of 0.0075045257923664595 training acc of 99.75708502024291 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340 with loss of 0.007839090107876038 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 341 with loss of 0.0077778412417276776 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 342 with loss of 0.00865690896347889 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 343 with loss of 0.007335222440749136 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 344 with loss of 0.008202563488652349 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 345 with loss of 0.008137430627604095 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 346 with loss of 0.008153559127963913 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 347 with loss of 0.008274089893786168 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 348 with loss of 0.00870364145865958 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 349 with loss of 0.008424570654831856 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 350 with loss of 0.007857855641271783 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 351 with loss of 0.007846108898859817 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 352 with loss of 0.008122242358418913 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 353 with loss of 0.0076536167001065155 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 354 with loss of 0.007731540914745587 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 355 with loss of 0.007205373566689136 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 356 with loss of 0.007339860856540591 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 357 with loss of 0.007619165895536904 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 358 with loss of 0.008272661973612934 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 359 with loss of 0.00857809235187255 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 360 with loss of 0.008234973508121846 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 361 with loss of 0.007287730164034103 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 362 with loss of 0.00890369847537535 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 363 with loss of 0.008452893799898565 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 364 with loss of 0.008593921236899353 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 365 with loss of 0.008127295904413906 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 366 with loss of 0.007588711139149323 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 367 with loss of 0.007380015470100725 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 368 with loss of 0.007317859586139741 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 369 with loss of 0.007281826171710115 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 370 with loss of 0.007680372939716557 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 371 with loss of 0.007404567621957496 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 372 with loss of 0.007904037688990775 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 373 with loss of 0.007066491901731262 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 374 with loss of 0.007903896186402753 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 375 with loss of 0.008152863944443282 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 376 with loss of 0.012712446377925605 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 377 with loss of 0.010054774632345628 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 378 with loss of 0.0074949179864856165 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 379 with loss of 0.00797856826267322 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 380 with loss of 0.007232050038591359 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 381 with loss of 0.007221172209772534 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 382 with loss of 0.007498909830047866 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 383 with loss of 0.008653701121847216 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 384 with loss of 0.010156490225891624 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 385 with loss of 0.006882983687576962 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 386 with loss of 0.007998446047259872 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 387 with loss of 0.007230701953406457 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 388 with loss of 0.007272239939662202 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 389 with loss of 0.007483626505631486 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 390 with loss of 0.008365155802851142 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 391 with loss of 0.007131820521505949 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 392 with loss of 0.00895305876789788 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 393 with loss of 0.007654219702251919 training acc of 99.83805668016194 testing acc of 55.0\n",
      "Epoch 394 with loss of 0.008755840335431536 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 395 with loss of 0.007172150352144018 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 396 with loss of 0.008027712560705615 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 397 with loss of 0.007689218121667502 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 398 with loss of 0.009121265153491093 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 399 with loss of 0.007547688763895023 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 400 with loss of 0.008091399156269788 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 401 with loss of 0.007510650336934699 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 402 with loss of 0.0076086087810320535 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 403 with loss of 0.007131979623192031 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 404 with loss of 0.0076985844614918374 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 405 with loss of 0.007207695909786162 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 406 with loss of 0.008042957918409274 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 407 with loss of 0.008059580404092728 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 408 with loss of 0.0074546156811192334 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 409 with loss of 0.0074225070613892635 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 410 with loss of 0.0075838505544959895 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 411 with loss of 0.007128618947331141 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 412 with loss of 0.007459336158872284 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 413 with loss of 0.007098553888405473 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 414 with loss of 0.007891293813998581 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 415 with loss of 0.007671336020787929 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 416 with loss of 0.0074890445397510705 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 417 with loss of 0.00837502459739052 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 418 with loss of 0.007702042235095461 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 419 with loss of 0.007399592933674844 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 420 with loss of 0.01124926066584131 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 421 with loss of 0.010453285364036332 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 422 with loss of 0.0079633321449055 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 423 with loss of 0.008575002442143277 training acc of 99.67611336032388 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424 with loss of 0.007576077795674202 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 425 with loss of 0.007117045098521675 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 426 with loss of 0.007762543365513898 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 427 with loss of 0.008126236042592237 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 428 with loss of 0.007724377034922029 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 429 with loss of 0.007901795483703315 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 430 with loss of 0.007265220845739361 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 431 with loss of 0.00719732279617807 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 432 with loss of 0.007326447461950423 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 433 with loss of 0.00751457812650099 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 434 with loss of 0.007864704444676426 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 435 with loss of 0.009750603719874613 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 436 with loss of 0.00787347924013279 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 437 with loss of 0.009914826830803806 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 438 with loss of 0.007969437979127804 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 439 with loss of 0.008002489557544425 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 440 with loss of 0.008165736795829133 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 441 with loss of 0.009041966937000672 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 442 with loss of 0.007944027208716058 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 443 with loss of 0.007781035960595376 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 444 with loss of 0.008113231860112535 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 445 with loss of 0.007401786658284631 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 446 with loss of 0.007555872028946183 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 447 with loss of 0.007117347812057597 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 448 with loss of 0.007389922345882877 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 449 with loss of 0.007727673642058302 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 450 with loss of 0.009006580663090836 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 451 with loss of 0.006951526490319143 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 452 with loss of 0.007045986554429863 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 453 with loss of 0.007072522950185244 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 454 with loss of 0.007290931775325133 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 455 with loss of 0.00824567810622862 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 456 with loss of 0.006776918175437974 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 457 with loss of 0.007552053707016459 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 458 with loss of 0.0073096317400296144 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 459 with loss of 0.007948353063684665 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 460 with loss of 0.007852531172078752 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 461 with loss of 0.008716750446035855 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 462 with loss of 0.007018847702078611 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 463 with loss of 0.007316986458029161 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 464 with loss of 0.006959279725301694 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 465 with loss of 0.007302661078297703 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 466 with loss of 0.007828042788717609 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 467 with loss of 0.009032524090216905 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 468 with loss of 0.007568186051556312 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 469 with loss of 0.007395540053253005 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 470 with loss of 0.007070318456627957 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 471 with loss of 0.007167993969310392 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 472 with loss of 0.007042521990737633 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 473 with loss of 0.007791694527047004 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 474 with loss of 0.00861082781909358 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 475 with loss of 0.007411357003588396 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 476 with loss of 0.007442375334384587 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 477 with loss of 0.007295423634124068 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 478 with loss of 0.007280429681788504 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 479 with loss of 0.007682794036587836 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 480 with loss of 0.007688832787718145 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 481 with loss of 0.007396308408280918 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 482 with loss of 0.007040618910352409 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 483 with loss of 0.0070494986366648376 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 484 with loss of 0.007054695902912602 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 485 with loss of 0.007334993614415498 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 486 with loss of 0.007004036847242769 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 487 with loss of 0.007425197334639607 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 488 with loss of 0.0074939896194226585 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 489 with loss of 0.0071591265991646875 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 490 with loss of 0.007896596002500873 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 491 with loss of 0.007897780625550698 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 492 with loss of 0.00731248128687895 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 493 with loss of 0.007019301621914513 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 494 with loss of 0.007196185498725555 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 495 with loss of 0.007740456706595415 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 496 with loss of 0.008965027574421499 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 497 with loss of 0.008181400432134507 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 498 with loss of 0.007504356854410488 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 499 with loss of 0.008299913577384764 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 500 with loss of 0.007771995139514711 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 501 with loss of 0.007145648029904345 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 502 with loss of 0.007211803627126899 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 503 with loss of 0.007018175698639561 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 504 with loss of 0.0072374017752456265 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 505 with loss of 0.007024560492251988 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 506 with loss of 0.007860937710052076 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 507 with loss of 0.00751141936311815 training acc of 99.75708502024291 testing acc of 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 508 with loss of 0.0072484306151213256 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 509 with loss of 0.007192626436663271 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 510 with loss of 0.008169048792882426 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 511 with loss of 0.006785531273311539 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 512 with loss of 0.007410419411200329 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 513 with loss of 0.007214272401374784 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 514 with loss of 0.007951492449454398 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 515 with loss of 0.007431829176764927 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 516 with loss of 0.007211675796033777 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 517 with loss of 0.007107199463325926 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 518 with loss of 0.006729194931608157 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 519 with loss of 0.007394577028005485 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 520 with loss of 0.0069507304018152204 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 521 with loss of 0.00791144013585832 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 522 with loss of 0.0070420383787497135 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 523 with loss of 0.0070476833554663455 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 524 with loss of 0.006763883033081105 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 525 with loss of 0.006967492487379595 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 526 with loss of 0.007257261555283689 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 527 with loss of 0.007531113627492563 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 528 with loss of 0.007112179126831804 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 529 with loss of 0.008251470039968156 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 530 with loss of 0.0075372304821100375 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 531 with loss of 0.008154344038476013 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 532 with loss of 0.006794887621861577 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 533 with loss of 0.0078036887302564886 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 534 with loss of 0.007068641639966423 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 535 with loss of 0.007560353398347567 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 536 with loss of 0.007384084998856219 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 537 with loss of 0.007558025850260231 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 538 with loss of 0.006972161771141384 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 539 with loss of 0.0071390944265341955 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 540 with loss of 0.007681794942385868 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 541 with loss of 0.007265349593897716 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 542 with loss of 0.0070128667159835515 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 543 with loss of 0.007371997896219917 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 544 with loss of 0.007357589457455555 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 545 with loss of 0.007098887086495274 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 546 with loss of 0.006984198468889069 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 547 with loss of 0.007246719002120408 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 548 with loss of 0.007142858328189331 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 549 with loss of 0.006823923533050879 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 550 with loss of 0.00705250564077492 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 551 with loss of 0.007154043477079511 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 552 with loss of 0.006944379488360665 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 553 with loss of 0.008035273059282588 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 554 with loss of 0.007445478773447742 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 555 with loss of 0.008037308644812538 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 556 with loss of 0.0072492877537995454 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 557 with loss of 0.00700593152486904 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 558 with loss of 0.00825827164473653 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 559 with loss of 0.007009296189250824 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 560 with loss of 0.007036104250175177 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 561 with loss of 0.00729362006550881 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 562 with loss of 0.007006885875761132 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 563 with loss of 0.0075641851172299854 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 564 with loss of 0.007685693278757908 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 565 with loss of 0.0068874468113510655 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 566 with loss of 0.007201199928818926 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 567 with loss of 0.007087049351605447 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 568 with loss of 0.006908845786817809 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 569 with loss of 0.007147108236830191 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 570 with loss of 0.007538609264177169 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 571 with loss of 0.008065169654515108 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 572 with loss of 0.006894716318048205 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 573 with loss of 0.0073050131679838716 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 574 with loss of 0.006866055853724497 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 575 with loss of 0.007046820907459994 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 576 with loss of 0.006958002655584034 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 577 with loss of 0.007002020640975535 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 578 with loss of 0.006912827112585653 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 579 with loss of 0.007601146403839623 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 580 with loss of 0.008026942958572677 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 581 with loss of 0.006895878171003404 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 582 with loss of 0.0070411716218446736 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 583 with loss of 0.0072023246172430964 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 584 with loss of 0.00721397942536782 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 585 with loss of 0.007394233922594744 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 586 with loss of 0.007275313154750915 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 587 with loss of 0.006926926712105842 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 588 with loss of 0.007353833739514753 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 589 with loss of 0.006916250058153817 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 590 with loss of 0.006907764630418587 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 591 with loss of 0.006859195011437905 training acc of 99.75708502024291 testing acc of 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592 with loss of 0.0069553935363029055 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 593 with loss of 0.007103176082169585 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 594 with loss of 0.0074368928971998245 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 595 with loss of 0.00715438954448252 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 596 with loss of 0.00814879877749182 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 597 with loss of 0.007529625425922123 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 598 with loss of 0.007798617143774978 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 599 with loss of 0.007236833068606282 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 600 with loss of 0.007014524667117997 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 601 with loss of 0.0070684803917255585 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 602 with loss of 0.006895385764945517 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 603 with loss of 0.007001469674947208 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 604 with loss of 0.007050176398773953 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 605 with loss of 0.006860198689905572 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 606 with loss of 0.007034473219200184 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 607 with loss of 0.007115624713766113 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 608 with loss of 0.006853975879140613 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 609 with loss of 0.0068835874473645925 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 610 with loss of 0.007344081112384411 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 611 with loss of 0.0070085225646621604 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 612 with loss of 0.007086108896790849 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 613 with loss of 0.007123485256397912 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 614 with loss of 0.007112267533781927 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 615 with loss of 0.00705061766421724 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 616 with loss of 0.00724137207513924 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 617 with loss of 0.007759270678191382 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 618 with loss of 0.007581423194939931 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 619 with loss of 0.007243891664439442 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 620 with loss of 0.006738061374967714 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 621 with loss of 0.007048384901532066 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 622 with loss of 0.007374578046820764 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 623 with loss of 0.007251764024345785 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 624 with loss of 0.008487561997827443 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 625 with loss of 0.0070960740747329915 training acc of 99.83805668016194 testing acc of 54.0\n",
      "Epoch 626 with loss of 0.007164072216903651 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 627 with loss of 0.00694074597828767 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 628 with loss of 0.007249602884950157 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 629 with loss of 0.007378114693277791 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 630 with loss of 0.006969252377390289 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 631 with loss of 0.007213828322698687 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 632 with loss of 0.007175967525179415 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 633 with loss of 0.007059016151917883 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 634 with loss of 0.00791311402113374 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 635 with loss of 0.006621725388532995 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 636 with loss of 0.007068411701147705 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 637 with loss of 0.008648499170354066 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 638 with loss of 0.00697479654798239 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 639 with loss of 0.007874913319477789 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 640 with loss of 0.007078658477148712 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 641 with loss of 0.0070821328596721 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 642 with loss of 0.007035683172804196 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 643 with loss of 0.0073747986231611 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 644 with loss of 0.006733201311001939 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 645 with loss of 0.007324148682115773 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 646 with loss of 0.008036891575374825 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 647 with loss of 0.006688637285199212 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 648 with loss of 0.007116004351200347 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 649 with loss of 0.0069932717287221075 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 650 with loss of 0.006604814562599669 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 651 with loss of 0.0069315508294328365 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 652 with loss of 0.007308550046564566 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 653 with loss of 0.007042032461481448 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 654 with loss of 0.007097599845990418 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 655 with loss of 0.007513656591532775 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 656 with loss of 0.006895367784787588 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 657 with loss of 0.007336547279396747 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 658 with loss of 0.00704804662735718 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 659 with loss of 0.006953560942449317 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 660 with loss of 0.007486126435117472 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 661 with loss of 0.007083020575341263 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 662 with loss of 0.007320536018692349 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 663 with loss of 0.0067345782605293 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 664 with loss of 0.007093247336567368 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 665 with loss of 0.006975649505371024 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 666 with loss of 0.006982101065379818 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 667 with loss of 0.0069130664218951445 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 668 with loss of 0.007947646663995528 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 669 with loss of 0.007288244044830502 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 670 with loss of 0.007899391992292877 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 671 with loss of 0.007252957451304323 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 672 with loss of 0.006816658306384255 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 673 with loss of 0.006612592666952632 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 674 with loss of 0.0069696444755058115 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 675 with loss of 0.006844932878159481 training acc of 99.75708502024291 testing acc of 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 676 with loss of 0.0072473571466108565 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 677 with loss of 0.007337933697272092 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 678 with loss of 0.006812133548591806 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 679 with loss of 0.0072238579550206565 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 680 with loss of 0.006968626493271365 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 681 with loss of 0.006838239730818401 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 682 with loss of 0.006849432861051137 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 683 with loss of 0.0070968175589765405 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 684 with loss of 0.0067891192535642604 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 685 with loss of 0.007719206307007957 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 686 with loss of 0.007307533239205802 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 687 with loss of 0.007006082696291554 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 688 with loss of 0.007318912491851358 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 689 with loss of 0.00702600041969947 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 690 with loss of 0.007195767446238216 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 691 with loss of 0.007967927731346879 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 692 with loss of 0.007365331233423467 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 693 with loss of 0.006924970990903713 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 694 with loss of 0.006967633551996209 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 695 with loss of 0.006764320761525272 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 696 with loss of 0.007814478955367346 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 697 with loss of 0.008297554383712805 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 698 with loss of 0.006751791817731854 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 699 with loss of 0.007293440239421028 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 700 with loss of 0.006909518817256805 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 701 with loss of 0.006801531493943279 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 702 with loss of 0.006735064538787468 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 703 with loss of 0.006794904550511073 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 704 with loss of 0.007338908686197027 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 705 with loss of 0.00779227106603379 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 706 with loss of 0.00896685438603917 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 707 with loss of 0.007162300051380371 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 708 with loss of 0.00704962019299568 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 709 with loss of 0.006997648542200174 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 710 with loss of 0.007120656948687456 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 711 with loss of 0.0069010264577727244 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 712 with loss of 0.006811564920426757 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 713 with loss of 0.006883300598715489 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 714 with loss of 0.006689936662237028 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 715 with loss of 0.00685521892648047 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 716 with loss of 0.007246784155463483 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 717 with loss of 0.006614203094566045 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 718 with loss of 0.006889060432343982 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 719 with loss of 0.006746874388820126 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 720 with loss of 0.006761926944045442 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 721 with loss of 0.006947217875835203 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 722 with loss of 0.006727995322206574 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 723 with loss of 0.006839107706961844 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 724 with loss of 0.00707915393139098 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 725 with loss of 0.006816123764696135 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 726 with loss of 0.006733396900260419 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 727 with loss of 0.00669214078094994 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 728 with loss of 0.007040251778551332 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 729 with loss of 0.006931907060376542 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 730 with loss of 0.006853749438496269 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 731 with loss of 0.006871943606199463 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 732 with loss of 0.00697926081328649 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 733 with loss of 0.007225219386314921 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 734 with loss of 0.007439280741118453 training acc of 99.51417004048584 testing acc of 53.0\n",
      "Epoch 735 with loss of 0.007082163287726842 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 736 with loss of 0.006762763449021046 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 737 with loss of 0.007051788136619426 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 738 with loss of 0.006838796888185884 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 739 with loss of 0.0070418282366039135 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 740 with loss of 0.006979428538101723 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 741 with loss of 0.006833039973306315 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 742 with loss of 0.006784361324189156 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 743 with loss of 0.006598464359243227 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 744 with loss of 0.007069287756432003 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 745 with loss of 0.007070025573116972 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 746 with loss of 0.007647712930362023 training acc of 99.59514170040485 testing acc of 56.0\n",
      "Epoch 747 with loss of 0.006811062752773282 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 748 with loss of 0.007098973927081954 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 749 with loss of 0.006654931112972129 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 750 with loss of 0.006953050275269598 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 751 with loss of 0.006925907948191998 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 752 with loss of 0.006979379459764518 training acc of 99.67611336032388 testing acc of 56.0\n",
      "Epoch 753 with loss of 0.0075925376300647225 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 754 with loss of 0.007060788867821698 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 755 with loss of 0.0069234323709532015 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 756 with loss of 0.006594855107360204 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 757 with loss of 0.006636930428358617 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 758 with loss of 0.007792794681059928 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 759 with loss of 0.006915008992113188 training acc of 99.75708502024291 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760 with loss of 0.007287717908434664 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 761 with loss of 0.006828551667001049 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 762 with loss of 0.007581429525140884 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 763 with loss of 0.007223288233282124 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 764 with loss of 0.007066153331097925 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 765 with loss of 0.006815804281933724 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 766 with loss of 0.006853557835136981 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 767 with loss of 0.0072271976594507755 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 768 with loss of 0.006869418963201736 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 769 with loss of 0.006766427567722144 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 770 with loss of 0.006760424578276997 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 771 with loss of 0.006754503166279473 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 772 with loss of 0.006996529581604256 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 773 with loss of 0.007509361989777309 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 774 with loss of 0.0070322365967225966 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 775 with loss of 0.006956202372579142 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 776 with loss of 0.006719066760962761 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 777 with loss of 0.00744811509279176 training acc of 99.59514170040485 testing acc of 55.0\n",
      "Epoch 778 with loss of 0.006742184160846127 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 779 with loss of 0.007449868825633788 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 780 with loss of 0.0070113219169481295 training acc of 99.51417004048584 testing acc of 54.0\n",
      "Epoch 781 with loss of 0.007028951377726654 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 782 with loss of 0.006837766630085363 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 783 with loss of 0.006816945635747336 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 784 with loss of 0.006668683909245499 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 785 with loss of 0.00676726803432411 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 786 with loss of 0.007349409744927522 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 787 with loss of 0.0068084677931831905 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 788 with loss of 0.008015700086535554 training acc of 99.75708502024291 testing acc of 57.0\n",
      "Epoch 789 with loss of 0.0067374995680571145 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 790 with loss of 0.006767572055823879 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 791 with loss of 0.0074855083201895 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 792 with loss of 0.006965770414674104 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 793 with loss of 0.006855477856072457 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 794 with loss of 0.006735627613923992 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 795 with loss of 0.006613978611831817 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 796 with loss of 0.007027778689898475 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 797 with loss of 0.0073990056353436424 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 798 with loss of 0.006822278630889826 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 799 with loss of 0.00721161309389307 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 800 with loss of 0.0067738762499127345 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 801 with loss of 0.006788381747153223 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 802 with loss of 0.006587360666024118 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 803 with loss of 0.006924539968712625 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 804 with loss of 0.006703216435721423 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 805 with loss of 0.006748395365840088 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 806 with loss of 0.006968057925434904 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 807 with loss of 0.0070962510512376335 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 808 with loss of 0.006623825880023462 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 809 with loss of 0.006739204038600984 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 810 with loss of 0.00689067416391967 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 811 with loss of 0.0067130161415672905 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 812 with loss of 0.0068616036257411065 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 813 with loss of 0.007053871779996706 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 814 with loss of 0.007001763226432341 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 815 with loss of 0.0067497415391861194 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 816 with loss of 0.006923376526829677 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 817 with loss of 0.006800101091519662 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 818 with loss of 0.006864982948879813 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 819 with loss of 0.006603847351585051 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 820 with loss of 0.006902185281190026 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 821 with loss of 0.007075117179616788 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 822 with loss of 0.006565783696567502 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 823 with loss of 0.006929733223816274 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 824 with loss of 0.006870799116094631 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 825 with loss of 0.006930076219292305 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 826 with loss of 0.006652094234772751 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 827 with loss of 0.006693823057495959 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 828 with loss of 0.006809235417316121 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 829 with loss of 0.0068694690134537 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 830 with loss of 0.006565587942171295 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 831 with loss of 0.006971203142221874 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 832 with loss of 0.006960334351556356 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 833 with loss of 0.006820419079083044 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 834 with loss of 0.006691214993464344 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 835 with loss of 0.006843489594521898 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 836 with loss of 0.006713347570774586 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 837 with loss of 0.007078433879000712 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 838 with loss of 0.008128749808534105 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 839 with loss of 0.00684252626268261 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 840 with loss of 0.006766317034025228 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 841 with loss of 0.006890952059900603 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 842 with loss of 0.006960120196940862 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 843 with loss of 0.006773464332177042 training acc of 99.75708502024291 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 844 with loss of 0.007237581372744619 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 845 with loss of 0.006821530883347518 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 846 with loss of 0.0070647772838776796 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 847 with loss of 0.006790841153119634 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 848 with loss of 0.007060254682684958 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 849 with loss of 0.006722072787443365 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 850 with loss of 0.006869577797768856 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 851 with loss of 0.0067782953677525525 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 852 with loss of 0.0066198543881394655 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 853 with loss of 0.0066848983625693555 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 854 with loss of 0.006647767195619948 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 855 with loss of 0.006779447828065076 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 856 with loss of 0.006997587144311691 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 857 with loss of 0.006767465179665429 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 858 with loss of 0.00704588525896036 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 859 with loss of 0.006501766845493423 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 860 with loss of 0.006804553142228911 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 861 with loss of 0.007046058047459148 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 862 with loss of 0.006692494696754631 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 863 with loss of 0.00670566711280691 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 864 with loss of 0.006602709803038356 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 865 with loss of 0.006811041017661535 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 866 with loss of 0.00677742413810722 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 867 with loss of 0.006835077800891675 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 868 with loss of 0.006975854327510416 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 869 with loss of 0.006813915481042448 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 870 with loss of 0.006884142711804803 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 871 with loss of 0.0066932184540507336 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 872 with loss of 0.007571105317102825 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 873 with loss of 0.006383631207147398 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 874 with loss of 0.006706190575008481 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 875 with loss of 0.007185609718885552 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 876 with loss of 0.007196200996927676 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 877 with loss of 0.006795164165515943 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 878 with loss of 0.006750201919777809 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 879 with loss of 0.0066027020377357705 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 880 with loss of 0.006639400706461514 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 881 with loss of 0.00697871180746595 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 882 with loss of 0.006500399392210126 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 883 with loss of 0.006819897906363165 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 884 with loss of 0.006729479012860038 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 885 with loss of 0.0068895404818522 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 886 with loss of 0.006578796669022606 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 887 with loss of 0.007222692122087184 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 888 with loss of 0.007655285209274063 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 889 with loss of 0.006906584008320646 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 890 with loss of 0.006664674616228367 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 891 with loss of 0.007288960345389429 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 892 with loss of 0.006722233717957595 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 893 with loss of 0.007495402254546929 training acc of 99.67611336032388 testing acc of 55.0\n",
      "Epoch 894 with loss of 0.007040071929310098 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 895 with loss of 0.006782243095327976 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 896 with loss of 0.0067926242002352005 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 897 with loss of 0.006556868670755236 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 898 with loss of 0.006639105927931788 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 899 with loss of 0.00736913902082703 training acc of 99.75708502024291 testing acc of 56.0\n",
      "Epoch 900 with loss of 0.007137713979451063 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 901 with loss of 0.006734892507842535 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 902 with loss of 0.007049347603338231 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 903 with loss of 0.006696472000331865 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 904 with loss of 0.006774000359957412 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 905 with loss of 0.007373009771716626 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 906 with loss of 0.006890878519057466 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 907 with loss of 0.006631475291568371 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 908 with loss of 0.006568774547609821 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 909 with loss of 0.006534353417635665 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 910 with loss of 0.007318521654450142 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 911 with loss of 0.007272167103379203 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 912 with loss of 0.006655195775193913 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 913 with loss of 0.006566036906726573 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 914 with loss of 0.006724715444175923 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 915 with loss of 0.006578088226497903 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 916 with loss of 0.006828936075754954 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 917 with loss of 0.006674451998994415 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 918 with loss of 0.006698821922636927 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 919 with loss of 0.00653300768503003 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 920 with loss of 0.006744464474064694 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 921 with loss of 0.006864800676185005 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 922 with loss of 0.006666787516849817 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 923 with loss of 0.0067460111892839415 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 924 with loss of 0.00670692452273981 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 925 with loss of 0.00666429196248257 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 926 with loss of 0.0065630028822726985 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 927 with loss of 0.0067365573343739805 training acc of 99.75708502024291 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928 with loss of 0.006622092091612262 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 929 with loss of 0.0068034186869952076 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 930 with loss of 0.006946601808102479 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 931 with loss of 0.006538477873567281 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 932 with loss of 0.006767739602510708 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 933 with loss of 0.006869149546333125 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 934 with loss of 0.00692037485903063 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 935 with loss of 0.006812649774396941 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 936 with loss of 0.006733647667494629 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 937 with loss of 0.006707345980345856 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 938 with loss of 0.007073832185173536 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 939 with loss of 0.006765548528764992 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 940 with loss of 0.006626048119287883 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 941 with loss of 0.006766465079672785 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 942 with loss of 0.00656348644524364 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 943 with loss of 0.006690344220495817 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 944 with loss of 0.007364447350210326 training acc of 99.59514170040485 testing acc of 53.0\n",
      "Epoch 945 with loss of 0.006929216322171887 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 946 with loss of 0.006721217939719701 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 947 with loss of 0.006582095918598159 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 948 with loss of 0.006520700890538829 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 949 with loss of 0.006689174107752758 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 950 with loss of 0.006958520689155334 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 951 with loss of 0.006880207533046965 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 952 with loss of 0.006782824298586538 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 953 with loss of 0.006620187005419322 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 954 with loss of 0.006595619047310525 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 955 with loss of 0.006604248361953304 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 956 with loss of 0.006753869835674519 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 957 with loss of 0.006757435409277008 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 958 with loss of 0.006826275034386029 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 959 with loss of 0.006885401992837103 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 960 with loss of 0.006990430448244341 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 961 with loss of 0.00696698791203221 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 962 with loss of 0.006672030249643536 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 963 with loss of 0.00672608828191824 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 964 with loss of 0.0067718204477810635 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 965 with loss of 0.006866544277094039 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 966 with loss of 0.006611141409071705 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 967 with loss of 0.006614867485968937 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 968 with loss of 0.006607755407443443 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 969 with loss of 0.006661940573644919 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 970 with loss of 0.0067714304508413245 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 971 with loss of 0.007062832789537063 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 972 with loss of 0.0066840149297733385 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 973 with loss of 0.006463647235416089 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 974 with loss of 0.007316047008533921 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 975 with loss of 0.007059483417627488 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 976 with loss of 0.006849903140332774 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 977 with loss of 0.006550948681811301 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 978 with loss of 0.007098778411231111 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 979 with loss of 0.0068765140693188885 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 980 with loss of 0.0065682699872120255 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 981 with loss of 0.006934835638618568 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 982 with loss of 0.006510498415487149 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 983 with loss of 0.006570656275209831 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 984 with loss of 0.006614594475955557 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 985 with loss of 0.006679405473589309 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 986 with loss of 0.006653306130173725 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 987 with loss of 0.006581806327384351 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 988 with loss of 0.006524102009589188 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 989 with loss of 0.006502292892883974 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 990 with loss of 0.0065550690267974065 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 991 with loss of 0.006871029967206496 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 992 with loss of 0.0066033863525664215 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 993 with loss of 0.006589670085842129 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 994 with loss of 0.006754140081148037 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 995 with loss of 0.006647475049189982 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 996 with loss of 0.006991709095592868 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 997 with loss of 0.0068714932768719 training acc of 99.67611336032388 testing acc of 54.0\n",
      "Epoch 998 with loss of 0.006556626165974772 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 999 with loss of 0.006604000961955689 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1000 with loss of 0.007047997696748426 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 1001 with loss of 0.006997587812285841 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 1002 with loss of 0.0067113514508929965 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 1003 with loss of 0.006631110642371164 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1004 with loss of 0.0065684969230486185 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1005 with loss of 0.006702045845107019 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1006 with loss of 0.006670254585533478 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1007 with loss of 0.006511675425116321 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1008 with loss of 0.006573388057474334 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1009 with loss of 0.006603535284172284 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1010 with loss of 0.006793317698178623 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1011 with loss of 0.006641138941631652 training acc of 99.75708502024291 testing acc of 53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1012 with loss of 0.006718076162693243 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1013 with loss of 0.007058488979950021 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1014 with loss of 0.006779025303721164 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1015 with loss of 0.0067705625782108975 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1016 with loss of 0.0069213340553275484 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1017 with loss of 0.006744275886751853 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1018 with loss of 0.006548904773042427 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1019 with loss of 0.006721550899499797 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 1020 with loss of 0.006889991709554327 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 1021 with loss of 0.006674293265731011 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1022 with loss of 0.006711473309779954 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1023 with loss of 0.006651178039945021 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1024 with loss of 0.00672896061590326 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1025 with loss of 0.00716479205048496 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1026 with loss of 0.006760586072928329 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1027 with loss of 0.0066508928508328576 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1028 with loss of 0.006803637948950002 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1029 with loss of 0.0067118551592525895 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1030 with loss of 0.006615508639318247 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1031 with loss of 0.006570527070345214 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1032 with loss of 0.006999418128146092 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1033 with loss of 0.006623807630920217 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1034 with loss of 0.0066596177446199695 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1035 with loss of 0.006763105018645042 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1036 with loss of 0.006583323760585825 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1037 with loss of 0.006605791784212585 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1038 with loss of 0.00660818529966653 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1039 with loss of 0.006863930822328993 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1040 with loss of 0.006574539159885492 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1041 with loss of 0.00695509177911355 training acc of 99.75708502024291 testing acc of 55.0\n",
      "Epoch 1042 with loss of 0.006680917022971301 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1043 with loss of 0.006825297822994802 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1044 with loss of 0.006940816629603543 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1045 with loss of 0.00681963759204156 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1046 with loss of 0.006575252922585984 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1047 with loss of 0.00660106264382812 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1048 with loss of 0.0065798072145213955 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1049 with loss of 0.0066089574001777325 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1050 with loss of 0.0065975222088064745 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1051 with loss of 0.007118416707260489 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1052 with loss of 0.00693553840262657 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1053 with loss of 0.006600001758526655 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1054 with loss of 0.006546424490777655 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1055 with loss of 0.006612774367380118 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1056 with loss of 0.006760668731577363 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1057 with loss of 0.0065792324584682555 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1058 with loss of 0.00659003239509726 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1059 with loss of 0.006651382377408426 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1060 with loss of 0.006512075861390899 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1061 with loss of 0.006676149072923494 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1062 with loss of 0.006460514098986972 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1063 with loss of 0.006538149396425136 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1064 with loss of 0.006647035745486263 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1065 with loss of 0.00661803198849164 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1066 with loss of 0.0067559645338236595 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1067 with loss of 0.0065892721378198625 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1068 with loss of 0.0066550352734014925 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1069 with loss of 0.006697427938737754 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1070 with loss of 0.006952203652795088 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1071 with loss of 0.006656423667391578 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1072 with loss of 0.006589382607601762 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1073 with loss of 0.006769484346393149 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1074 with loss of 0.006703077805104104 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1075 with loss of 0.006620030873148751 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1076 with loss of 0.006852389163814149 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1077 with loss of 0.006466165839928685 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1078 with loss of 0.0067962545467689555 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1079 with loss of 0.00662051192070037 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1080 with loss of 0.006671076364118157 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1081 with loss of 0.00675936007218507 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1082 with loss of 0.006636843048938671 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1083 with loss of 0.006517118879693828 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1084 with loss of 0.0069096401030221775 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1085 with loss of 0.006677718371374868 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1086 with loss of 0.006610961950022109 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1087 with loss of 0.006793689027087864 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1088 with loss of 0.0066302581610127925 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1089 with loss of 0.006587331254585615 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1090 with loss of 0.006584836751977189 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1091 with loss of 0.006540048445753097 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1092 with loss of 0.006664197079871257 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1093 with loss of 0.007163435965664506 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1094 with loss of 0.006551927721927575 training acc of 99.75708502024291 testing acc of 53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1095 with loss of 0.006695864712706341 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1096 with loss of 0.006632884826722294 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1097 with loss of 0.006683087135124989 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1098 with loss of 0.0065579468182072245 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1099 with loss of 0.006617568799733799 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1100 with loss of 0.006575237876465328 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1101 with loss of 0.006634739301118411 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1102 with loss of 0.00667826941281393 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1103 with loss of 0.0067112415688214185 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1104 with loss of 0.007078807587972899 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1105 with loss of 0.006740685086343852 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1106 with loss of 0.006729216775211207 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1107 with loss of 0.006677894954359194 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1108 with loss of 0.006637899642235219 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1109 with loss of 0.006514645738589982 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1110 with loss of 0.006597342625682653 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1111 with loss of 0.006662834583459279 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1112 with loss of 0.006567586517482378 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1113 with loss of 0.007202160595360057 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1114 with loss of 0.007311806437538923 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1115 with loss of 0.006478174695002079 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1116 with loss of 0.006553066354474125 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1117 with loss of 0.006726084066708553 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1118 with loss of 0.006833091726633136 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1119 with loss of 0.006667347349648775 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1120 with loss of 0.0065782013556791325 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1121 with loss of 0.0065782324007468635 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1122 with loss of 0.00654935430610922 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1123 with loss of 0.006574620699016509 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1124 with loss of 0.006754139841601124 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1125 with loss of 0.00664311352417793 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1126 with loss of 0.006627401443604127 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1127 with loss of 0.006676811633016459 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1128 with loss of 0.0066894124525854635 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1129 with loss of 0.0068415339096701 training acc of 99.59514170040485 testing acc of 54.0\n",
      "Epoch 1130 with loss of 0.006711974562106179 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1131 with loss of 0.006649397152640031 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1132 with loss of 0.0065610322717512175 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1133 with loss of 0.006554665851080601 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1134 with loss of 0.006530871372851393 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1135 with loss of 0.006663048349856776 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1136 with loss of 0.006657949880442498 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1137 with loss of 0.0064998195271091614 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1138 with loss of 0.0065550014773371485 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1139 with loss of 0.006537659056695025 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1140 with loss of 0.006571071498616323 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1141 with loss of 0.006626764728795164 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1142 with loss of 0.0066341662258409375 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1143 with loss of 0.006796682628825107 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1144 with loss of 0.007019213438250863 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1145 with loss of 0.00676698752642168 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1146 with loss of 0.007024182588086362 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1147 with loss of 0.006449598069814706 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1148 with loss of 0.0066509040155688865 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1149 with loss of 0.0065605157353481195 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1150 with loss of 0.006489574253195727 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1151 with loss of 0.006683840849099828 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1152 with loss of 0.006478933156288873 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1153 with loss of 0.006805591720490064 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1154 with loss of 0.00684507171385865 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1155 with loss of 0.006479879916295162 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1156 with loss of 0.0065088950374813164 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1157 with loss of 0.006613523227142392 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1158 with loss of 0.007006094577076075 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1159 with loss of 0.006581005900907368 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1160 with loss of 0.006502295397271531 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1161 with loss of 0.0065453232559904515 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1162 with loss of 0.00652960355184795 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1163 with loss of 0.006616345868207446 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1164 with loss of 0.006542314619989123 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1165 with loss of 0.006746882978102818 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1166 with loss of 0.006525835488801647 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1167 with loss of 0.006682902041479553 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1168 with loss of 0.00680805962233001 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1169 with loss of 0.006909436169521707 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1170 with loss of 0.0065042560178148355 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1171 with loss of 0.006454649951986719 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1172 with loss of 0.006540725646228982 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1173 with loss of 0.006486570383756712 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1174 with loss of 0.006622808054943808 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1175 with loss of 0.006648639548413912 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1176 with loss of 0.006774773983942201 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1177 with loss of 0.006570242121987712 training acc of 99.75708502024291 testing acc of 52.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1178 with loss of 0.006610102574944285 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1179 with loss of 0.006519850883212248 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1180 with loss of 0.0065890866470676825 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1181 with loss of 0.006685196461159781 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1182 with loss of 0.006487084148454455 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1183 with loss of 0.006589486228402512 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1184 with loss of 0.006643794210868518 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1185 with loss of 0.00676744688820256 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1186 with loss of 0.0065711030899405646 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1187 with loss of 0.006730053135993238 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1188 with loss of 0.0071812288559099414 training acc of 99.59514170040485 testing acc of 52.0\n",
      "Epoch 1189 with loss of 0.006720597535781355 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1190 with loss of 0.00663020988450379 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1191 with loss of 0.006535484768009107 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1192 with loss of 0.006626938099959707 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1193 with loss of 0.0065145213784674 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1194 with loss of 0.0065363955765072755 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1195 with loss of 0.006539666334375445 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1196 with loss of 0.0065984578223108135 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1197 with loss of 0.007099284665792924 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1198 with loss of 0.006658078527333957 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1199 with loss of 0.006683011810449758 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1200 with loss of 0.006504442734617423 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1201 with loss of 0.006409121720007313 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1202 with loss of 0.006834340224755166 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1203 with loss of 0.006754366880847633 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1204 with loss of 0.0066138678633321365 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1205 with loss of 0.006655061286673217 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1206 with loss of 0.006798087870398148 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1207 with loss of 0.006576435780435469 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1208 with loss of 0.006574788575669489 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1209 with loss of 0.006586403016678268 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1210 with loss of 0.0067109459385922485 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1211 with loss of 0.0068305287610782835 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1212 with loss of 0.006469260174206078 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1213 with loss of 0.006449306540972146 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1214 with loss of 0.006531594024791084 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1215 with loss of 0.00665783330228798 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1216 with loss of 0.006292341358467502 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1217 with loss of 0.007195972738059818 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1218 with loss of 0.006815371113865371 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1219 with loss of 0.006440335649974373 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1220 with loss of 0.006491643377782533 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1221 with loss of 0.006639768375711363 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1222 with loss of 0.0065718054183779295 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1223 with loss of 0.006546065253199116 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1224 with loss of 0.007063991182317829 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1225 with loss of 0.006570636538158525 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1226 with loss of 0.006678886746430721 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1227 with loss of 0.006696726913506878 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1228 with loss of 0.006547224576595481 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1229 with loss of 0.006491513086458374 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1230 with loss of 0.006442449969171899 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1231 with loss of 0.006516333236504579 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1232 with loss of 0.0064897195746304905 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1233 with loss of 0.006499597493102887 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1234 with loss of 0.00653309596425322 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1235 with loss of 0.006758585728395981 training acc of 99.59514170040485 testing acc of 52.0\n",
      "Epoch 1236 with loss of 0.00670394081008943 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1237 with loss of 0.0065219934255106 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1238 with loss of 0.00644598443033777 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1239 with loss of 0.006516594619640792 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1240 with loss of 0.006843547305473511 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1241 with loss of 0.00644660261855555 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1242 with loss of 0.006462113443785158 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1243 with loss of 0.006581801565402651 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1244 with loss of 0.006550781777971528 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1245 with loss of 0.006485774241669928 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1246 with loss of 0.006522887690926111 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1247 with loss of 0.006606288608485269 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1248 with loss of 0.006666545233514743 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1249 with loss of 0.006464009707275616 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1250 with loss of 0.006493050601759451 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1251 with loss of 0.006629341628368536 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1252 with loss of 0.006452932515038321 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1253 with loss of 0.0065501847974616684 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1254 with loss of 0.006487878520914937 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1255 with loss of 0.006649951650129503 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1256 with loss of 0.006494540366747601 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1257 with loss of 0.006614014678061463 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1258 with loss of 0.006559472731272181 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1259 with loss of 0.006539145683521416 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1260 with loss of 0.0065676029881589475 training acc of 99.75708502024291 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1261 with loss of 0.0065979897779485505 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1262 with loss of 0.00662478678474174 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1263 with loss of 0.006518226974751699 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1264 with loss of 0.0066358167228599075 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1265 with loss of 0.006729006056431038 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1266 with loss of 0.0067112084063763785 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1267 with loss of 0.00644840435351166 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1268 with loss of 0.006562519002105354 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1269 with loss of 0.006550790570981033 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1270 with loss of 0.006683334617072863 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1271 with loss of 0.0065047654628104185 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1272 with loss of 0.007020168313011904 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1273 with loss of 0.006616913129552928 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1274 with loss of 0.006498343403039213 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1275 with loss of 0.006611069060550347 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1276 with loss of 0.006768192810952373 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1277 with loss of 0.006797392086272254 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1278 with loss of 0.006666071155122769 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1279 with loss of 0.0066542605859486 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1280 with loss of 0.006542778540584579 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1281 with loss of 0.006512630223686008 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1282 with loss of 0.006421684015185011 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1283 with loss of 0.0065532695857121286 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1284 with loss of 0.006560401537838328 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1285 with loss of 0.006773423462003745 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1286 with loss of 0.006581850839461644 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1287 with loss of 0.006516381148022679 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1288 with loss of 0.0064659668884413535 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1289 with loss of 0.006640180456758428 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1290 with loss of 0.006598776480769088 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1291 with loss of 0.006626105207835818 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1292 with loss of 0.006510920777329765 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1293 with loss of 0.006855397387103721 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1294 with loss of 0.0065994327175393835 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1295 with loss of 0.006630438916243308 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1296 with loss of 0.006886344572054581 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1297 with loss of 0.006609955728077023 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1298 with loss of 0.006508256757428289 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1299 with loss of 0.006594938403465606 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1300 with loss of 0.006727190285364707 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1301 with loss of 0.006542553451132696 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1302 with loss of 0.006548957721172806 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1303 with loss of 0.006534551338647089 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1304 with loss of 0.00654550834543822 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1305 with loss of 0.006584926863718056 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1306 with loss of 0.006463613176305009 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1307 with loss of 0.0065332362893733165 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1308 with loss of 0.0064913730240386425 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1309 with loss of 0.006447392776947816 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1310 with loss of 0.006524752611815194 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1311 with loss of 0.006561874511919403 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1312 with loss of 0.006573128562066176 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1313 with loss of 0.006530928514805445 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1314 with loss of 0.006547323624309584 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1315 with loss of 0.006631927016834061 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1316 with loss of 0.0064827977243819375 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1317 with loss of 0.006420823823344123 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1318 with loss of 0.006445779910533565 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1319 with loss of 0.006616567924365623 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1320 with loss of 0.006495979933401849 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1321 with loss of 0.0065020829687075996 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1322 with loss of 0.0065664527812576284 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1323 with loss of 0.006419519032640354 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1324 with loss of 0.00647700544499023 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1325 with loss of 0.006627256704319616 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1326 with loss of 0.0066003591478352155 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1327 with loss of 0.006810649970465299 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1328 with loss of 0.006564293351358553 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1329 with loss of 0.006502327458557828 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1330 with loss of 0.00667113419139672 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1331 with loss of 0.006672496264536492 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1332 with loss of 0.006572699544681255 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1333 with loss of 0.006566087388233678 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1334 with loss of 0.006532304488028906 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1335 with loss of 0.006538843552445048 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1336 with loss of 0.006519480504389892 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1337 with loss of 0.006461189026440443 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1338 with loss of 0.006591470458956958 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1339 with loss of 0.006579158111054286 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1340 with loss of 0.00662710558366679 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1341 with loss of 0.006541614210092169 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1342 with loss of 0.00658915917316512 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1343 with loss of 0.006534787801613744 training acc of 99.75708502024291 testing acc of 53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1344 with loss of 0.00658873062085234 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1345 with loss of 0.006510958689826291 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1346 with loss of 0.006395652761300158 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1347 with loss of 0.0065738960713155655 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1348 with loss of 0.0065546819943979925 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1349 with loss of 0.006480616716378621 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1350 with loss of 0.006522827090844908 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1351 with loss of 0.006553814718926322 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1352 with loss of 0.006529820104613184 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1353 with loss of 0.006610338104850385 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1354 with loss of 0.0068956490801430495 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1355 with loss of 0.006946509660544253 training acc of 99.67611336032388 testing acc of 53.0\n",
      "Epoch 1356 with loss of 0.006508118796979689 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1357 with loss of 0.006663002682235167 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1358 with loss of 0.006535913556510666 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1359 with loss of 0.006431864325637243 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1360 with loss of 0.006502478591317612 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1361 with loss of 0.006889840086816239 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1362 with loss of 0.0065768693935891725 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1363 with loss of 0.006473318995108549 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1364 with loss of 0.006432221873816485 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1365 with loss of 0.006424051645403466 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1366 with loss of 0.006455416340944838 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1367 with loss of 0.006500711460992659 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1368 with loss of 0.006553877465864245 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1369 with loss of 0.0065495655489424464 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1370 with loss of 0.006607690419009799 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1371 with loss of 0.006459377678707117 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1372 with loss of 0.006867887420961087 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1373 with loss of 0.006710131059294455 training acc of 99.67611336032388 testing acc of 52.0\n",
      "Epoch 1374 with loss of 0.0068510556018972334 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1375 with loss of 0.006414477493853329 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1376 with loss of 0.006553450740236593 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1377 with loss of 0.006453512004565842 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1378 with loss of 0.006414550197202814 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1379 with loss of 0.006520764441227046 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1380 with loss of 0.006677913943017873 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1381 with loss of 0.006388752983773678 training acc of 99.75708502024291 testing acc of 54.0\n",
      "Epoch 1382 with loss of 0.00649236787856662 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1383 with loss of 0.006476933844073517 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1384 with loss of 0.006460446413433903 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1385 with loss of 0.006498535479762877 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1386 with loss of 0.00641288868126154 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1387 with loss of 0.0065042298458712665 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1388 with loss of 0.0065646637790506 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1389 with loss of 0.006507867865498837 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1390 with loss of 0.006470255014771286 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1391 with loss of 0.006467679209640388 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1392 with loss of 0.006533475921859047 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1393 with loss of 0.006470953442013977 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1394 with loss of 0.006436425469322792 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1395 with loss of 0.006451354193707258 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1396 with loss of 0.006578696342905077 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1397 with loss of 0.006502055109154267 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1398 with loss of 0.0064590381158408355 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1399 with loss of 0.006592062885424326 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1400 with loss of 0.006561664341443027 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1401 with loss of 0.006700398869739906 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1402 with loss of 0.006531832214899226 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1403 with loss of 0.00663907732676101 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1404 with loss of 0.006871023297657236 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1405 with loss of 0.006769721972393645 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1406 with loss of 0.00649401044689799 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1407 with loss of 0.006438792198780641 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1408 with loss of 0.006500863686000348 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1409 with loss of 0.006558450565126448 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1410 with loss of 0.006698869287235726 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1411 with loss of 0.006783981253702886 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1412 with loss of 0.006546708930554556 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1413 with loss of 0.006697510575558915 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1414 with loss of 0.006528171623741448 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1415 with loss of 0.006525656487415677 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1416 with loss of 0.0065069710536086265 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1417 with loss of 0.006526088932571982 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1418 with loss of 0.0065025134882095515 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1419 with loss of 0.006543825398944413 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1420 with loss of 0.006458955695383677 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1421 with loss of 0.006462678472265914 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1422 with loss of 0.00651426185645192 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1423 with loss of 0.006544222614031779 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1424 with loss of 0.006429271046285221 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1425 with loss of 0.006675083190084884 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1426 with loss of 0.006512595554528596 training acc of 99.75708502024291 testing acc of 53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1427 with loss of 0.00653901259618225 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1428 with loss of 0.006404155366717053 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1429 with loss of 0.00646527842112301 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1430 with loss of 0.006426353629516826 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1431 with loss of 0.006724742738561147 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1432 with loss of 0.006516048453800306 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1433 with loss of 0.0068648467483145076 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1434 with loss of 0.006493893325057113 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1435 with loss of 0.0065607204208497395 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1436 with loss of 0.006583924520219231 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1437 with loss of 0.006403190149138078 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1438 with loss of 0.006494986609211013 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1439 with loss of 0.006530838008078761 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1440 with loss of 0.006488036458683459 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1441 with loss of 0.006569697886824053 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1442 with loss of 0.0064010319560045105 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1443 with loss of 0.006501909942517712 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1444 with loss of 0.006626038455726251 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1445 with loss of 0.006563825491067707 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1446 with loss of 0.006446306891794748 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1447 with loss of 0.006639844914956013 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1448 with loss of 0.006489580413604698 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1449 with loss of 0.006575343264292771 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1450 with loss of 0.00651138289099859 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1451 with loss of 0.006677224181970932 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1452 with loss of 0.006542800874018647 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1453 with loss of 0.006577961026557282 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1454 with loss of 0.006649231003938613 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1455 with loss of 0.006538715583167646 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1456 with loss of 0.0064396756659165265 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1457 with loss of 0.006663969593079712 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1458 with loss of 0.006611489296406752 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1459 with loss of 0.00645476083186604 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1460 with loss of 0.006523329581623071 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1461 with loss of 0.0065206857066462945 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1462 with loss of 0.006440775530960522 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1463 with loss of 0.006490349300484492 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1464 with loss of 0.0064272452481922895 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1465 with loss of 0.006509548424602192 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1466 with loss of 0.0064232922746808845 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1467 with loss of 0.006530861958432393 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1468 with loss of 0.006699596486472183 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1469 with loss of 0.006416594946727737 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1470 with loss of 0.0064441747831827925 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1471 with loss of 0.006518066084284692 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1472 with loss of 0.006495747951615007 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1473 with loss of 0.006515704767638535 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1474 with loss of 0.006633348146329544 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1475 with loss of 0.006669703938233335 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1476 with loss of 0.0066563332863298135 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1477 with loss of 0.006670987617385233 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1478 with loss of 0.0064907738746011676 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1479 with loss of 0.006538325141054184 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1480 with loss of 0.0065486830353915036 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1481 with loss of 0.0064722122200147 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1482 with loss of 0.0064314454161186856 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1483 with loss of 0.006559393246531362 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1484 with loss of 0.006506947761486135 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1485 with loss of 0.0064846700886198455 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1486 with loss of 0.0065340475408403485 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1487 with loss of 0.006440044949096022 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1488 with loss of 0.006473072592889219 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1489 with loss of 0.006613751884243736 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1490 with loss of 0.006523515695279703 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1491 with loss of 0.006457895670869239 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1492 with loss of 0.0064947927862722505 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1493 with loss of 0.006431352512420557 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1494 with loss of 0.006583205637637986 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Epoch 1495 with loss of 0.006641737702777486 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1496 with loss of 0.006522467203541029 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1497 with loss of 0.006431994370565663 training acc of 99.75708502024291 testing acc of 53.0\n",
      "Epoch 1498 with loss of 0.006516739256678951 training acc of 99.75708502024291 testing acc of 52.0\n",
      "Epoch 1499 with loss of 0.006575151305147993 training acc of 99.75708502024291 testing acc of 51.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_value = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    train_loss, train_acc = train_model(train_iter, epoch)\n",
    "    _, val_acc = eval_model(train_iter)\n",
    "    _, test_acc = eval_model(test_iter)\n",
    "    \n",
    "    loss_value.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print('Epoch', epoch, 'with loss of', train_loss, 'training acc of', train_acc, 'testing acc of', test_acc)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value:  0.006575151305147993\n",
      "Training accuracy:  99.75708502024291\n",
      "Testing accuracy:  51.0\n"
     ]
    }
   ],
   "source": [
    "print('Loss value: ', loss_value[-1])\n",
    "print('Training accuracy: ', train_accuracy[-1])\n",
    "print('Testing accuracy: ', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fee66681dd8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_title('Learning Curve'.format('default'), color='C0')\n",
    "ax.set_ylabel('Cross Entrpy Loss')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.plot(list(i+1 for i in range(len(loss_value))), loss_value, 'b')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_title('Accuracy'.format('default'), color='C0')\n",
    "ax.set_ylabel('Accuracy Rate')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.plot(list(i+1 for i in range(len(train_accuracy))), train_accuracy, 'b', label='train')\n",
    "ax.plot(list(i+1 for i in range(len(test_accuracy))), test_accuracy, 'tab:orange', label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
