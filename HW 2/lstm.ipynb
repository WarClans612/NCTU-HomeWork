{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "HIDDEN_SIZE = 32\n",
    "EMBEDDING_LEN = 300\n",
    "EPOCH = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   name  label\n",
      "0     Unsupervised Representation Learning by Predic...      1\n",
      "1     Emergent Communication in a Multi-Modal, Multi...      1\n",
      "2     FastGCN: Fast Learning with Graph Convolutiona...      1\n",
      "3     Emergent Translation in Multi-Agent Communication      1\n",
      "4     An efficient framework for learning sentence r...      1\n",
      "5     NerveNet: Learning Structured Policy with Grap...      1\n",
      "6     Learning Latent Representations in Neural Netw...      1\n",
      "7                    Adversarial Dropout Regularization      1\n",
      "8                                 Demystifying MMD GANs      1\n",
      "9     Smooth Loss Functions for Deep Top-k Classific...      1\n",
      "10    Deep Learning as a Mixed Convex-Combinatorial ...      1\n",
      "11    Learning Approximate Inference Networks for St...      1\n",
      "12    LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYIN...      1\n",
      "13    Model compression via distillation and quantiz...      1\n",
      "14    Variational Message Passing with Structured In...      1\n",
      "15    Action-dependent Control Variates for Policy O...      1\n",
      "16    Variational image compression with a scale hyp...      1\n",
      "17    Variational Inference of Disentangled Latent C...      1\n",
      "18    Flipout: Efficient Pseudo-Independent Weight P...      1\n",
      "19                Kernel Implicit Variational Inference      1\n",
      "20    A Scalable Laplace Approximation for Neural Ne...      1\n",
      "21    The High-Dimensional Geometry of Binary Neural...      1\n",
      "22    Apprentice: Using Knowledge Distillation Techn...      1\n",
      "23            Distributed Prioritized Experience Replay      1\n",
      "24    Learning from Between-class Examples for Deep ...      1\n",
      "25    Training Confidence-calibrated Classifiers for...      1\n",
      "26    VoiceLoop: Voice Fitting and Synthesis via a P...      1\n",
      "27    Large scale distributed neural network trainin...      1\n",
      "28    Learning Differentially Private Recurrent Lang...      1\n",
      "29    Mastering the Dungeon: Grounded Language Learn...      1\n",
      "...                                                 ...    ...\n",
      "1205  Compact Embedding of Binary-coded Inputs and O...      0\n",
      "1206  A Deep Learning Approach for Joint Video Frame...      0\n",
      "1207  Pedestrian Detection Based On Fast R-CNN and B...      0\n",
      "1208  An Actor-critic Algorithm for Learning Rate Le...      0\n",
      "1209  Training Group Orthogonal Neural Networks with...      0\n",
      "1210  Progressive Attention Networks for Visual Attr...      0\n",
      "1211                  A Neural Knowledge Language Model      0\n",
      "1212  Fuzzy paraphrases in learning word representat...      0\n",
      "1213  What Is the Best Practice for CNNs Applied to ...      0\n",
      "1214         Rectified Factor Networks for Biclustering      0\n",
      "1215                       Sampling Generative Networks      0\n",
      "1216  Learning Locomotion Skills Using DeepRL: Does ...      0\n",
      "1217       Iterative Refinement for Machine Translation      0\n",
      "1218  Semantic Noise Modeling for Better Representat...      0\n",
      "1219  Ternary Weight Decomposition and Binary Activa...      0\n",
      "1220  Vocabulary Selection Strategies for Neural Mac...      0\n",
      "1221  Efficient Communications in Training Large Sca...      0\n",
      "1222                           Dynamic Partition Models      0\n",
      "1223  Learning Efficient Algorithms with Hierarchica...      0\n",
      "1224             Revisiting Distributed Synchronous SGD      0\n",
      "1225  New Learning Approach By Genetic Algorithm In ...      0\n",
      "1226  PREDICTION OF POTENTIAL HUMAN INTENTION USING ...      0\n",
      "1227  Conditional Image Synthesis With Auxiliary Cla...      0\n",
      "1228      Multi-label learning with semantic embeddings      0\n",
      "1229  Significance of Softmax-Based Features over Me...      0\n",
      "1230  Improving Sampling from Generative Autoencoder...      0\n",
      "1231  Learning to Protect Communications with Advers...      0\n",
      "1232  Deep unsupervised learning through spatial con...      0\n",
      "1233  SoftTarget Regularization: An Effective Techni...      0\n",
      "1234    Surprisal-Driven Feedback in Recurrent Networks      0\n",
      "\n",
      "[1235 rows x 2 columns]\n",
      "                                                 name  label\n",
      "0   Minimal-Entropy Correlation Alignment for Unsu...      1\n",
      "1   Large Scale Optimal Transport and Mapping Esti...      1\n",
      "2   TRUNCATED HORIZON POLICY SEARCH: COMBINING REI...      1\n",
      "3     Model-Ensemble Trust-Region Policy Optimization      1\n",
      "4          A Neural Representation of Sketch Drawings      1\n",
      "5           Deep Learning with Logged Bandit Feedback      1\n",
      "6   Learning Latent Permutations with Gumbel-Sinkh...      1\n",
      "7   Learning an Embedding Space for Transferable R...      1\n",
      "8   Unsupervised Learning of Goal Spaces for Intri...      1\n",
      "9   Multi-View Data Generation Without View Superv...      1\n",
      "10  Deep Bayesian Bandits Showdown:  An Empirical ...      1\n",
      "11          Semantic Interpolation in Implicit Models      1\n",
      "12                         Fidelity-Weighted Learning      1\n",
      "13  Latent Space Oddity: on the Curvature of Deep ...      1\n",
      "14  Imitation Learning from Visual Data with Multi...      1\n",
      "15   Hyperparameter optimization: a spectral approach      1\n",
      "16  Leveraging Grammar and Reinforcement Learning ...      1\n",
      "17  Efficient Sparse-Winograd Convolutional Neural...      1\n",
      "18  Espresso: Efficient Forward Propagation for Bi...      1\n",
      "19  Auto-Conditioned Recurrent Networks for Extend...      1\n",
      "20         Decoupling the Layers in Residual Networks      1\n",
      "21                         Polar Transformer Networks      1\n",
      "22  Enhancing The Reliability of Out-of-distributi...      1\n",
      "23  Stabilizing Adversarial Nets with Prediction M...      1\n",
      "24                           Graph Attention Networks      1\n",
      "25  Minimax Curriculum Learning: Machine Teaching ...      1\n",
      "26  Generalizing Hamiltonian Monte Carlo with Neur...      1\n",
      "27  An Online Learning Approach to Generative Adve...      1\n",
      "28             Improving GANs Using Optimal Transport      1\n",
      "29  The Kanerva Machine: A Generative Distributed ...      1\n",
      "..                                                ...    ...\n",
      "70                 Pointing Out SQL Queries From Text      0\n",
      "71    Achieving morphological agreement with Concorde      0\n",
      "72  Generative Models for Alignment and Data Effic...      0\n",
      "73  Estimation of cross-lingual news similarities ...      0\n",
      "74  Jiffy: A Convolutional Approach to Learning Ti...      0\n",
      "75  Optimizing the Latent Space of Generative Netw...      0\n",
      "76  Real-valued (Medical) Time Series Generation w...      0\n",
      "77  Statestream: A toolbox to explore layerwise-pa...      0\n",
      "78  BinaryFlex: On-the-Fly Kernel Generation in Bi...      0\n",
      "79          ResBinNet: Residual Binary Neural Network      0\n",
      "80                                           No Title      0\n",
      "81  Generalization of Learning using Reservoir Com...      0\n",
      "82  Interpreting Deep Classification Models With B...      0\n",
      "83  Evaluation of generative networks through thei...      0\n",
      "84                                           No Title      0\n",
      "85                                           No Title      0\n",
      "86  MACH: Embarrassingly parallel $K$-class classi...      0\n",
      "87        Code Synthesis with Priority Queue Training      0\n",
      "88                Composable Planning with Attributes      0\n",
      "89                        Neural Task Graph Execution      0\n",
      "90                                           No Title      0\n",
      "91  Realtime query completion via deep language mo...      0\n",
      "92         Learning what to learn in a neural program      0\n",
      "93  Discovery of Predictive Representations With a...      0\n",
      "94  Autostacker: an Automatic Evolutionary Hierarc...      0\n",
      "95              Lifelong Learning with Output Kernels      0\n",
      "96  Network Iterative Learning for Dynamic Deep Ne...      0\n",
      "97  Adversarial Learning for Semi-Supervised Seman...      0\n",
      "98        Deep Asymmetric Multi-task Feature Learning      0\n",
      "99              Feature Map Variational Auto-Encoders      0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "accepted = pd.read_excel('ICLR_accepted.xlsx', index_col=[0])\n",
    "rejected = pd.read_excel('ICLR_rejected.xlsx', index_col=[0])\n",
    "accepted.rename({0: 'name'}, axis = 1, inplace=True)\n",
    "rejected.rename({0: 'name'}, axis = 1, inplace=True)\n",
    "accepted['label'] = 1\n",
    "rejected['label'] = 0\n",
    "\n",
    "df_train = pd.concat([accepted[50:], rejected[50:]]).reset_index().drop('index', axis=1)\n",
    "df_test = pd.concat([accepted[:50], rejected[:50]]).reset_index().drop('index', axis=1)\n",
    "\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "\n",
    "df_train.to_csv('ICLR_train.csv', encoding='utf-8')\n",
    "df_test.to_csv('ICLR_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_only(text):\n",
    "    return \"\".join((char if char.isalpha() else \" \") for char in text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 2422\n",
      "Vector size of Text Vocabulary:  torch.Size([2422, 300])\n",
      "Label Length: 2\n",
      "(tensor([[  34,   60,    8,   78,   67,    2,  719,    3,    1,    1],\n",
      "        [1336,  420,   21,  762,    7, 2347,   42,    1,    1,    1],\n",
      "        [   3,  338,   28,    8, 1922, 1194,    1,    1,    1,    1],\n",
      "        [2343,  131,   26, 1978,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,  273,   25, 1918,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,   13, 1296, 2077,    7,  128,   13,  946, 1732,    2],\n",
      "        [2258,  179,   19,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 101,  187, 1758,  109,    3,    7, 1771,    1,    1,    1],\n",
      "        [1587,  126,    3,    2,  307,  230,   11, 1639,    8,   32],\n",
      "        [   3,  693,   84,    7,   14,  123,    2,  347, 1531,    1],\n",
      "        [ 324,    8, 1645,  146,   25,  794,  171,    7,  294,    9],\n",
      "        [  68,  449,   11, 1668, 2113,    1,    1,    1,    1,    1],\n",
      "        [1042, 1495,   46,    2,   18,    5,    4,    1,    1,    1],\n",
      "        [1897,  105,  145,   21,   70, 1992,    9,   97,  155,    1],\n",
      "        [ 507, 1521,    8,   15,    9,  319,  106,   87,    1,    1],\n",
      "        [2235,    9,   79,  110, 1049, 1054,    5,   31,   73,    1],\n",
      "        [  66,    5,   30,   19,    7,   10,   98, 1060,    1,    1],\n",
      "        [   6,    3,    2,  501, 1957,  460,  508, 2080,  100,    1],\n",
      "        [   6,  450,  411,  424,   60,    1,    1,    1,    1,    1],\n",
      "        [ 659, 1551,  239,    2,   82,    3,    7,   23,    4,    1],\n",
      "        [2320,    9,  985,  199,  548,   19,    2,    6,   17,    3],\n",
      "        [2416,   29, 1535,   73,    2, 1683,  339,    1,    1,    1],\n",
      "        [   3,  350, 1103,  181,    2,  268,  353,  171,    1,    1],\n",
      "        [  58,    8,  353,  456, 1578,   22,  370,  613,    3,    1],\n",
      "        [1467,  688,   43,   27,    2, 1491,   39,    3,    1,    1],\n",
      "        [1276,  916,   11, 2131, 1676,    9,  297,    8,   44,  208],\n",
      "        [1314,  367,   45,   16,    1,    1,    1,    1,    1,    1],\n",
      "        [  68, 2126,  239,   12,   44, 1714,    1,    1,    1,    1],\n",
      "        [ 902,  904,  570,    5,   16,    1,    1,    1,    1,    1],\n",
      "        [2060,   46,    2, 2150, 1787,  113,    1,    1,    1,    1],\n",
      "        [ 116,  336,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 128,    8,    6,   23,    5,   16,   13, 1943,  984, 1436],\n",
      "        [1769,  846,    2,  154,  448,   16,    1,    1,    1,    1],\n",
      "        [ 581, 1225,   12,    6,  443,    4,   24, 1907,  494, 2149],\n",
      "        [1605,   10, 2083,  274,   46,   92,    2,    6,    3,    1],\n",
      "        [1219,   10,  680,   74,    2,  390,    6,    5,   16, 1369],\n",
      "        [1171, 1842,   39,    2, 1488,   49,  186,    1,    1,    1],\n",
      "        [1589,    5,   16,   18,   24,    3,  142,  657,    1,    1],\n",
      "        [2079,    3,   57,  148,   56,  245,    1,    1,    1,    1],\n",
      "        [  10,   86, 2089,  102,   69,    1,    1,    1,    1,    1],\n",
      "        [ 309,  203,    8,  553,    2,   37,  412,   23,    5,    4],\n",
      "        [ 173,  194,   19,    2, 1457,  572, 2210,    1,    1,    1],\n",
      "        [ 325,   15,   14,    4,    1,    1,    1,    1,    1,    1],\n",
      "        [   5,  104,  141,    7,   17,    3,    1,    1,    1,    1],\n",
      "        [ 324,    8, 1432,   38,  113,   25,    3,   13,  990,   12],\n",
      "        [  51,   59,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,  689,   95, 2314,    1,    1,    1,    1,    1,    1],\n",
      "        [  21,   11, 1572,    8, 1371, 1747,  528,    2,   44,   46],\n",
      "        [ 525,   80,   88,  529,    8,  347,  150,    2,  109,    3],\n",
      "        [  51,   59,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [  11, 1135, 1281,   10,   98, 2026,    8,  130,  185,  867],\n",
      "        [  14,   31,    3,  234,  187,    1,    1,    1,    1,    1],\n",
      "        [ 224,    9,  491,  470,   30,   19,    1,    1,    1,    1],\n",
      "        [ 148,   43,    4,    2,   31,  223,    1,    1,    1,    1],\n",
      "        [ 612,    8, 1872,    9, 1462, 2052,    2,  189, 1022,    1],\n",
      "        [ 256, 1938,  157,   12,  231,   28,    8,    5,  786,    1],\n",
      "        [2203,   32,   75,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,  693,  194,  728,    1,    1,    1,    1,    1,    1],\n",
      "        [1752, 1686,   10, 1516,   31,  112,   65,  197,    1,    1],\n",
      "        [2250, 2025,  417,  337, 2263,    7,   91,  417,  144,  381],\n",
      "        [   3,  347,  547,    7,  319,    9, 1722,   64,  108,  220],\n",
      "        [  11, 1546, 2127,    5,  215,  272, 1904,   21,    3,   28],\n",
      "        [2332,   20,    5,    4,    7,   17,    3,    1,    1,    1],\n",
      "        [  21,   11,  442,  277,    8,    6,    5,    4,    1,    1],\n",
      "        [ 717,  142,  184,    8,    6,    4,    1,    1,    1,    1],\n",
      "        [ 744, 1791, 1994, 1017,  170,  825,    2,    6,    5,    4],\n",
      "        [   6,  354,  195,    4,    2,  125,   58,    9,   29,    3],\n",
      "        [ 919,   33,  110, 1543,    3,    2,  552,    6,    4,    1],\n",
      "        [1310,  965,   43,   13,  286,   36,    7,   11,   20, 1267],\n",
      "        [ 194,   15,    5,    4,    1,    1,    1,    1,    1,    1],\n",
      "        [2182,  558,   12,   17,    3,   40,    5,  203,    1,    1],\n",
      "        [1736,  282,   15,   14,    4,    1,    1,    1,    1,    1],\n",
      "        [  21,   11,  442,  277,    8, 1861,  115,    8,    6,    3],\n",
      "        [  10,  618,   43,   16,    2,  697,  111,  127,    1,    1],\n",
      "        [ 156,   97,  155,    2,  505,  106,   87,    1,    1,    1],\n",
      "        [ 385,  644, 1598, 2240,   13,  206,   79, 2269,    1,    1],\n",
      "        [  11,  108,  840,    8,   93,   80,  326,    9,   11,  364],\n",
      "        [1630,   51, 2292,    3,   13, 2043,    2,  808,    9,  404],\n",
      "        [ 182, 1705,   25, 1248,  607,    7,   20,    5,    4,    1],\n",
      "        [ 350,  660,   91,  379,    2,    6,   23,    5,    4,    1],\n",
      "        [2293,   11,  387,   95,    7,   20,  327,    4,    1,    1],\n",
      "        [   3,   10, 2199,  954,   10,  599,  158,   21,   10, 2290],\n",
      "        [  21,   11,   72,  438,    8,  633,   27,  293,    1,    1],\n",
      "        [ 419,    3,   12, 1756,    4,    1,    1,    1,    1,    1],\n",
      "        [  41,  152, 1089,  757,  106,  167,   63,    7,  167,  915],\n",
      "        [ 961,    5,  215,   10, 2375, 2294,  407,  415,    9,  298],\n",
      "        [2107,  120,   86, 1987,   25,   79,   30,   22,   17,    3],\n",
      "        [   5,   31,   73,    7,   53,  124,    8,   38,    9,   81],\n",
      "        [   3,   25, 1025,   50,    2,    6,  828,  186,    1,    1],\n",
      "        [ 409,  255,    6,    5,    4,  755,   78,  234,   10,  103],\n",
      "        [  10,  272,   92,    8,  517,   69,    2,   81,   35,    1],\n",
      "        [1352,    3,  140,    5,   16,  181,   22,   27,  326,    1],\n",
      "        [  33, 1360,    2,  173,   19,    1,    1,    1,    1,    1],\n",
      "        [  11,  277,    8, 1218,    4,    2, 1387,   79,  150,    1],\n",
      "        [  44,  144,  139,    2,  140,   14,  316,    1,    1,    1],\n",
      "        [  41,  808,    6,  121,   29,  316,   96, 1456,   14,  129],\n",
      "        [2007,  687,  214,   62,   57,  405,    1,    1,    1,    1],\n",
      "        [  89, 1884,   19,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [2232,  165,   60,    8, 1075, 2339,    5,    4,    7, 2029],\n",
      "        [ 757,  716,  457,   46,    9, 1241,  276,  509,    1,    1]]), tensor([ 8,  7,  6,  4,  4, 10,  3,  7, 10,  9, 10,  5,  7,  9,  8,  9,  8,  9,\n",
      "         5,  9, 10,  7,  8,  9,  8, 10,  4,  6,  5,  6,  2, 10,  6, 10,  9, 10,\n",
      "         7,  8,  6,  5, 10,  7,  4,  6, 10,  2,  4, 10, 10,  2, 10,  5,  6,  6,\n",
      "         9,  9,  3,  4,  8, 10, 10, 10,  7,  8,  6, 10, 10,  9, 10,  4,  8,  5,\n",
      "        10,  8,  7,  8, 10, 10,  9,  9,  8, 10,  8,  5, 10, 10, 10, 10,  8, 10,\n",
      "         9,  9,  5,  9,  7, 10,  6,  3, 10,  8]))\n"
     ]
    }
   ],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=10)\n",
    "LABEL = data.LabelField()\n",
    "trn_datafields = [(\"id\", None),\n",
    "                    (\"name\", TEXT),\n",
    "                    (\"label\", LABEL)]\n",
    "\n",
    "train_data = TabularDataset(\n",
    "               path=\"./ICLR_train.csv\",\n",
    "               format='csv',\n",
    "               skip_header=True,\n",
    "               fields=trn_datafields)\n",
    "\n",
    "tst_datafields = [(\"id\", None), \n",
    "                 (\"name\", TEXT),\n",
    "                 (\"label\", LABEL)]\n",
    "\n",
    "test_data = TabularDataset(path = \"./ICLR_test.csv\",\n",
    "                    format='csv',\n",
    "                    skip_header=True,\n",
    "                    fields=tst_datafields)\n",
    "#train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=EMBEDDING_LEN))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "train_iter = data.BucketIterator(train_data, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text), repeat=False, shuffle=True)\n",
    "test_iter = data.BucketIterator(test_data, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text), repeat=False, shuffle=False)\n",
    "\n",
    "print(next(iter(train_iter)).name)\n",
    "vocab_size = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size, num_layers=10, bidirectional=True)\n",
    "        \n",
    "        self.label = nn.Linear(20*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):    \n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        h_0 = torch.zeros(20, input_sentences.size()[0], self.hidden_size).cuda()\n",
    "        c_0 = torch.zeros(20, input_sentences.size()[0], self.hidden_size).cuda()\n",
    "        output, (h_n, c_n) = self.lstm(input, (h_0, c_0))\n",
    "\n",
    "        h_n = h_n.permute(1, 0, 2)\n",
    "        h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "        logits = self.label(h_n)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "net = LSTM(2, HIDDEN_SIZE, vocab_size, EMBEDDING_LEN, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00005, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_iter, epoch):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    net.cuda()\n",
    "    steps = 0\n",
    "    net.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.name[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(text)\n",
    "        loss = criterion(prediction, target)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.name[0]\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = net(text)\n",
    "            loss = criterion(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with loss of 0.6883288942850553 training acc of 57.12087895320012 testing acc of 50.0\n",
      "Epoch 1 with loss of 0.6856632782862737 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 2 with loss of 0.6832753007228558 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 3 with loss of 0.6820107560891372 training acc of 57.21978026169997 testing acc of 50.0\n",
      "Epoch 4 with loss of 0.6811694411131052 training acc of 57.36263744647686 testing acc of 50.0\n",
      "Epoch 5 with loss of 0.6831943897100595 training acc of 56.505494631253754 testing acc of 50.0\n",
      "Epoch 6 with loss of 0.6829040279755225 training acc of 56.505494631253754 testing acc of 50.0\n",
      "Epoch 7 with loss of 0.6813893547424903 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 8 with loss of 0.6807663257305439 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 9 with loss of 0.6800905741178073 training acc of 57.21978026169997 testing acc of 50.0\n",
      "Epoch 10 with loss of 0.6794695533238925 training acc of 57.21978026169997 testing acc of 50.0\n",
      "Epoch 11 with loss of 0.682131446324862 training acc of 56.21978026169997 testing acc of 50.0\n",
      "Epoch 12 with loss of 0.6801788440117469 training acc of 56.79120870736929 testing acc of 50.0\n",
      "Epoch 13 with loss of 0.680185460127317 training acc of 56.64835181603065 testing acc of 50.0\n",
      "Epoch 14 with loss of 0.6791340662882879 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 15 with loss of 0.6771475305924048 training acc of 57.505494337815506 testing acc of 50.0\n",
      "Epoch 16 with loss of 0.6778131952652564 training acc of 57.21978026169997 testing acc of 50.0\n",
      "Epoch 17 with loss of 0.6796346123401935 training acc of 56.36263744647686 testing acc of 50.0\n",
      "Epoch 18 with loss of 0.6775555885755099 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 19 with loss of 0.6767984582827642 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 20 with loss of 0.676770398249993 training acc of 56.79120870736929 testing acc of 50.0\n",
      "Epoch 21 with loss of 0.6753265399199265 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 22 with loss of 0.6759207982283372 training acc of 56.79120870736929 testing acc of 50.0\n",
      "Epoch 23 with loss of 0.6755978923577529 training acc of 56.79120870736929 testing acc of 50.0\n",
      "Epoch 24 with loss of 0.6748414589808538 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 25 with loss of 0.675449916949639 training acc of 56.36263744647686 testing acc of 50.0\n",
      "Epoch 26 with loss of 0.6728219710863553 training acc of 57.07692307692308 testing acc of 50.0\n",
      "Epoch 27 with loss of 0.6724793085685143 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 28 with loss of 0.67103223158763 training acc of 57.21978026169997 testing acc of 50.0\n",
      "Epoch 29 with loss of 0.6719217346264765 training acc of 56.93406589214619 testing acc of 50.0\n",
      "Epoch 30 with loss of 0.670373953305758 training acc of 57.36263744647686 testing acc of 50.0\n",
      "Epoch 31 with loss of 0.6710451107758743 training acc of 56.582417708176834 testing acc of 50.0\n",
      "Epoch 32 with loss of 0.670730682519766 training acc of 56.72527489295373 testing acc of 50.0\n",
      "Epoch 33 with loss of 0.668292719584245 training acc of 57.08791204599234 testing acc of 50.0\n",
      "Epoch 34 with loss of 0.6669940994336054 training acc of 57.60439564631535 testing acc of 50.0\n",
      "Epoch 35 with loss of 0.665179179264949 training acc of 57.97802206186148 testing acc of 50.0\n",
      "Epoch 36 with loss of 0.66788617005715 training acc of 57.20879129263071 testing acc of 50.0\n",
      "Epoch 37 with loss of 0.664159527191749 training acc of 58.131868215707634 testing acc of 50.0\n",
      "Epoch 38 with loss of 0.6618783749066867 training acc of 58.48351639967699 testing acc of 50.0\n",
      "Epoch 39 with loss of 0.6616808221890376 training acc of 58.60439564631535 testing acc of 51.0\n",
      "Epoch 40 with loss of 0.6624803588940547 training acc of 58.79120870736929 testing acc of 51.0\n",
      "Epoch 41 with loss of 0.658795966551854 training acc of 59.58241741473858 testing acc of 52.0\n",
      "Epoch 42 with loss of 0.6581959128379822 training acc of 59.29670333862305 testing acc of 51.0\n",
      "Epoch 43 with loss of 0.6579210804058955 training acc of 59.70329666137695 testing acc of 50.0\n",
      "Epoch 44 with loss of 0.6535917887320886 training acc of 60.494505662184494 testing acc of 51.0\n",
      "Epoch 45 with loss of 0.6548524682338421 training acc of 59.956044123722954 testing acc of 50.0\n",
      "Epoch 46 with loss of 0.6481708288192749 training acc of 61.25274716890775 testing acc of 50.0\n",
      "Epoch 47 with loss of 0.6472557737277105 training acc of 61.8021979698768 testing acc of 49.0\n",
      "Epoch 48 with loss of 0.6468362074631911 training acc of 62.36263744647686 testing acc of 51.0\n",
      "Epoch 49 with loss of 0.6431162036382235 training acc of 63.494505662184494 testing acc of 54.0\n",
      "Epoch 50 with loss of 0.6407794998242304 training acc of 62.83516487708459 testing acc of 51.0\n",
      "Epoch 51 with loss of 0.6382000538019034 training acc of 62.120879246638374 testing acc of 54.0\n",
      "Epoch 52 with loss of 0.6379386553397546 training acc of 63.56043947660006 testing acc of 55.0\n",
      "Epoch 53 with loss of 0.629942747262808 training acc of 64.4945056621845 testing acc of 54.0\n",
      "Epoch 54 with loss of 0.6305738412416898 training acc of 63.043956169715294 testing acc of 54.0\n",
      "Epoch 55 with loss of 0.6246440731562101 training acc of 65.84615384615384 testing acc of 57.0\n",
      "Epoch 56 with loss of 0.6233281676585858 training acc of 65.89010972243089 testing acc of 56.0\n",
      "Epoch 57 with loss of 0.6149168381324182 training acc of 67.05494513878456 testing acc of 56.0\n",
      "Epoch 58 with loss of 0.6108164328795213 training acc of 67.32967024583083 testing acc of 57.0\n",
      "Epoch 59 with loss of 0.6097012483156644 training acc of 67.01098896906926 testing acc of 56.0\n",
      "Epoch 60 with loss of 0.6050673035474924 training acc of 68.08791204599234 testing acc of 57.0\n",
      "Epoch 61 with loss of 0.5944725871086121 training acc of 67.92307692307692 testing acc of 58.0\n",
      "Epoch 62 with loss of 0.5971697431344253 training acc of 67.96703309279222 testing acc of 59.0\n",
      "Epoch 63 with loss of 0.5880544598285968 training acc of 68.89010972243089 testing acc of 56.0\n",
      "Epoch 64 with loss of 0.5821718298471891 training acc of 69.36263744647687 testing acc of 57.0\n",
      "Epoch 65 with loss of 0.5734186814381526 training acc of 70.86813178429237 testing acc of 55.0\n",
      "Epoch 66 with loss of 0.5653661306087787 training acc of 70.63736255352313 testing acc of 57.0\n",
      "Epoch 67 with loss of 0.5636303722858429 training acc of 70.5054943378155 testing acc of 59.0\n",
      "Epoch 68 with loss of 0.5538020133972168 training acc of 72.01098926250751 testing acc of 58.0\n",
      "Epoch 69 with loss of 0.5464407205581665 training acc of 71.18681335449219 testing acc of 58.0\n",
      "Epoch 70 with loss of 0.5438202344454252 training acc of 72.28571436955379 testing acc of 60.0\n",
      "Epoch 71 with loss of 0.5340659251579871 training acc of 72.36263744647687 testing acc of 59.0\n",
      "Epoch 72 with loss of 0.5313865519486941 training acc of 73.41758258526141 testing acc of 58.0\n",
      "Epoch 73 with loss of 0.5367189324819125 training acc of 73.27472540048453 testing acc of 59.0\n",
      "Epoch 74 with loss of 0.5229897957581741 training acc of 73.75824180016151 testing acc of 58.0\n",
      "Epoch 75 with loss of 0.5046407396976764 training acc of 75.53846153846153 testing acc of 57.0\n",
      "Epoch 76 with loss of 0.5008681966708257 training acc of 75.48351639967699 testing acc of 59.0\n",
      "Epoch 77 with loss of 0.49351032422139096 training acc of 75.96703279935397 testing acc of 57.0\n",
      "Epoch 78 with loss of 0.49071850914221543 training acc of 76.3956046471229 testing acc of 60.0\n",
      "Epoch 79 with loss of 0.48640265373083263 training acc of 77.12087895320012 testing acc of 54.0\n",
      "Epoch 80 with loss of 0.46822314308239865 training acc of 77.98901073749249 testing acc of 54.0\n",
      "Epoch 81 with loss of 0.4655364935214703 training acc of 78.0989010150616 testing acc of 57.0\n",
      "Epoch 82 with loss of 0.44748358772351193 training acc of 79.2087912926307 testing acc of 57.0\n",
      "Epoch 83 with loss of 0.4421143027452322 training acc of 80.2087912926307 testing acc of 55.0\n",
      "Epoch 84 with loss of 0.43560269245734584 training acc of 81.12087895320012 testing acc of 59.0\n",
      "Epoch 85 with loss of 0.4217328360447517 training acc of 81.27472510704628 testing acc of 56.0\n",
      "Epoch 86 with loss of 0.4168655230448796 training acc of 81.54945080096905 testing acc of 54.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 with loss of 0.41694714243595415 training acc of 80.78022003173828 testing acc of 57.0\n",
      "Epoch 88 with loss of 0.3984848994475145 training acc of 82.73626356858473 testing acc of 60.0\n",
      "Epoch 89 with loss of 0.38690961324251616 training acc of 84.03296720064603 testing acc of 58.0\n",
      "Epoch 90 with loss of 0.38709385578448957 training acc of 84.0989010150616 testing acc of 57.0\n",
      "Epoch 91 with loss of 0.3690621279753171 training acc of 84.5054943378155 testing acc of 57.0\n",
      "Epoch 92 with loss of 0.3622370476906116 training acc of 85.68131842980019 testing acc of 56.0\n",
      "Epoch 93 with loss of 0.343448573580155 training acc of 86.6043953528771 testing acc of 57.0\n",
      "Epoch 94 with loss of 0.34663459887871373 training acc of 86.52747227595403 testing acc of 55.0\n",
      "Epoch 95 with loss of 0.3204261064529419 training acc of 88.0989010150616 testing acc of 57.0\n",
      "Epoch 96 with loss of 0.3107877946816958 training acc of 88.57142873910757 testing acc of 56.0\n",
      "Epoch 97 with loss of 0.29664454322594863 training acc of 89.25274716890775 testing acc of 57.0\n",
      "Epoch 98 with loss of 0.29251828789711 training acc of 89.41758258526141 testing acc of 59.0\n",
      "Epoch 99 with loss of 0.2847594916820526 training acc of 90.16483541635367 testing acc of 60.0\n",
      "Epoch 100 with loss of 0.2937150035913174 training acc of 89.01098926250751 testing acc of 56.0\n",
      "Epoch 101 with loss of 0.26535338507248807 training acc of 90.48351639967699 testing acc of 58.0\n",
      "Epoch 102 with loss of 0.26119520228642684 training acc of 91.26373643141527 testing acc of 58.0\n",
      "Epoch 103 with loss of 0.25835755123541904 training acc of 91.27472510704628 testing acc of 58.0\n",
      "Epoch 104 with loss of 0.24509039062720078 training acc of 92.01098926250751 testing acc of 57.0\n",
      "Epoch 105 with loss of 0.23259021571049324 training acc of 92.54945080096905 testing acc of 55.0\n",
      "Epoch 106 with loss of 0.2430285788499392 training acc of 92.06593381441556 testing acc of 57.0\n",
      "Epoch 107 with loss of 0.24379669817594382 training acc of 92.38461538461539 testing acc of 58.0\n",
      "Epoch 108 with loss of 0.2273067465195289 training acc of 93.12087895320012 testing acc of 58.0\n",
      "Epoch 109 with loss of 0.20431007100985601 training acc of 94.23076923076923 testing acc of 56.0\n",
      "Epoch 110 with loss of 0.20557118723025689 training acc of 93.94505486121544 testing acc of 58.0\n",
      "Epoch 111 with loss of 0.20081818848848343 training acc of 94.24175849327675 testing acc of 57.0\n",
      "Epoch 112 with loss of 0.19242331786797598 training acc of 94.3956046471229 testing acc of 57.0\n",
      "Epoch 113 with loss of 0.19226669749388328 training acc of 94.18681335449219 testing acc of 57.0\n",
      "Epoch 114 with loss of 0.1844481694010588 training acc of 94.78022003173828 testing acc of 57.0\n",
      "Epoch 115 with loss of 0.18490347266197205 training acc of 95.01098926250751 testing acc of 57.0\n",
      "Epoch 116 with loss of 0.18172963651326987 training acc of 94.93406618558444 testing acc of 60.0\n",
      "Epoch 117 with loss of 0.19780538173822257 training acc of 94.08791233943059 testing acc of 57.0\n",
      "Epoch 118 with loss of 0.18218626253879988 training acc of 94.47252772404597 testing acc of 59.0\n",
      "Epoch 119 with loss of 0.20132441016343924 training acc of 93.15384615384616 testing acc of 56.0\n",
      "Epoch 120 with loss of 0.18131677233255827 training acc of 94.95604412372296 testing acc of 56.0\n",
      "Epoch 121 with loss of 0.17754815748104683 training acc of 95.32967024583083 testing acc of 58.0\n",
      "Epoch 122 with loss of 0.17147754075435492 training acc of 95.32967024583083 testing acc of 58.0\n",
      "Epoch 123 with loss of 0.16238811153631943 training acc of 95.53846153846153 testing acc of 58.0\n",
      "Epoch 124 with loss of 0.17782332089084846 training acc of 94.8021979698768 testing acc of 60.0\n",
      "Epoch 125 with loss of 0.19090828528771034 training acc of 94.16483541635367 testing acc of 62.0\n",
      "Epoch 126 with loss of 0.16514683629457766 training acc of 95.08791233943059 testing acc of 59.0\n",
      "Epoch 127 with loss of 0.17086679402452248 training acc of 95.18681335449219 testing acc of 57.0\n",
      "Epoch 128 with loss of 0.15050352594027153 training acc of 96.08791233943059 testing acc of 57.0\n",
      "Epoch 129 with loss of 0.14690365527684873 training acc of 96.16483541635367 testing acc of 58.0\n",
      "Epoch 130 with loss of 0.1407409539589515 training acc of 96.3956046471229 testing acc of 57.0\n",
      "Epoch 131 with loss of 0.13571167507996926 training acc of 96.3956046471229 testing acc of 57.0\n",
      "Epoch 132 with loss of 0.14933388135754144 training acc of 95.97802206186148 testing acc of 58.0\n",
      "Epoch 133 with loss of 0.12706178999864137 training acc of 96.84615384615384 testing acc of 57.0\n",
      "Epoch 134 with loss of 0.1363307762031372 training acc of 96.4945056621845 testing acc of 57.0\n",
      "Epoch 135 with loss of 0.13060146254988816 training acc of 96.7032969548152 testing acc of 57.0\n",
      "Epoch 136 with loss of 0.1368738911472834 training acc of 96.41758258526141 testing acc of 58.0\n",
      "Epoch 137 with loss of 0.1228483531337518 training acc of 96.92307692307692 testing acc of 58.0\n",
      "Epoch 138 with loss of 0.12967501523403022 training acc of 96.71428563044621 testing acc of 57.0\n",
      "Epoch 139 with loss of 0.13556299401590458 training acc of 96.48351639967699 testing acc of 55.0\n",
      "Epoch 140 with loss of 0.15383720140044504 training acc of 95.47252772404597 testing acc of 55.0\n",
      "Epoch 141 with loss of 0.15956519773373237 training acc of 95.13186821570763 testing acc of 56.0\n",
      "Epoch 142 with loss of 0.1557453844982844 training acc of 95.61538461538461 testing acc of 57.0\n",
      "Epoch 143 with loss of 0.15652146190404892 training acc of 95.16483541635367 testing acc of 58.0\n",
      "Epoch 144 with loss of 0.14685891740597212 training acc of 95.3956046471229 testing acc of 58.0\n",
      "Epoch 145 with loss of 0.13155918109875459 training acc of 96.54945080096905 testing acc of 58.0\n",
      "Epoch 146 with loss of 0.13526014639781073 training acc of 96.03296720064603 testing acc of 56.0\n",
      "Epoch 147 with loss of 0.12104120191473228 training acc of 96.92307692307692 testing acc of 56.0\n",
      "Epoch 148 with loss of 0.11411224907407394 training acc of 97.15384615384616 testing acc of 57.0\n",
      "Epoch 149 with loss of 0.11615935827677067 training acc of 96.94505486121544 testing acc of 58.0\n",
      "Epoch 150 with loss of 0.1186011379154829 training acc of 96.94505486121544 testing acc of 58.0\n",
      "Epoch 151 with loss of 0.1121930846801171 training acc of 97.08791233943059 testing acc of 58.0\n",
      "Epoch 152 with loss of 0.11033417857610263 training acc of 97.23076923076923 testing acc of 57.0\n",
      "Epoch 153 with loss of 0.11074313521385193 training acc of 97.3076923076923 testing acc of 58.0\n",
      "Epoch 154 with loss of 0.10835619886907247 training acc of 97.23076923076923 testing acc of 58.0\n",
      "Epoch 155 with loss of 0.10679553664074494 training acc of 97.38461538461539 testing acc of 58.0\n",
      "Epoch 156 with loss of 0.10793660853344661 training acc of 97.3076923076923 testing acc of 57.0\n",
      "Epoch 157 with loss of 0.10976646152826455 training acc of 97.0989010150616 testing acc of 57.0\n",
      "Epoch 158 with loss of 0.11334749583441478 training acc of 97.03296720064603 testing acc of 58.0\n",
      "Epoch 159 with loss of 0.11228649834027657 training acc of 97.02197793813852 testing acc of 57.0\n",
      "Epoch 160 with loss of 0.1097230573113148 training acc of 97.17582409198468 testing acc of 58.0\n",
      "Epoch 161 with loss of 0.09958515889369525 training acc of 97.46153846153847 testing acc of 56.0\n",
      "Epoch 162 with loss of 0.107700884485474 training acc of 97.4065933227539 testing acc of 56.0\n",
      "Epoch 163 with loss of 0.09658471007759754 training acc of 97.6923076923077 testing acc of 57.0\n",
      "Epoch 164 with loss of 0.09591561957047536 training acc of 97.6923076923077 testing acc of 57.0\n",
      "Epoch 165 with loss of 0.09385772708516854 training acc of 97.76923076923077 testing acc of 56.0\n",
      "Epoch 166 with loss of 0.09529617529075879 training acc of 97.62637387789212 testing acc of 56.0\n",
      "Epoch 167 with loss of 0.09701693430542946 training acc of 97.54945080096905 testing acc of 58.0\n",
      "Epoch 168 with loss of 0.09253229616353145 training acc of 97.76923076923077 testing acc of 57.0\n",
      "Epoch 169 with loss of 0.10404989338265015 training acc of 97.15384615384616 testing acc of 56.0\n",
      "Epoch 170 with loss of 0.09966660290956497 training acc of 97.3956046471229 testing acc of 58.0\n",
      "Epoch 171 with loss of 0.10000444991657367 training acc of 97.46153846153847 testing acc of 54.0\n",
      "Epoch 172 with loss of 0.09878733281332713 training acc of 97.4065933227539 testing acc of 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173 with loss of 0.09641509445813987 training acc of 97.62637387789212 testing acc of 55.0\n",
      "Epoch 174 with loss of 0.09458759856911805 training acc of 97.78022003173828 testing acc of 56.0\n",
      "Epoch 175 with loss of 0.08861389197409153 training acc of 97.92307692307692 testing acc of 56.0\n",
      "Epoch 176 with loss of 0.0942507553845644 training acc of 97.62637387789212 testing acc of 58.0\n",
      "Epoch 177 with loss of 0.09262148620417485 training acc of 97.61538461538461 testing acc of 56.0\n",
      "Epoch 178 with loss of 0.09421760101731007 training acc of 97.7032969548152 testing acc of 56.0\n",
      "Epoch 179 with loss of 0.10975637487494029 training acc of 97.01098926250751 testing acc of 56.0\n",
      "Epoch 180 with loss of 0.10787711937267047 training acc of 97.34065950833835 testing acc of 58.0\n",
      "Epoch 181 with loss of 0.09057203064171168 training acc of 97.6923076923077 testing acc of 56.0\n",
      "Epoch 182 with loss of 0.09296266963848701 training acc of 97.7032969548152 testing acc of 56.0\n",
      "Epoch 183 with loss of 0.10406299365254548 training acc of 97.35164818396935 testing acc of 56.0\n",
      "Epoch 184 with loss of 0.0923191227305394 training acc of 97.78022003173828 testing acc of 56.0\n",
      "Epoch 185 with loss of 0.08669865059737976 training acc of 97.92307692307692 testing acc of 56.0\n",
      "Epoch 186 with loss of 0.09089668052127728 training acc of 97.78022003173828 testing acc of 56.0\n",
      "Epoch 187 with loss of 0.09300351630036648 training acc of 97.63736255352313 testing acc of 56.0\n",
      "Epoch 188 with loss of 0.08482851302967621 training acc of 97.84615384615384 testing acc of 56.0\n",
      "Epoch 189 with loss of 0.08511662411575134 training acc of 98.0 testing acc of 57.0\n",
      "Epoch 190 with loss of 0.08844730558876808 training acc of 97.85714310866136 testing acc of 57.0\n",
      "Epoch 191 with loss of 0.08305613968807918 training acc of 98.0 testing acc of 56.0\n",
      "Epoch 192 with loss of 0.08282455830619885 training acc of 98.0 testing acc of 57.0\n",
      "Epoch 193 with loss of 0.08570488656942661 training acc of 97.93406618558444 testing acc of 59.0\n",
      "Epoch 194 with loss of 0.08423164386588794 training acc of 97.93406618558444 testing acc of 59.0\n",
      "Epoch 195 with loss of 0.07986691995308949 training acc of 98.07692307692308 testing acc of 59.0\n",
      "Epoch 196 with loss of 0.08691602572798729 training acc of 97.62637387789212 testing acc of 59.0\n",
      "Epoch 197 with loss of 0.14348584083983532 training acc of 95.86813178429237 testing acc of 57.0\n",
      "Epoch 198 with loss of 0.1993814712533584 training acc of 93.4065933227539 testing acc of 58.0\n",
      "Epoch 199 with loss of 0.14986895827146676 training acc of 95.3076923076923 testing acc of 55.0\n",
      "Epoch 200 with loss of 0.14607150078966066 training acc of 95.02197793813852 testing acc of 54.0\n",
      "Epoch 201 with loss of 0.10473094651332268 training acc of 97.08791233943059 testing acc of 57.0\n",
      "Epoch 202 with loss of 0.09943974992403617 training acc of 97.4065933227539 testing acc of 57.0\n",
      "Epoch 203 with loss of 0.08682500370419942 training acc of 98.0 testing acc of 58.0\n",
      "Epoch 204 with loss of 0.0859281041014653 training acc of 97.7032969548152 testing acc of 58.0\n",
      "Epoch 205 with loss of 0.0807557376818015 training acc of 98.07692307692308 testing acc of 57.0\n",
      "Epoch 206 with loss of 0.07895260999122491 training acc of 98.07692307692308 testing acc of 58.0\n",
      "Epoch 207 with loss of 0.07853115607912724 training acc of 98.07692307692308 testing acc of 59.0\n",
      "Epoch 208 with loss of 0.08803770118034802 training acc of 97.7912087073693 testing acc of 59.0\n",
      "Epoch 209 with loss of 0.08117057497684772 training acc of 97.93406618558444 testing acc of 58.0\n",
      "Epoch 210 with loss of 0.07653032329220039 training acc of 98.07692307692308 testing acc of 59.0\n",
      "Epoch 211 with loss of 0.07534455894850768 training acc of 98.15384615384616 testing acc of 58.0\n",
      "Epoch 212 with loss of 0.07442940585315228 training acc of 98.07692307692308 testing acc of 58.0\n",
      "Epoch 213 with loss of 0.08486490295483516 training acc of 97.8021979698768 testing acc of 58.0\n",
      "Epoch 214 with loss of 0.07071637863723132 training acc of 98.23076923076923 testing acc of 58.0\n",
      "Epoch 215 with loss of 0.07392957620322704 training acc of 98.24175849327675 testing acc of 58.0\n",
      "Epoch 216 with loss of 0.07254611829725596 training acc of 98.24175849327675 testing acc of 58.0\n",
      "Epoch 217 with loss of 0.06807986976435551 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 218 with loss of 0.06753407655140528 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 219 with loss of 0.06960795834087409 training acc of 98.16483541635367 testing acc of 57.0\n",
      "Epoch 220 with loss of 0.06654510417809853 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 221 with loss of 0.0718571489246992 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 222 with loss of 0.06596571486443281 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 223 with loss of 0.0741509203440868 training acc of 98.17582409198468 testing acc of 57.0\n",
      "Epoch 224 with loss of 0.06572556452682385 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 225 with loss of 0.07195095223589586 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 226 with loss of 0.06534527285167804 training acc of 98.46153846153847 testing acc of 57.0\n",
      "Epoch 227 with loss of 0.07004439171690208 training acc of 98.3956046471229 testing acc of 57.0\n",
      "Epoch 228 with loss of 0.06852592886067353 training acc of 98.3956046471229 testing acc of 57.0\n",
      "Epoch 229 with loss of 0.06813545754322639 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 230 with loss of 0.06413032558674996 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 231 with loss of 0.06331393710122658 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 232 with loss of 0.06618148152931379 training acc of 98.3956046471229 testing acc of 56.0\n",
      "Epoch 233 with loss of 0.07358846018234125 training acc of 98.16483541635367 testing acc of 55.0\n",
      "Epoch 234 with loss of 0.10658409661398484 training acc of 96.95604412372296 testing acc of 59.0\n",
      "Epoch 235 with loss of 0.0837914187174577 training acc of 97.76923076923077 testing acc of 55.0\n",
      "Epoch 236 with loss of 0.07520764693617821 training acc of 98.23076923076923 testing acc of 57.0\n",
      "Epoch 237 with loss of 0.07888715685560153 training acc of 97.85714310866136 testing acc of 58.0\n",
      "Epoch 238 with loss of 0.07359787126859793 training acc of 98.07692307692308 testing acc of 58.0\n",
      "Epoch 239 with loss of 0.0712704721551675 training acc of 98.16483541635367 testing acc of 57.0\n",
      "Epoch 240 with loss of 0.07721565269793455 training acc of 97.94505486121544 testing acc of 58.0\n",
      "Epoch 241 with loss of 0.06884544500364707 training acc of 98.24175849327675 testing acc of 58.0\n",
      "Epoch 242 with loss of 0.06371020697630368 training acc of 98.53846153846153 testing acc of 59.0\n",
      "Epoch 243 with loss of 0.06839559929302105 training acc of 98.3956046471229 testing acc of 57.0\n",
      "Epoch 244 with loss of 0.0706475623525106 training acc of 98.25274716890775 testing acc of 59.0\n",
      "Epoch 245 with loss of 0.06358502583148387 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 246 with loss of 0.06307979845083676 training acc of 98.46153846153847 testing acc of 58.0\n",
      "Epoch 247 with loss of 0.0706350844926559 training acc of 98.25274716890775 testing acc of 57.0\n",
      "Epoch 248 with loss of 0.06700783753051208 training acc of 98.3956046471229 testing acc of 58.0\n",
      "Epoch 249 with loss of 0.06315160693170932 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 250 with loss of 0.06240455290445915 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 251 with loss of 0.06201667582186369 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 252 with loss of 0.06125823565973686 training acc of 98.53846153846153 testing acc of 57.0\n",
      "Epoch 253 with loss of 0.06568959584602943 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 254 with loss of 0.06816555144122013 training acc of 98.24175849327675 testing acc of 57.0\n",
      "Epoch 255 with loss of 0.06322669180539939 training acc of 98.38461538461539 testing acc of 58.0\n",
      "Epoch 256 with loss of 0.0693252428363149 training acc of 98.01098926250751 testing acc of 59.0\n",
      "Epoch 257 with loss of 0.0704927326251681 training acc of 98.07692307692308 testing acc of 58.0\n",
      "Epoch 258 with loss of 0.06535311372807392 training acc of 98.38461538461539 testing acc of 56.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259 with loss of 0.06840534837773213 training acc of 98.24175849327675 testing acc of 56.0\n",
      "Epoch 260 with loss of 0.06544812503629006 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 261 with loss of 0.06439357788230364 training acc of 98.47252772404597 testing acc of 57.0\n",
      "Epoch 262 with loss of 0.06509204022586346 training acc of 98.3956046471229 testing acc of 58.0\n",
      "Epoch 263 with loss of 0.06412443704903126 training acc of 98.3956046471229 testing acc of 54.0\n",
      "Epoch 264 with loss of 0.06598654367889349 training acc of 98.16483541635367 testing acc of 59.0\n",
      "Epoch 265 with loss of 0.07665742432268766 training acc of 97.92307692307692 testing acc of 59.0\n",
      "Epoch 266 with loss of 0.07704077001947623 training acc of 97.71428563044621 testing acc of 55.0\n",
      "Epoch 267 with loss of 0.059353580411810145 training acc of 98.53846153846153 testing acc of 60.0\n",
      "Epoch 268 with loss of 0.0616643105705197 training acc of 98.47252772404597 testing acc of 55.0\n",
      "Epoch 269 with loss of 0.06149768643081188 training acc of 98.47252772404597 testing acc of 56.0\n",
      "Epoch 270 with loss of 0.05720487817262228 training acc of 98.61538461538461 testing acc of 56.0\n",
      "Epoch 271 with loss of 0.05646150989028124 training acc of 98.6923076923077 testing acc of 58.0\n",
      "Epoch 272 with loss of 0.06152874611031551 training acc of 98.54945080096905 testing acc of 56.0\n",
      "Epoch 273 with loss of 0.05957598518580198 training acc of 98.54945080096905 testing acc of 57.0\n",
      "Epoch 274 with loss of 0.06276922536870608 training acc of 98.54945080096905 testing acc of 56.0\n",
      "Epoch 275 with loss of 0.06056238123430656 training acc of 98.47252772404597 testing acc of 58.0\n",
      "Epoch 276 with loss of 0.05540249016709053 training acc of 98.6923076923077 testing acc of 58.0\n",
      "Epoch 277 with loss of 0.054881324203541644 training acc of 98.6923076923077 testing acc of 58.0\n",
      "Epoch 278 with loss of 0.056094229221343994 training acc of 98.6923076923077 testing acc of 57.0\n",
      "Epoch 279 with loss of 0.05962006747722626 training acc of 98.54945080096905 testing acc of 57.0\n",
      "Epoch 280 with loss of 0.05849836960148353 training acc of 98.62637387789212 testing acc of 56.0\n",
      "Epoch 281 with loss of 0.060101469119007774 training acc of 98.3956046471229 testing acc of 57.0\n",
      "Epoch 282 with loss of 0.06296650184175143 training acc of 98.31868157019981 testing acc of 57.0\n",
      "Epoch 283 with loss of 0.06107651234532778 training acc of 98.38461538461539 testing acc of 59.0\n",
      "Epoch 284 with loss of 0.05521016066464094 training acc of 98.61538461538461 testing acc of 57.0\n",
      "Epoch 285 with loss of 0.056395373557909176 training acc of 98.61538461538461 testing acc of 58.0\n",
      "Epoch 286 with loss of 0.05388195196596476 training acc of 98.6923076923077 testing acc of 56.0\n",
      "Epoch 287 with loss of 0.06279786131702937 training acc of 98.48351639967699 testing acc of 57.0\n",
      "Epoch 288 with loss of 0.05126134829165844 training acc of 98.84615384615384 testing acc of 57.0\n",
      "Epoch 289 with loss of 0.04882917316773763 training acc of 98.84615384615384 testing acc of 57.0\n",
      "Epoch 290 with loss of 0.057768878097144455 training acc of 98.54945080096905 testing acc of 57.0\n",
      "Epoch 291 with loss of 0.048315311495501265 training acc of 98.92307692307692 testing acc of 57.0\n",
      "Epoch 292 with loss of 0.06653377170173022 training acc of 98.24175849327675 testing acc of 57.0\n",
      "Epoch 293 with loss of 0.05978491472510191 training acc of 98.54945080096905 testing acc of 59.0\n",
      "Epoch 294 with loss of 0.061359801616233126 training acc of 98.48351639967699 testing acc of 57.0\n",
      "Epoch 295 with loss of 0.051745513310799233 training acc of 98.6923076923077 testing acc of 57.0\n",
      "Epoch 296 with loss of 0.056075177871837065 training acc of 98.62637387789212 testing acc of 58.0\n",
      "Epoch 297 with loss of 0.05840184826117296 training acc of 98.56043947660007 testing acc of 56.0\n",
      "Epoch 298 with loss of 0.045100122260359615 training acc of 98.92307692307692 testing acc of 56.0\n",
      "Epoch 299 with loss of 0.04767622949125675 training acc of 98.84615384615384 testing acc of 56.0\n",
      "Epoch 300 with loss of 0.04734728621462217 training acc of 98.84615384615384 testing acc of 57.0\n",
      "Epoch 301 with loss of 0.04448549134226946 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 302 with loss of 0.0455648905525987 training acc of 98.92307692307692 testing acc of 57.0\n",
      "Epoch 303 with loss of 0.04281012291231981 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 304 with loss of 0.04238045588135719 training acc of 99.0 testing acc of 55.0\n",
      "Epoch 305 with loss of 0.045185144095180124 training acc of 98.92307692307692 testing acc of 56.0\n",
      "Epoch 306 with loss of 0.048576926275228076 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 307 with loss of 0.04401104481747517 training acc of 98.85714310866136 testing acc of 55.0\n",
      "Epoch 308 with loss of 0.05095016540816197 training acc of 98.71428563044621 testing acc of 55.0\n",
      "Epoch 309 with loss of 0.042145919806968704 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 310 with loss of 0.040711688092694834 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 311 with loss of 0.04071830155757757 training acc of 99.0 testing acc of 55.0\n",
      "Epoch 312 with loss of 0.046438383153424814 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 313 with loss of 0.045638219166833624 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 314 with loss of 0.04089742254179258 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 315 with loss of 0.040744202283139415 training acc of 98.92307692307692 testing acc of 53.0\n",
      "Epoch 316 with loss of 0.04503428696009975 training acc of 98.93406618558444 testing acc of 54.0\n",
      "Epoch 317 with loss of 0.039808986780162044 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 318 with loss of 0.04155565042478534 training acc of 98.92307692307692 testing acc of 53.0\n",
      "Epoch 319 with loss of 0.03999389937290779 training acc of 99.0 testing acc of 56.0\n",
      "Epoch 320 with loss of 0.040066430560098246 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 321 with loss of 0.03913015291954462 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 322 with loss of 0.0383626612691352 training acc of 99.15384615384616 testing acc of 53.0\n",
      "Epoch 323 with loss of 0.04037090570021134 training acc of 98.93406618558444 testing acc of 56.0\n",
      "Epoch 324 with loss of 0.03939854016957375 training acc of 99.0 testing acc of 54.0\n",
      "Epoch 325 with loss of 0.03853039755127751 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 326 with loss of 0.044784159471209235 training acc of 99.01098926250751 testing acc of 53.0\n",
      "Epoch 327 with loss of 0.03764725114720372 training acc of 99.15384615384616 testing acc of 53.0\n",
      "Epoch 328 with loss of 0.03790607310544986 training acc of 99.15384615384616 testing acc of 53.0\n",
      "Epoch 329 with loss of 0.03872390603646636 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 330 with loss of 0.03793197820106378 training acc of 99.15384615384616 testing acc of 53.0\n",
      "Epoch 331 with loss of 0.03801177416999753 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 332 with loss of 0.038409870046262555 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 333 with loss of 0.0391769170976029 training acc of 99.07692307692308 testing acc of 53.0\n",
      "Epoch 334 with loss of 0.03786269514463269 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 335 with loss of 0.04290985409170389 training acc of 98.93406618558444 testing acc of 52.0\n",
      "Epoch 336 with loss of 0.03724248413569652 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 337 with loss of 0.037884861015929625 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 338 with loss of 0.037002360806442224 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 339 with loss of 0.036864276688832506 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 340 with loss of 0.039889119243106015 training acc of 98.71428563044621 testing acc of 53.0\n",
      "Epoch 341 with loss of 0.04069009362361752 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 342 with loss of 0.03845244130262962 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 343 with loss of 0.03733866181797706 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 344 with loss of 0.04307949055845921 training acc of 99.01098926250751 testing acc of 53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345 with loss of 0.03720110278719893 training acc of 99.0 testing acc of 56.0\n",
      "Epoch 346 with loss of 0.037576193240686104 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 347 with loss of 0.038829833949701145 training acc of 99.01098926250751 testing acc of 54.0\n",
      "Epoch 348 with loss of 0.03767031334483853 training acc of 99.0 testing acc of 56.0\n",
      "Epoch 349 with loss of 0.03717079404025124 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 350 with loss of 0.03884756744194489 training acc of 99.0 testing acc of 54.0\n",
      "Epoch 351 with loss of 0.037455748981581286 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 352 with loss of 0.03743913031827945 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 353 with loss of 0.037034482086220614 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 354 with loss of 0.03663887642323971 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 355 with loss of 0.036232065266141526 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 356 with loss of 0.03630583647351999 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 357 with loss of 0.036446346972997375 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 358 with loss of 0.042468489720844306 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 359 with loss of 0.03631042760725205 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 360 with loss of 0.04291312418018396 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 361 with loss of 0.037068841011764914 training acc of 99.07692307692308 testing acc of 54.0\n",
      "Epoch 362 with loss of 0.036652570948577844 training acc of 99.07692307692308 testing acc of 54.0\n",
      "Epoch 363 with loss of 0.03704282974537749 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 364 with loss of 0.036951635140352525 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 365 with loss of 0.03811572775101432 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 366 with loss of 0.03596737006535897 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 367 with loss of 0.04126796738889355 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 368 with loss of 0.03673009595905359 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 369 with loss of 0.052143546394430675 training acc of 98.72527489295372 testing acc of 57.0\n",
      "Epoch 370 with loss of 0.04426204441831662 training acc of 98.78022003173828 testing acc of 57.0\n",
      "Epoch 371 with loss of 0.053795900912239 training acc of 98.38461538461539 testing acc of 58.0\n",
      "Epoch 372 with loss of 0.14744233006898028 training acc of 96.47252772404597 testing acc of 55.0\n",
      "Epoch 373 with loss of 0.1918067912069651 training acc of 94.63736255352313 testing acc of 54.0\n",
      "Epoch 374 with loss of 0.0931164430311093 training acc of 97.07692307692308 testing acc of 56.0\n",
      "Epoch 375 with loss of 0.07713677275639313 training acc of 97.7032969548152 testing acc of 56.0\n",
      "Epoch 376 with loss of 0.0586534716332188 training acc of 98.61538461538461 testing acc of 55.0\n",
      "Epoch 377 with loss of 0.053502729162573814 training acc of 98.48351639967699 testing acc of 54.0\n",
      "Epoch 378 with loss of 0.04866109602153301 training acc of 98.78022003173828 testing acc of 54.0\n",
      "Epoch 379 with loss of 0.04185681568028835 training acc of 98.92307692307692 testing acc of 55.0\n",
      "Epoch 380 with loss of 0.03965364475376331 training acc of 98.92307692307692 testing acc of 53.0\n",
      "Epoch 381 with loss of 0.03874595742672682 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 382 with loss of 0.03775527356908871 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 383 with loss of 0.03711150597351102 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 384 with loss of 0.03704969778370399 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 385 with loss of 0.036303682503505394 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 386 with loss of 0.03647519673149173 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 387 with loss of 0.047939465631945774 training acc of 98.86813178429237 testing acc of 54.0\n",
      "Epoch 388 with loss of 0.037328750683138005 training acc of 99.07692307692308 testing acc of 54.0\n",
      "Epoch 389 with loss of 0.04511698629133976 training acc of 98.86813178429237 testing acc of 56.0\n",
      "Epoch 390 with loss of 0.035529081064921156 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 391 with loss of 0.03617789965266219 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 392 with loss of 0.03501164264833698 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 393 with loss of 0.03979449346661568 training acc of 99.01098926250751 testing acc of 55.0\n",
      "Epoch 394 with loss of 0.040141265087116226 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 395 with loss of 0.03491974586191086 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 396 with loss of 0.03471877268300606 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 397 with loss of 0.03474064848314111 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 398 with loss of 0.03481012751133396 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 399 with loss of 0.03435937555220265 training acc of 99.15384615384616 testing acc of 55.0\n",
      "Epoch 400 with loss of 0.037082864258151785 training acc of 99.01098926250751 testing acc of 57.0\n",
      "Epoch 401 with loss of 0.03502607102004381 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 402 with loss of 0.03519092758114521 training acc of 99.0 testing acc of 56.0\n",
      "Epoch 403 with loss of 0.040722512353498205 training acc of 99.01098926250751 testing acc of 55.0\n",
      "Epoch 404 with loss of 0.034085157757195145 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 405 with loss of 0.04019247188877601 training acc of 98.85714310866136 testing acc of 57.0\n",
      "Epoch 406 with loss of 0.03471566647147903 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 407 with loss of 0.03347208354478845 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 408 with loss of 0.034615164873405144 training acc of 99.07692307692308 testing acc of 54.0\n",
      "Epoch 409 with loss of 0.03455636087948313 training acc of 99.15384615384616 testing acc of 54.0\n",
      "Epoch 410 with loss of 0.034137642405067496 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 411 with loss of 0.03420383171536601 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 412 with loss of 0.03393022082029627 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 413 with loss of 0.03791645269554395 training acc of 99.01098926250751 testing acc of 57.0\n",
      "Epoch 414 with loss of 0.03366501880093263 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 415 with loss of 0.0384642409447294 training acc of 99.01098926250751 testing acc of 55.0\n",
      "Epoch 416 with loss of 0.03492400146877536 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 417 with loss of 0.03785793092818214 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 418 with loss of 0.04290910606057598 training acc of 98.7912087073693 testing acc of 57.0\n",
      "Epoch 419 with loss of 0.03301296458364679 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 420 with loss of 0.033468350338248104 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 421 with loss of 0.033410234878269524 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 422 with loss of 0.03434581531641575 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 423 with loss of 0.033106908560372315 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 424 with loss of 0.03757615032820748 training acc of 99.01098926250751 testing acc of 57.0\n",
      "Epoch 425 with loss of 0.0337621601155171 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 426 with loss of 0.03825221114003888 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 427 with loss of 0.03366582389347828 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 428 with loss of 0.03285345854237676 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 429 with loss of 0.03252041565540891 training acc of 99.15384615384616 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430 with loss of 0.032352433181726016 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 431 with loss of 0.0326321146164376 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 432 with loss of 0.03238907033720842 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 433 with loss of 0.033323044082722984 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 434 with loss of 0.032654388037581854 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 435 with loss of 0.031685917566602044 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 436 with loss of 0.03264201511270725 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 437 with loss of 0.037024607380422264 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 438 with loss of 0.032326920387836605 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 439 with loss of 0.032027237045650296 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 440 with loss of 0.03201580566998858 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 441 with loss of 0.03154815300009572 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 442 with loss of 0.035888637559345134 training acc of 98.93406618558444 testing acc of 57.0\n",
      "Epoch 443 with loss of 0.03524769015180377 training acc of 98.93406618558444 testing acc of 57.0\n",
      "Epoch 444 with loss of 0.03252726270315739 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 445 with loss of 0.031626918353140354 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 446 with loss of 0.03134679991322068 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 447 with loss of 0.034537308276272737 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 448 with loss of 0.03146156597022827 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 449 with loss of 0.03266010766562361 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 450 with loss of 0.032219999541456885 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 451 with loss of 0.03156375569792894 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 452 with loss of 0.03426219390418667 training acc of 98.93406618558444 testing acc of 58.0\n",
      "Epoch 453 with loss of 0.03654261230706023 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 454 with loss of 0.03564135056848709 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 455 with loss of 0.03010616232999242 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 456 with loss of 0.033353401276354604 training acc of 98.92307692307692 testing acc of 55.0\n",
      "Epoch 457 with loss of 0.03263349447829219 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 458 with loss of 0.030282151348029192 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 459 with loss of 0.029493194771930575 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 460 with loss of 0.030221361201256514 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 461 with loss of 0.033620099716175064 training acc of 99.01098926250751 testing acc of 57.0\n",
      "Epoch 462 with loss of 0.029628623133668534 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 463 with loss of 0.029917969690779082 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 464 with loss of 0.029262316055022754 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 465 with loss of 0.029995973096587338 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 466 with loss of 0.02951938667907738 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 467 with loss of 0.029810325206758883 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 468 with loss of 0.032271393503134065 training acc of 98.85714310866136 testing acc of 57.0\n",
      "Epoch 469 with loss of 0.029773212145440854 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 470 with loss of 0.030817377345206644 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 471 with loss of 0.029424680038713492 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 472 with loss of 0.028850763427236907 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 473 with loss of 0.028540560366729133 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 474 with loss of 0.02857987951630583 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 475 with loss of 0.029345455758560162 training acc of 99.0 testing acc of 55.0\n",
      "Epoch 476 with loss of 0.029560118555449523 training acc of 99.0 testing acc of 59.0\n",
      "Epoch 477 with loss of 0.03134154728971995 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 478 with loss of 0.029300276548243485 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 479 with loss of 0.02801145537971304 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 480 with loss of 0.029733971656801608 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 481 with loss of 0.0280104332531874 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 482 with loss of 0.033932793455628246 training acc of 98.93406618558444 testing acc of 58.0\n",
      "Epoch 483 with loss of 0.028249649975735407 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 484 with loss of 0.02776091671190583 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 485 with loss of 0.027032531547145203 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 486 with loss of 0.03048268587400134 training acc of 98.93406618558444 testing acc of 58.0\n",
      "Epoch 487 with loss of 0.02786089813050169 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 488 with loss of 0.029411347797856882 training acc of 98.85714310866136 testing acc of 55.0\n",
      "Epoch 489 with loss of 0.027779174252198294 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 490 with loss of 0.0306778486030033 training acc of 98.93406618558444 testing acc of 55.0\n",
      "Epoch 491 with loss of 0.02735842359610475 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 492 with loss of 0.02556554497613643 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 493 with loss of 0.027289484555904683 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 494 with loss of 0.027189254527911544 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 495 with loss of 0.026772879243183594 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 496 with loss of 0.027600387970988568 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 497 with loss of 0.02695932497198765 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 498 with loss of 0.025400513460716374 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 499 with loss of 0.027206730742294054 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 500 with loss of 0.025061029421452146 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 501 with loss of 0.028375940612302378 training acc of 98.85714310866136 testing acc of 59.0\n",
      "Epoch 502 with loss of 0.028940071530926686 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 503 with loss of 0.025039298030046318 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 504 with loss of 0.024633059922892313 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 505 with loss of 0.026095536501648333 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 506 with loss of 0.026703837435119428 training acc of 98.93406618558444 testing acc of 56.0\n",
      "Epoch 507 with loss of 0.025466065782193955 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 508 with loss of 0.025236918196941797 training acc of 99.07692307692308 testing acc of 56.0\n",
      "Epoch 509 with loss of 0.024603710903857764 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 510 with loss of 0.03459365639047554 training acc of 98.86813178429237 testing acc of 55.0\n",
      "Epoch 511 with loss of 0.023349594593477938 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 512 with loss of 0.032172838118500434 training acc of 98.7032969548152 testing acc of 57.0\n",
      "Epoch 513 with loss of 0.04648428675360405 training acc of 98.38461538461539 testing acc of 58.0\n",
      "Epoch 514 with loss of 0.047344542609957546 training acc of 98.24175849327675 testing acc of 57.0\n",
      "Epoch 515 with loss of 0.04309401281464558 training acc of 98.62637387789212 testing acc of 55.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516 with loss of 0.051095911946434244 training acc of 98.07692307692308 testing acc of 56.0\n",
      "Epoch 517 with loss of 0.04248158490428558 training acc of 98.61538461538461 testing acc of 56.0\n",
      "Epoch 518 with loss of 0.03526501149798815 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 519 with loss of 0.029387642772724994 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 520 with loss of 0.028577887882980015 training acc of 99.01098926250751 testing acc of 55.0\n",
      "Epoch 521 with loss of 0.026894914237065956 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 522 with loss of 0.02610813915872803 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 523 with loss of 0.025178599207160566 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 524 with loss of 0.02485732239885972 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 525 with loss of 0.024246074689122345 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 526 with loss of 0.023648673972974602 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 527 with loss of 0.02496794041676017 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 528 with loss of 0.0261465917663792 training acc of 99.15384615384616 testing acc of 56.0\n",
      "Epoch 529 with loss of 0.024791060534950633 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 530 with loss of 0.026293080873214282 training acc of 99.01098926250751 testing acc of 59.0\n",
      "Epoch 531 with loss of 0.022934442565131646 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 532 with loss of 0.02318367311874261 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 533 with loss of 0.024630123579229873 training acc of 99.01098926250751 testing acc of 59.0\n",
      "Epoch 534 with loss of 0.02310257376386569 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 535 with loss of 0.025515795309239857 training acc of 99.01098926250751 testing acc of 58.0\n",
      "Epoch 536 with loss of 0.023294329625339463 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 537 with loss of 0.02393656077149969 training acc of 98.93406618558444 testing acc of 56.0\n",
      "Epoch 538 with loss of 0.02409793918307584 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 539 with loss of 0.022968551609665155 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 540 with loss of 0.02436782201178945 training acc of 99.01098926250751 testing acc of 56.0\n",
      "Epoch 541 with loss of 0.022523958880740862 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 542 with loss of 0.022571648918808654 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 543 with loss of 0.02307284028770832 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 544 with loss of 0.024131293730953567 training acc of 98.93406618558444 testing acc of 58.0\n",
      "Epoch 545 with loss of 0.02178840093816129 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 546 with loss of 0.02279516631209005 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 547 with loss of 0.023409729787650015 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 548 with loss of 0.024579322896897793 training acc of 98.93406618558444 testing acc of 60.0\n",
      "Epoch 549 with loss of 0.021354580620447032 training acc of 99.15384615384616 testing acc of 60.0\n",
      "Epoch 550 with loss of 0.02097722218157007 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 551 with loss of 0.020629339869349048 training acc of 99.0 testing acc of 59.0\n",
      "Epoch 552 with loss of 0.02447395159218174 training acc of 98.93406618558444 testing acc of 59.0\n",
      "Epoch 553 with loss of 0.020866970361496966 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 554 with loss of 0.02417683441979954 training acc of 98.93406618558444 testing acc of 59.0\n",
      "Epoch 555 with loss of 0.024452560115605593 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 556 with loss of 0.024403449900161762 training acc of 99.0 testing acc of 58.0\n",
      "Epoch 557 with loss of 0.02180065354332328 training acc of 99.23076923076923 testing acc of 56.0\n",
      "Epoch 558 with loss of 0.021133648007749938 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 559 with loss of 0.021731359463256713 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 560 with loss of 0.023303687841129992 training acc of 98.93406618558444 testing acc of 59.0\n",
      "Epoch 561 with loss of 0.023023921244132977 training acc of 98.93406618558444 testing acc of 59.0\n",
      "Epoch 562 with loss of 0.020063941635621283 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 563 with loss of 0.02111539035104215 training acc of 98.85714310866136 testing acc of 59.0\n",
      "Epoch 564 with loss of 0.019300606316671923 training acc of 99.15384615384616 testing acc of 57.0\n",
      "Epoch 565 with loss of 0.027247947909367774 training acc of 98.93406618558444 testing acc of 58.0\n",
      "Epoch 566 with loss of 0.028287578087586623 training acc of 98.76923076923077 testing acc of 57.0\n",
      "Epoch 567 with loss of 0.024835040130151007 training acc of 99.0 testing acc of 56.0\n",
      "Epoch 568 with loss of 0.022845119166259583 training acc of 98.92307692307692 testing acc of 57.0\n",
      "Epoch 569 with loss of 0.020295793942820568 training acc of 99.07692307692308 testing acc of 58.0\n",
      "Epoch 570 with loss of 0.019458684792670492 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 571 with loss of 0.019070743726423152 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 572 with loss of 0.018844686564989388 training acc of 99.0 testing acc of 57.0\n",
      "Epoch 573 with loss of 0.020841353572905064 training acc of 99.15384615384616 testing acc of 60.0\n",
      "Epoch 574 with loss of 0.019969604421371166 training acc of 99.07692307692308 testing acc of 55.0\n",
      "Epoch 575 with loss of 0.018767287130825795 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 576 with loss of 0.019146371014917698 training acc of 99.07692307692308 testing acc of 59.0\n",
      "Epoch 577 with loss of 0.02832001719910365 training acc of 98.93406618558444 testing acc of 56.0\n",
      "Epoch 578 with loss of 0.017856008530473515 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 579 with loss of 0.020528919314249203 training acc of 98.93406618558444 testing acc of 57.0\n",
      "Epoch 580 with loss of 0.01765095044930394 training acc of 99.23076923076923 testing acc of 58.0\n",
      "Epoch 581 with loss of 0.019035866716876626 training acc of 99.01098926250751 testing acc of 57.0\n",
      "Epoch 582 with loss of 0.017184969731785644 training acc of 99.15384615384616 testing acc of 58.0\n",
      "Epoch 583 with loss of 0.018446435327999867 training acc of 99.23076923076923 testing acc of 58.0\n",
      "Epoch 584 with loss of 0.019611830429102365 training acc of 99.07692307692308 testing acc of 57.0\n",
      "Epoch 585 with loss of 0.01893960167832959 training acc of 99.38461538461539 testing acc of 55.0\n",
      "Epoch 586 with loss of 0.019871449742752772 training acc of 99.01098926250751 testing acc of 59.0\n",
      "Epoch 587 with loss of 0.019729851709248927 training acc of 99.15384615384616 testing acc of 59.0\n",
      "Epoch 588 with loss of 0.018478902352329057 training acc of 99.23076923076923 testing acc of 55.0\n",
      "Epoch 589 with loss of 0.02072220281339609 training acc of 99.08791233943059 testing acc of 56.0\n",
      "Epoch 590 with loss of 0.02315093533923993 training acc of 99.08791233943059 testing acc of 55.0\n",
      "Epoch 591 with loss of 0.01918036214864025 training acc of 99.16483541635367 testing acc of 59.0\n",
      "Epoch 592 with loss of 0.021940497896419123 training acc of 99.01098926250751 testing acc of 55.0\n",
      "Epoch 593 with loss of 0.03274323043297045 training acc of 98.84615384615384 testing acc of 53.0\n",
      "Epoch 594 with loss of 0.052508494189868755 training acc of 98.31868157019981 testing acc of 55.0\n",
      "Epoch 595 with loss of 0.038268207447030224 training acc of 98.84615384615384 testing acc of 58.0\n",
      "Epoch 596 with loss of 0.023763296122734364 training acc of 99.23076923076923 testing acc of 57.0\n",
      "Epoch 597 with loss of 0.02039485268939573 training acc of 99.23076923076923 testing acc of 59.0\n",
      "Epoch 598 with loss of 0.019196216590129413 training acc of 99.3076923076923 testing acc of 59.0\n",
      "Epoch 599 with loss of 0.017338953715247605 training acc of 99.23076923076923 testing acc of 59.0\n",
      "Epoch 600 with loss of 0.01615226748981513 training acc of 99.23076923076923 testing acc of 59.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601 with loss of 0.016245593039247278 training acc of 99.23076923076923 testing acc of 58.0\n",
      "Epoch 602 with loss of 0.0217266665914884 training acc of 99.02197793813852 testing acc of 58.0\n",
      "Epoch 603 with loss of 0.015834772780251045 training acc of 99.23076923076923 testing acc of 61.0\n",
      "Epoch 604 with loss of 0.015208420392949708 training acc of 99.3076923076923 testing acc of 58.0\n",
      "Epoch 605 with loss of 0.015024878657781161 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 606 with loss of 0.015835106542978723 training acc of 99.3076923076923 testing acc of 60.0\n",
      "Epoch 607 with loss of 0.015627054994603477 training acc of 99.3076923076923 testing acc of 58.0\n",
      "Epoch 608 with loss of 0.016248430142205998 training acc of 99.24175849327675 testing acc of 58.0\n",
      "Epoch 609 with loss of 0.015777138482707623 training acc of 99.38461538461539 testing acc of 58.0\n",
      "Epoch 610 with loss of 0.015141560330028789 training acc of 99.23076923076923 testing acc of 59.0\n",
      "Epoch 611 with loss of 0.014582807553009704 training acc of 99.46153846153847 testing acc of 59.0\n",
      "Epoch 612 with loss of 0.015543371519575326 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 613 with loss of 0.01554681535848291 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 614 with loss of 0.0142307189749912 training acc of 99.46153846153847 testing acc of 60.0\n",
      "Epoch 615 with loss of 0.014129545047091177 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 616 with loss of 0.014746440914817728 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 617 with loss of 0.015611016142289512 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 618 with loss of 0.018022916906584915 training acc of 99.24175849327675 testing acc of 60.0\n",
      "Epoch 619 with loss of 0.014684150014475632 training acc of 99.38461538461539 testing acc of 58.0\n",
      "Epoch 620 with loss of 0.014171758277090983 training acc of 99.46153846153847 testing acc of 59.0\n",
      "Epoch 621 with loss of 0.014282459028226394 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 622 with loss of 0.0157393630182084 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 623 with loss of 0.01423426670058129 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 624 with loss of 0.014273137404905776 training acc of 99.46153846153847 testing acc of 59.0\n",
      "Epoch 625 with loss of 0.013431609608232975 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 626 with loss of 0.014009200108165924 training acc of 99.53846153846153 testing acc of 59.0\n",
      "Epoch 627 with loss of 0.013224775947702046 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 628 with loss of 0.014630462683271617 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 629 with loss of 0.01411806766051226 training acc of 99.53846153846153 testing acc of 59.0\n",
      "Epoch 630 with loss of 0.013196298962261958 training acc of 99.46153846153847 testing acc of 57.0\n",
      "Epoch 631 with loss of 0.013669046916989967 training acc of 99.3076923076923 testing acc of 59.0\n",
      "Epoch 632 with loss of 0.013237224447388703 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 633 with loss of 0.014979456215559576 training acc of 99.38461538461539 testing acc of 60.0\n",
      "Epoch 634 with loss of 0.014184146874378292 training acc of 99.24175849327675 testing acc of 58.0\n",
      "Epoch 635 with loss of 0.01302163538415558 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 636 with loss of 0.013403094767664488 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 637 with loss of 0.013747634050490048 training acc of 99.31868157019981 testing acc of 58.0\n",
      "Epoch 638 with loss of 0.014267299269424537 training acc of 99.47252772404597 testing acc of 58.0\n",
      "Epoch 639 with loss of 0.012412829807614729 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 640 with loss of 0.013771262503444003 training acc of 99.46153846153847 testing acc of 57.0\n",
      "Epoch 641 with loss of 0.013317031555253869 training acc of 99.53846153846153 testing acc of 59.0\n",
      "Epoch 642 with loss of 0.020260310660188015 training acc of 99.16483541635367 testing acc of 60.0\n",
      "Epoch 643 with loss of 0.016279595877187185 training acc of 99.31868157019981 testing acc of 57.0\n",
      "Epoch 644 with loss of 0.013371903064230887 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 645 with loss of 0.013367741825417258 training acc of 99.31868157019981 testing acc of 57.0\n",
      "Epoch 646 with loss of 0.012249100299064035 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 647 with loss of 0.012836014601746753 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 648 with loss of 0.014056570777029265 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 649 with loss of 0.01128502386060203 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 650 with loss of 0.012340413744543465 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 651 with loss of 0.015543472594939746 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 652 with loss of 0.013862637462667547 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 653 with loss of 0.014465445957074944 training acc of 99.31868157019981 testing acc of 59.0\n",
      "Epoch 654 with loss of 0.013496989133552862 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 655 with loss of 0.011855381639459385 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 656 with loss of 0.013413132536627997 training acc of 99.3956046471229 testing acc of 59.0\n",
      "Epoch 657 with loss of 0.012266060480704674 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 658 with loss of 0.012463271161183141 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 659 with loss of 0.01117982638355058 training acc of 99.53846153846153 testing acc of 59.0\n",
      "Epoch 660 with loss of 0.012771149815167658 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 661 with loss of 0.011329193939813055 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 662 with loss of 0.011765381371123321 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 663 with loss of 0.012877901192181386 training acc of 99.46153846153847 testing acc of 57.0\n",
      "Epoch 664 with loss of 0.01131678594818088 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 665 with loss of 0.01250650765051922 training acc of 99.38461538461539 testing acc of 59.0\n",
      "Epoch 666 with loss of 0.012497082016824815 training acc of 99.31868157019981 testing acc of 58.0\n",
      "Epoch 667 with loss of 0.01115306971377532 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 668 with loss of 0.012380396369665574 training acc of 99.47252772404597 testing acc of 58.0\n",
      "Epoch 669 with loss of 0.011212296003157882 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 670 with loss of 0.011512605974320859 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 671 with loss of 0.011023761932917226 training acc of 99.53846153846153 testing acc of 57.0\n",
      "Epoch 672 with loss of 0.011307571347480496 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 673 with loss of 0.012417885183822364 training acc of 99.31868157019981 testing acc of 58.0\n",
      "Epoch 674 with loss of 0.01122682271306985 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 675 with loss of 0.012854965396400075 training acc of 99.47252772404597 testing acc of 58.0\n",
      "Epoch 676 with loss of 0.01135167101613031 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 677 with loss of 0.015567614769679494 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 678 with loss of 0.01662759927029793 training acc of 99.3076923076923 testing acc of 59.0\n",
      "Epoch 679 with loss of 0.013375869108131155 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 680 with loss of 0.012021056349252691 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 681 with loss of 0.012268764465541327 training acc of 99.46153846153847 testing acc of 58.0\n",
      "Epoch 682 with loss of 0.012038412432257947 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 683 with loss of 0.011483360981891075 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 684 with loss of 0.018453714001225308 training acc of 99.17582409198468 testing acc of 58.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685 with loss of 0.011207272345647933 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 686 with loss of 0.011487948312815906 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 687 with loss of 0.01113448040604663 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 688 with loss of 0.010868113987761227 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 689 with loss of 0.010681094438321171 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 690 with loss of 0.010778879347176721 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 691 with loss of 0.01160448406321498 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 692 with loss of 0.011225955454462495 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 693 with loss of 0.010495912041873313 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 694 with loss of 0.011500885683255127 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 695 with loss of 0.013284525215567555 training acc of 99.61538461538461 testing acc of 59.0\n",
      "Epoch 696 with loss of 0.011066542365230046 training acc of 99.53846153846153 testing acc of 58.0\n",
      "Epoch 697 with loss of 0.010445709274231026 training acc of 99.61538461538461 testing acc of 59.0\n",
      "Epoch 698 with loss of 0.01104086013219785 training acc of 99.6923076923077 testing acc of 59.0\n",
      "Epoch 699 with loss of 0.010014903365607517 training acc of 99.61538461538461 testing acc of 59.0\n",
      "Epoch 700 with loss of 0.009866472557545282 training acc of 99.61538461538461 testing acc of 59.0\n",
      "Epoch 701 with loss of 0.010636220838373097 training acc of 99.6923076923077 testing acc of 59.0\n",
      "Epoch 702 with loss of 0.009903968038158312 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 703 with loss of 0.010821467671820965 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 704 with loss of 0.015321970921762 training acc of 99.3956046471229 testing acc of 58.0\n",
      "Epoch 705 with loss of 0.011639751498128485 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 706 with loss of 0.013545594200527726 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 707 with loss of 0.013051349598054703 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 708 with loss of 0.011645540824527135 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 709 with loss of 0.010287314787721978 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 710 with loss of 0.009932471501927536 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 711 with loss of 0.009479791848347165 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 712 with loss of 0.009446882702132616 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 713 with loss of 0.01412965702351917 training acc of 99.62637387789212 testing acc of 58.0\n",
      "Epoch 714 with loss of 0.013188561393270412 training acc of 99.54945080096905 testing acc of 59.0\n",
      "Epoch 715 with loss of 0.012227433366486086 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 716 with loss of 0.010013375703308087 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 717 with loss of 0.009833884301770013 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 718 with loss of 0.00878095094398864 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 719 with loss of 0.011204570419907283 training acc of 99.54945080096905 testing acc of 58.0\n",
      "Epoch 720 with loss of 0.010578847100567565 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 721 with loss of 0.009526568617170246 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 722 with loss of 0.00903469716789774 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 723 with loss of 0.008538789156143768 training acc of 99.6923076923077 testing acc of 59.0\n",
      "Epoch 724 with loss of 0.0078078370135279745 training acc of 99.6923076923077 testing acc of 59.0\n",
      "Epoch 725 with loss of 0.00791148828144427 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 726 with loss of 0.008044731942041276 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 727 with loss of 0.008593444264708804 training acc of 99.47252772404597 testing acc of 58.0\n",
      "Epoch 728 with loss of 0.0073164592031389475 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 729 with loss of 0.007701003537826741 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 730 with loss of 0.008182050534042692 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 731 with loss of 0.009220785973481655 training acc of 99.54945080096905 testing acc of 58.0\n",
      "Epoch 732 with loss of 0.008017202151957182 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 733 with loss of 0.00974321875764872 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 734 with loss of 0.012360798442107178 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 735 with loss of 0.014723280670855625 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 736 with loss of 0.011862036598568711 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 737 with loss of 0.02792049740673974 training acc of 98.84615384615384 testing acc of 55.0\n",
      "Epoch 738 with loss of 0.09111427942004341 training acc of 97.3956046471229 testing acc of 58.0\n",
      "Epoch 739 with loss of 0.10264347328876074 training acc of 96.15384615384616 testing acc of 57.0\n",
      "Epoch 740 with loss of 0.07641197397158696 training acc of 97.0 testing acc of 56.0\n",
      "Epoch 741 with loss of 0.048258171488459296 training acc of 98.24175849327675 testing acc of 57.0\n",
      "Epoch 742 with loss of 0.02158112175619373 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 743 with loss of 0.01608101445106933 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 744 with loss of 0.009760802446040683 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 745 with loss of 0.009023826685734093 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 746 with loss of 0.008430498813350614 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 747 with loss of 0.007861732626154732 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 748 with loss of 0.007697013426178063 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 749 with loss of 0.008247997916231934 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 750 with loss of 0.007684879837772594 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 751 with loss of 0.0074622944498864505 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 752 with loss of 0.009217092015135746 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 753 with loss of 0.008143260674282478 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 754 with loss of 0.007277407155085642 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 755 with loss of 0.007591294362590326 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 756 with loss of 0.007273586493773529 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 757 with loss of 0.0072786368949052235 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 758 with loss of 0.007234684083064517 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 759 with loss of 0.006963506150686254 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 760 with loss of 0.0070022871950641274 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 761 with loss of 0.007614385218822952 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 762 with loss of 0.007182338588441221 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 763 with loss of 0.007003807362007837 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 764 with loss of 0.008203933799030403 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 765 with loss of 0.00824541193791307 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 766 with loss of 0.006924124139074523 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 767 with loss of 0.00694334152369545 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 768 with loss of 0.00681471340067219 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 769 with loss of 0.006989452566743309 training acc of 99.61538461538461 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770 with loss of 0.0065988379294419875 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 771 with loss of 0.00707609410612629 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 772 with loss of 0.0072045986638001455 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 773 with loss of 0.0068095234685013285 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 774 with loss of 0.006731981514559056 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 775 with loss of 0.007241130463975983 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 776 with loss of 0.0070614299167153565 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 777 with loss of 0.008021138038692208 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 778 with loss of 0.0067664488630655864 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 779 with loss of 0.00782801352705484 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 780 with loss of 0.007360343462026391 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 781 with loss of 0.006659633946233394 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 782 with loss of 0.010796357716362063 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 783 with loss of 0.008248811990667421 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 784 with loss of 0.007365849587726048 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 785 with loss of 0.0073149249723288585 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 786 with loss of 0.008227599509924542 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 787 with loss of 0.007949325503432192 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 788 with loss of 0.00961723466976904 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 789 with loss of 0.007562149669236253 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 790 with loss of 0.006646149845507283 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 791 with loss of 0.006722415272983758 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 792 with loss of 0.00756970466598152 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 793 with loss of 0.006293905433267355 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 794 with loss of 0.009327281998524155 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 795 with loss of 0.006939802059115699 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 796 with loss of 0.006822254650335078 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 797 with loss of 0.006642430474480184 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 798 with loss of 0.007049019394729 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 799 with loss of 0.00672416191530199 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 800 with loss of 0.006917840794463141 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 801 with loss of 0.00711128471504288 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 802 with loss of 0.006792662459282348 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 803 with loss of 0.006880832981830911 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 804 with loss of 0.0071422954116804665 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 805 with loss of 0.010406131950171234 training acc of 99.47252772404597 testing acc of 57.0\n",
      "Epoch 806 with loss of 0.008137855629544131 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 807 with loss of 0.008888375022741877 training acc of 99.47252772404597 testing acc of 57.0\n",
      "Epoch 808 with loss of 0.006599108500477786 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 809 with loss of 0.006818118695474158 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 810 with loss of 0.006366504379492164 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 811 with loss of 0.007246426687253496 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 812 with loss of 0.007875565550560938 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 813 with loss of 0.0070002047712198244 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 814 with loss of 0.0074610685593282915 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 815 with loss of 0.006889513348981451 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 816 with loss of 0.007305277139843943 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 817 with loss of 0.007175874102708453 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 818 with loss of 0.007091243610305425 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 819 with loss of 0.007993016376321275 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 820 with loss of 0.007193739866264737 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 821 with loss of 0.00654962087075826 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 822 with loss of 0.007602381856346396 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 823 with loss of 0.006691053939553408 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 824 with loss of 0.006504085682816087 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 825 with loss of 0.0072593783300432665 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 826 with loss of 0.007064707176491188 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 827 with loss of 0.006496491953909684 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 828 with loss of 0.006975352338765963 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 829 with loss of 0.006659151267600604 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 830 with loss of 0.0070314574427007195 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 831 with loss of 0.006761828766544708 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 832 with loss of 0.009529264238573467 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 833 with loss of 0.007164782950824771 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 834 with loss of 0.006348281406994479 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 835 with loss of 0.007713555635116511 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 836 with loss of 0.006749951045584077 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 837 with loss of 0.007444986106398014 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 838 with loss of 0.008247762394043652 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 839 with loss of 0.006849961385551768 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 840 with loss of 0.006407527917102785 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 841 with loss of 0.0064946377273232565 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 842 with loss of 0.008507348176723238 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 843 with loss of 0.006469945528526575 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 844 with loss of 0.006691683194591091 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 845 with loss of 0.008068577209577108 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 846 with loss of 0.007119468800598183 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 847 with loss of 0.006691823585872323 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 848 with loss of 0.013469075500324834 training acc of 99.48351639967699 testing acc of 57.0\n",
      "Epoch 849 with loss of 0.006492876845396285 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 850 with loss of 0.0067675441493450375 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 851 with loss of 0.006533500104873942 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 852 with loss of 0.006576951982704206 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 853 with loss of 0.008663121230291346 training acc of 99.47252772404597 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 854 with loss of 0.006248742605049091 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 855 with loss of 0.007126410947631274 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 856 with loss of 0.007230911919088192 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 857 with loss of 0.008595490778903835 training acc of 99.54945080096905 testing acc of 58.0\n",
      "Epoch 858 with loss of 0.009603440047402937 training acc of 99.3956046471229 testing acc of 57.0\n",
      "Epoch 859 with loss of 0.008775199763476849 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 860 with loss of 0.00679638859527096 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 861 with loss of 0.006437503070628736 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 862 with loss of 0.006949232409869392 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 863 with loss of 0.00784510946743047 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 864 with loss of 0.009106415370926978 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 865 with loss of 0.0068228685323937005 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 866 with loss of 0.0063940924632893605 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 867 with loss of 0.006621197645122616 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 868 with loss of 0.006574595025785339 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 869 with loss of 0.006984704551331771 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 870 with loss of 0.006645051322546071 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 871 with loss of 0.00651049751363294 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 872 with loss of 0.006173727511990224 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 873 with loss of 0.006750844209259062 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 874 with loss of 0.007214799975582327 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 875 with loss of 0.0065606684673604415 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 876 with loss of 0.006533837523490477 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 877 with loss of 0.006706384822162083 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 878 with loss of 0.007001180931603393 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 879 with loss of 0.006914867748631737 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 880 with loss of 0.007927314358732054 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 881 with loss of 0.007739095997049634 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 882 with loss of 0.007618352531538291 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 883 with loss of 0.009174942420661235 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 884 with loss of 0.006993034375997153 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 885 with loss of 0.006757748465483578 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 886 with loss of 0.007136900015421606 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 887 with loss of 0.006562389846994148 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 888 with loss of 0.006955697391486655 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 889 with loss of 0.007053860864778891 training acc of 99.6923076923077 testing acc of 54.0\n",
      "Epoch 890 with loss of 0.0066549990123558715 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 891 with loss of 0.006532354522138261 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 892 with loss of 0.011408521305733861 training acc of 99.62637387789212 testing acc of 58.0\n",
      "Epoch 893 with loss of 0.006657294175126411 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 894 with loss of 0.006544833223765286 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 895 with loss of 0.0063886641627714895 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 896 with loss of 0.006498246402211057 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 897 with loss of 0.006760283460607752 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 898 with loss of 0.0072266118661970654 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 899 with loss of 0.0065491360236102575 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 900 with loss of 0.008223079642098478 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 901 with loss of 0.006788166141632246 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 902 with loss of 0.006825403316235259 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 903 with loss of 0.00786837007693136 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 904 with loss of 0.006337701336703434 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 905 with loss of 0.006880333516724372 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 906 with loss of 0.0065103970608526235 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 907 with loss of 0.00640880837174616 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 908 with loss of 0.006350158699485921 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 909 with loss of 0.008343180873523055 training acc of 99.62637387789212 testing acc of 58.0\n",
      "Epoch 910 with loss of 0.007073455256501732 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 911 with loss of 0.006572600650402959 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 912 with loss of 0.00671272431631783 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 913 with loss of 0.006320753791302335 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 914 with loss of 0.006426751948310994 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 915 with loss of 0.007838004016565928 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 916 with loss of 0.0066942972873500995 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 917 with loss of 0.007827416120562702 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 918 with loss of 0.008352325742844887 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 919 with loss of 0.008104771669213032 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 920 with loss of 0.00680162960690303 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 921 with loss of 0.006723859033977183 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 922 with loss of 0.010445567194927627 training acc of 99.54945080096905 testing acc of 54.0\n",
      "Epoch 923 with loss of 0.01187037463163366 training acc of 99.6923076923077 testing acc of 59.0\n",
      "Epoch 924 with loss of 0.008378196505173737 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 925 with loss of 0.008656628725405496 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 926 with loss of 0.008284469847030077 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 927 with loss of 0.0071036328804788915 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 928 with loss of 0.006790835368659687 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 929 with loss of 0.0062977172945186165 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 930 with loss of 0.007208306813061846 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 931 with loss of 0.006332278181686819 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 932 with loss of 0.006871722176443795 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 933 with loss of 0.007058641327743176 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 934 with loss of 0.00730172603261263 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 935 with loss of 0.006822079007598894 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 936 with loss of 0.0066336713408908015 training acc of 99.6923076923077 testing acc of 58.0\n",
      "Epoch 937 with loss of 0.006437769193955598 training acc of 99.76923076923077 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 938 with loss of 0.006505093618006168 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 939 with loss of 0.007550983007604373 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 940 with loss of 0.007128179692349827 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 941 with loss of 0.006981221822990427 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 942 with loss of 0.007130085322037653 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 943 with loss of 0.006769980416160811 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 944 with loss of 0.0072974462869192045 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 945 with loss of 0.006452003749403117 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 946 with loss of 0.006374897674294726 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 947 with loss of 0.00686169456009968 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 948 with loss of 0.007242407081391251 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 949 with loss of 0.00698592488417769 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 950 with loss of 0.006555275483361374 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 951 with loss of 0.006409087727521323 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 952 with loss of 0.006299785468190049 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 953 with loss of 0.006735661832723194 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 954 with loss of 0.0064339666345264186 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 955 with loss of 0.007293249834149789 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 956 with loss of 0.009115084463981194 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 957 with loss of 0.009993760761482498 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 958 with loss of 0.007199740214183783 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 959 with loss of 0.006616924732113078 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 960 with loss of 0.006341158574147043 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 961 with loss of 0.0063881725671728 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 962 with loss of 0.006782015153266212 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 963 with loss of 0.006567542102717338 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 964 with loss of 0.007180145543176108 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 965 with loss of 0.0070118860961743985 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 966 with loss of 0.006758975985576399 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 967 with loss of 0.006810715319191401 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 968 with loss of 0.006607501795420271 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 969 with loss of 0.006857296038875715 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 970 with loss of 0.006335867306356354 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 971 with loss of 0.012415575050209345 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 972 with loss of 0.009091420521704445 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 973 with loss of 0.02098842065494794 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 974 with loss of 0.03013841190841049 training acc of 98.84615384615384 testing acc of 58.0\n",
      "Epoch 975 with loss of 0.04265564595026752 training acc of 97.84615384615384 testing acc of 56.0\n",
      "Epoch 976 with loss of 0.028417926801082034 training acc of 99.23076923076923 testing acc of 54.0\n",
      "Epoch 977 with loss of 0.02158889621429038 training acc of 99.3076923076923 testing acc of 56.0\n",
      "Epoch 978 with loss of 0.015580070813974509 training acc of 99.31868157019981 testing acc of 55.0\n",
      "Epoch 979 with loss of 0.013031042796613362 training acc of 99.53846153846153 testing acc of 55.0\n",
      "Epoch 980 with loss of 0.00823431508671582 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 981 with loss of 0.007370691719608238 training acc of 99.84615384615384 testing acc of 57.0\n",
      "Epoch 982 with loss of 0.006762665967331626 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 983 with loss of 0.006883096467040909 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 984 with loss of 0.006482544404067225 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 985 with loss of 0.006433271740276653 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 986 with loss of 0.008673808282205405 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 987 with loss of 0.008567958784204585 training acc of 99.61538461538461 testing acc of 58.0\n",
      "Epoch 988 with loss of 0.007326079865099298 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 989 with loss of 0.006613250675190312 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 990 with loss of 0.007281222110927606 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 991 with loss of 0.006630149814345014 training acc of 99.84615384615384 testing acc of 57.0\n",
      "Epoch 992 with loss of 0.008699005316325033 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 993 with loss of 0.006761846898231083 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 994 with loss of 0.00680595831055633 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 995 with loss of 0.006355051264668314 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 996 with loss of 0.006931228616919655 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 997 with loss of 0.006422065303363174 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 998 with loss of 0.006843106910729637 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 999 with loss of 0.0065237230695716925 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1000 with loss of 0.006482496275286674 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1001 with loss of 0.006331595643552226 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1002 with loss of 0.010756133777669255 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1003 with loss of 0.010522893148635585 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1004 with loss of 0.006523698163688935 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1005 with loss of 0.006883463146060911 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1006 with loss of 0.006188036067564658 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1007 with loss of 0.006842425947699946 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1008 with loss of 0.006480698286815403 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1009 with loss of 0.006332082971941697 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1010 with loss of 0.006328345173432563 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1011 with loss of 0.011226654905700482 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1012 with loss of 0.006687235521474997 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1013 with loss of 0.006872822167889143 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 1014 with loss of 0.008429378005250608 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1015 with loss of 0.0066101704052506155 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1016 with loss of 0.006361810550375734 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1017 with loss of 0.006282255456776161 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1018 with loss of 0.00648389339543288 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1019 with loss of 0.006270767372580639 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1020 with loss of 0.007261951475251967 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1021 with loss of 0.006383828257150106 training acc of 99.76923076923077 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1022 with loss of 0.006408109166211664 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1023 with loss of 0.006837396252824244 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1024 with loss of 0.006401203236936663 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1025 with loss of 0.006384963230839975 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1026 with loss of 0.007946068640124464 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1027 with loss of 0.006608709112557475 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1028 with loss of 0.006175231603405196 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1029 with loss of 0.00811430985833375 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1030 with loss of 0.006423952473806611 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1031 with loss of 0.006278559012431428 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1032 with loss of 0.00824605750788648 training acc of 99.62637387789212 testing acc of 54.0\n",
      "Epoch 1033 with loss of 0.006729745733677051 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1034 with loss of 0.00700435935645006 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1035 with loss of 0.006078370707165539 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1036 with loss of 0.006843765928771553 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1037 with loss of 0.006684001963759119 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1038 with loss of 0.007022750323113332 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1039 with loss of 0.0063820179826647155 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1040 with loss of 0.006269350286367206 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1041 with loss of 0.006556000856037896 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1042 with loss of 0.006300556025123045 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1043 with loss of 0.0070474767369607715 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1044 with loss of 0.006246701069148213 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1045 with loss of 0.0063227788626905885 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1046 with loss of 0.006511261238931463 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1047 with loss of 0.006203858066538277 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1048 with loss of 0.007255717689706496 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1049 with loss of 0.006565260314696038 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1050 with loss of 0.00838611520493242 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1051 with loss of 0.007266901252128614 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1052 with loss of 0.006876680015501244 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1053 with loss of 0.006852730189096361 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1054 with loss of 0.008021732541237725 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1055 with loss of 0.006328110506449145 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1056 with loss of 0.006459034478981406 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1057 with loss of 0.006309530901489779 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1058 with loss of 0.006372646467263201 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1059 with loss of 0.006769881653711379 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1060 with loss of 0.006702171587116586 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1061 with loss of 0.009221514673649485 training acc of 99.62637387789212 testing acc of 54.0\n",
      "Epoch 1062 with loss of 0.0061316835988411466 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1063 with loss of 0.006683030022871277 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1064 with loss of 0.006265098551011761 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1065 with loss of 0.006213613021268079 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1066 with loss of 0.00622657832956555 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1067 with loss of 0.006288153488220325 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1068 with loss of 0.007072127707383165 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1069 with loss of 0.006434985547597394 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1070 with loss of 0.005933425791786599 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1071 with loss of 0.006227010351722129 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1072 with loss of 0.006340339595995969 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1073 with loss of 0.006217880018084543 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1074 with loss of 0.007155966027102505 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1075 with loss of 0.006177517658355869 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1076 with loss of 0.00638141250881465 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1077 with loss of 0.006359821556053738 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1078 with loss of 0.00637643132764564 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1079 with loss of 0.010101060851481564 training acc of 99.47252772404597 testing acc of 54.0\n",
      "Epoch 1080 with loss of 0.008396983109728003 training acc of 99.54945080096905 testing acc of 55.0\n",
      "Epoch 1081 with loss of 0.006551607252101978 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1082 with loss of 0.006349531292541472 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1083 with loss of 0.006348762828215863 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1084 with loss of 0.006426360702719718 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1085 with loss of 0.006235038075898886 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1086 with loss of 0.006361012923382357 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1087 with loss of 0.006736880136556395 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1088 with loss of 0.006184973761684467 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1089 with loss of 0.006446435501297506 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1090 with loss of 0.007317187210831504 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 1091 with loss of 0.006787272160951174 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1092 with loss of 0.0064600964967842 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1093 with loss of 0.006390144332097127 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1094 with loss of 0.006456970237195492 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1095 with loss of 0.006434229840544084 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1096 with loss of 0.007243029449595129 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1097 with loss of 0.00670104391275135 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1098 with loss of 0.0063571757211600645 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1099 with loss of 0.008547497447720022 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1100 with loss of 0.007642950970526167 training acc of 99.54945080096905 testing acc of 55.0\n",
      "Epoch 1101 with loss of 0.0061865377132422645 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1102 with loss of 0.00667141007810502 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1103 with loss of 0.00816160268508471 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1104 with loss of 0.006778388997190632 training acc of 99.76923076923077 testing acc of 56.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1105 with loss of 0.006815609996920102 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 1106 with loss of 0.0067162629790031 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1107 with loss of 0.0063600785412675645 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1108 with loss of 0.00672690445040084 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1109 with loss of 0.006355702532276224 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1110 with loss of 0.006997665005306212 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1111 with loss of 0.006230375261881049 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1112 with loss of 0.0062491938890441535 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1113 with loss of 0.006573093071403071 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1114 with loss of 0.006362259302057702 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1115 with loss of 0.006428896776364686 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1116 with loss of 0.007108016310107464 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1117 with loss of 0.007057517864898098 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1118 with loss of 0.006233562813087281 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1119 with loss of 0.006420923174748448 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1120 with loss of 0.006459923510332233 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1121 with loss of 0.006338147600147819 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1122 with loss of 0.006387321011201353 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1123 with loss of 0.010830959116831834 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1124 with loss of 0.006740715327536096 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1125 with loss of 0.006156086920012827 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1126 with loss of 0.006377218991474365 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1127 with loss of 0.0062227134787742845 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1128 with loss of 0.006164709520957191 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1129 with loss of 0.006246241418081739 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1130 with loss of 0.006230710948819671 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1131 with loss of 0.006377013675983807 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1132 with loss of 0.006430916690498214 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1133 with loss of 0.006286552026862269 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1134 with loss of 0.006299215133507319 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1135 with loss of 0.006774479468451598 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 1136 with loss of 0.006937924363368298 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1137 with loss of 0.007385472725638045 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1138 with loss of 0.0070159988933413 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1139 with loss of 0.006475367034139568 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1140 with loss of 0.008624165214944067 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1141 with loss of 0.007148038153200804 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 1142 with loss of 0.006979358495580248 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1143 with loss of 0.00625201565428422 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1144 with loss of 0.006304891688579608 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1145 with loss of 0.006504560202861635 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1146 with loss of 0.006337076775078178 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1147 with loss of 0.006585888063664942 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1148 with loss of 0.006196458941583562 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1149 with loss of 0.006435533287003636 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1150 with loss of 0.006263685049046986 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1151 with loss of 0.006217228712417777 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1152 with loss of 0.006169549927807776 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1153 with loss of 0.006235567001007998 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1154 with loss of 0.006284758091799001 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1155 with loss of 0.007644409351576962 training acc of 99.62637387789212 testing acc of 58.0\n",
      "Epoch 1156 with loss of 0.006533085664998977 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 1157 with loss of 0.006268760986970916 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1158 with loss of 0.00629612691087599 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1159 with loss of 0.006593035475816578 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1160 with loss of 0.006715206483092446 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1161 with loss of 0.008231105636486272 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1162 with loss of 0.00836751392433563 training acc of 99.47252772404597 testing acc of 56.0\n",
      "Epoch 1163 with loss of 0.007045720591962051 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1164 with loss of 0.006581128979493 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1165 with loss of 0.006352988342069483 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1166 with loss of 0.00635195633647247 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1167 with loss of 0.006511432468029783 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1168 with loss of 0.006333106584301836 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1169 with loss of 0.006461384094738885 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1170 with loss of 0.0075161475285242954 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1171 with loss of 0.008450400609175492 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1172 with loss of 0.006330193347261788 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1173 with loss of 0.006405343234221688 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1174 with loss of 0.006250149652399928 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1175 with loss of 0.006303400483934988 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1176 with loss of 0.00726646645447405 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1177 with loss of 0.012295664345191862 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1178 with loss of 0.007218445139117718 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1179 with loss of 0.006536370053674126 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1180 with loss of 0.006311937276083112 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1181 with loss of 0.006351828471591803 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1182 with loss of 0.006286823568119031 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1183 with loss of 0.006427556658029341 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1184 with loss of 0.008116924519596908 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1185 with loss of 0.006689022539201408 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1186 with loss of 0.0066634797000845606 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1187 with loss of 0.0066058976460856055 training acc of 99.6923076923077 testing acc of 56.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1188 with loss of 0.006411287602811801 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1189 with loss of 0.006598892175763747 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1190 with loss of 0.011111238188627444 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1191 with loss of 0.21073122610015652 training acc of 96.93406618558444 testing acc of 57.0\n",
      "Epoch 1192 with loss of 0.25617845055575555 training acc of 93.15384615384616 testing acc of 57.0\n",
      "Epoch 1193 with loss of 0.13522530175172365 training acc of 93.64835181603065 testing acc of 57.0\n",
      "Epoch 1194 with loss of 0.05829196036435091 training acc of 98.84615384615384 testing acc of 54.0\n",
      "Epoch 1195 with loss of 0.04213308019993397 training acc of 98.78022003173828 testing acc of 57.0\n",
      "Epoch 1196 with loss of 0.029166654074707858 training acc of 99.53846153846153 testing acc of 55.0\n",
      "Epoch 1197 with loss of 0.022423240105406597 training acc of 99.46153846153847 testing acc of 54.0\n",
      "Epoch 1198 with loss of 0.019568363025497932 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1199 with loss of 0.016575756885756094 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 1200 with loss of 0.015457172215414735 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1201 with loss of 0.015402954161310425 training acc of 99.61538461538461 testing acc of 54.0\n",
      "Epoch 1202 with loss of 0.01509631471708417 training acc of 99.61538461538461 testing acc of 56.0\n",
      "Epoch 1203 with loss of 0.008847334252025645 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1204 with loss of 0.008322647065282442 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1205 with loss of 0.0073570295174319586 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1206 with loss of 0.007144976859518255 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1207 with loss of 0.006974321028862435 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1208 with loss of 0.006839730902216756 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1209 with loss of 0.006813514319499238 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1210 with loss of 0.007493221398013143 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1211 with loss of 0.006768838856190156 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1212 with loss of 0.006678011514640485 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1213 with loss of 0.006560432167204384 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1214 with loss of 0.006577211765733619 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1215 with loss of 0.00723521113440466 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1216 with loss of 0.006727805933153901 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1217 with loss of 0.0065343171680489415 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1218 with loss of 0.006565496862794344 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1219 with loss of 0.007422814825146746 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1220 with loss of 0.006416930594311383 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1221 with loss of 0.006420844089007005 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1222 with loss of 0.006405408194181151 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1223 with loss of 0.006361873935720024 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1224 with loss of 0.007230868439924402 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1225 with loss of 0.006665378364022427 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1226 with loss of 0.00812164308780876 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1227 with loss of 0.006587559507282164 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1228 with loss of 0.0063271377478556065 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1229 with loss of 0.006428908638414354 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1230 with loss of 0.00811943235860851 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1231 with loss of 0.006346168822626798 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1232 with loss of 0.0065035897546644825 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1233 with loss of 0.006521391339797096 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1234 with loss of 0.006294529064549492 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1235 with loss of 0.006559574850633418 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1236 with loss of 0.00625498308587479 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1237 with loss of 0.006329590983044069 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1238 with loss of 0.006342424847669183 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1239 with loss of 0.006472015949279571 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1240 with loss of 0.006364018876271215 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1241 with loss of 0.006222059536062611 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1242 with loss of 0.006256183592016056 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1243 with loss of 0.008590331850707745 training acc of 99.62637387789212 testing acc of 59.0\n",
      "Epoch 1244 with loss of 0.006291817352086162 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1245 with loss of 0.008009841117130306 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1246 with loss of 0.0064490355059835846 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1247 with loss of 0.006935677442995187 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1248 with loss of 0.006401798544594875 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1249 with loss of 0.006504196292483427 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1250 with loss of 0.0063730803001537705 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1251 with loss of 0.006284302796792382 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1252 with loss of 0.007970548688228099 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1253 with loss of 0.006507642618928982 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 1254 with loss of 0.006456257179916765 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1255 with loss of 0.006299232114822819 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1256 with loss of 0.00619890954168603 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1257 with loss of 0.006304080971368911 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1258 with loss of 0.006288142408056256 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1259 with loss of 0.00783073232965902 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1260 with loss of 0.006448342612622162 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1261 with loss of 0.0063869122996738255 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1262 with loss of 0.0062512365856001945 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1263 with loss of 0.006195717844373296 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1264 with loss of 0.006415174188217721 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1265 with loss of 0.006237895482627209 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1266 with loss of 0.0062502628246035715 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1267 with loss of 0.006963275892373461 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1268 with loss of 0.006374737465823553 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1269 with loss of 0.006537907510601844 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1270 with loss of 0.0061769608661192115 training acc of 99.76923076923077 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1271 with loss of 0.006375681376644375 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1272 with loss of 0.006322623065968331 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1273 with loss of 0.006408260011364921 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1274 with loss of 0.006499148388684262 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1275 with loss of 0.0063218105354350814 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1276 with loss of 0.006214526269244603 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1277 with loss of 0.0061438299979692185 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1278 with loss of 0.006616374318232724 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1279 with loss of 0.006117036912305944 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1280 with loss of 0.006207794014839097 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1281 with loss of 0.006250546134721775 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1282 with loss of 0.006758698763195963 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1283 with loss of 0.01066527270450024 training acc of 99.62637387789212 testing acc of 58.0\n",
      "Epoch 1284 with loss of 0.006473069000863829 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1285 with loss of 0.0065947087801200375 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1286 with loss of 0.006697415150343799 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1287 with loss of 0.006220750954526011 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1288 with loss of 0.006335845989479612 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1289 with loss of 0.00832197448471561 training acc of 99.47252772404597 testing acc of 57.0\n",
      "Epoch 1290 with loss of 0.006402401413652115 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1291 with loss of 0.006291848852225275 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1292 with loss of 0.008001283781665664 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1293 with loss of 0.00638670908581564 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1294 with loss of 0.006319622239411378 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1295 with loss of 0.006491252005127115 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1296 with loss of 0.006512296301550053 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1297 with loss of 0.006339367293549003 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1298 with loss of 0.00644930156704504 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1299 with loss of 0.006215269738347091 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1300 with loss of 0.006264883411159434 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1301 with loss of 0.006175261630582659 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1302 with loss of 0.006470676693425048 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1303 with loss of 0.006455736462857413 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1304 with loss of 0.006376585165316311 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1305 with loss of 0.0062250312677441305 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1306 with loss of 0.00634227091583083 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1307 with loss of 0.006170274138220031 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1308 with loss of 0.007066830446442159 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1309 with loss of 0.006514131232576732 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1310 with loss of 0.006216265761255412 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1311 with loss of 0.006256717539284951 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1312 with loss of 0.006142057548109174 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1313 with loss of 0.006211393649629407 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1314 with loss of 0.006416948866479708 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1315 with loss of 0.006318972163941138 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1316 with loss of 0.006466806307658912 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1317 with loss of 0.010927343166538496 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1318 with loss of 0.006518630511261738 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1319 with loss of 0.006782750512894171 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1320 with loss of 0.006253312420225344 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1321 with loss of 0.006294387421803549 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1322 with loss of 0.0126279353803861 training acc of 99.48351639967699 testing acc of 59.0\n",
      "Epoch 1323 with loss of 0.0062135700052469755 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1324 with loss of 0.006892401279541305 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1325 with loss of 0.006100879431939505 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1326 with loss of 0.006389956803804335 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1327 with loss of 0.007100340039939441 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1328 with loss of 0.006390621333244221 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1329 with loss of 0.006622226200516497 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1330 with loss of 0.006234975209316382 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1331 with loss of 0.006969700271908481 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1332 with loss of 0.006362856040001274 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1333 with loss of 0.006810974167400183 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1334 with loss of 0.0061187534430735884 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1335 with loss of 0.006900647443007284 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1336 with loss of 0.0062181321901824465 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1337 with loss of 0.0063564500300204736 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1338 with loss of 0.006391257941196762 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1339 with loss of 0.006181063207455177 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1340 with loss of 0.008591650241354588 training acc of 99.62637387789212 testing acc of 56.0\n",
      "Epoch 1341 with loss of 0.006966251373971597 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1342 with loss of 0.006121665700517882 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1343 with loss of 0.006615319983281482 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1344 with loss of 0.0062927131405628575 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1345 with loss of 0.00616652833112032 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1346 with loss of 0.006320540733353342 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1347 with loss of 0.007032822320560137 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1348 with loss of 0.006262326428371195 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1349 with loss of 0.006577865664999431 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1350 with loss of 0.0063177590060289595 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1351 with loss of 0.006405883660940722 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1352 with loss of 0.006150184749108471 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1353 with loss of 0.006624670231058889 training acc of 99.76923076923077 testing acc of 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1354 with loss of 0.006602853708430372 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1355 with loss of 0.006692282191719501 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1356 with loss of 0.007918015983495234 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1357 with loss of 0.006794912531264485 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1358 with loss of 0.006532940995384706 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1359 with loss of 0.006377016176534548 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1360 with loss of 0.006276284777413821 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1361 with loss of 0.007222156603417646 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1362 with loss of 0.006347820446675401 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1363 with loss of 0.0062773862888347 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1364 with loss of 0.0061492843622265635 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1365 with loss of 0.01114407349870397 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1366 with loss of 0.006353949326204691 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1367 with loss of 0.007242130202380044 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1368 with loss of 0.0063550533422340565 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1369 with loss of 0.006415672818687968 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1370 with loss of 0.006093619400277161 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1371 with loss of 0.006232704011311468 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1372 with loss of 0.006899643875490606 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1373 with loss of 0.007546669481178889 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1374 with loss of 0.0062545031821802976 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1375 with loss of 0.006792758319604148 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1376 with loss of 0.006431550911936658 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1377 with loss of 0.006491409358344614 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1378 with loss of 0.006458635228717377 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1379 with loss of 0.006261640837832461 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1380 with loss of 0.006896864726093974 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1381 with loss of 0.006691582047586473 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1382 with loss of 0.00624912945484259 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1383 with loss of 0.006277301106400466 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1384 with loss of 0.006256008853177683 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1385 with loss of 0.006641962295431698 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1386 with loss of 0.007680763891515268 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1387 with loss of 0.005995528420848691 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1388 with loss of 0.009254263476647723 training acc of 99.6923076923077 testing acc of 54.0\n",
      "Epoch 1389 with loss of 0.010139690895672314 training acc of 99.61538461538461 testing acc of 55.0\n",
      "Epoch 1390 with loss of 0.007337441369879973 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1391 with loss of 0.0077510770973346485 training acc of 99.54945080096905 testing acc of 54.0\n",
      "Epoch 1392 with loss of 0.006883264054172637 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1393 with loss of 0.006337579445523891 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1394 with loss of 0.006284004953368612 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1395 with loss of 0.006327939788976577 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1396 with loss of 0.006298540416397513 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1397 with loss of 0.0062469411274763 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1398 with loss of 0.006905500659531054 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1399 with loss of 0.006280614679081527 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1400 with loss of 0.006279651761220661 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1401 with loss of 0.006237876663782607 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1402 with loss of 0.00632915075038909 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1403 with loss of 0.006795208972527269 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1404 with loss of 0.006652292650799679 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1405 with loss of 0.0067087853245678935 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1406 with loss of 0.006151072243553175 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1407 with loss of 0.0064439877527407725 training acc of 99.76923076923077 testing acc of 54.0\n",
      "Epoch 1408 with loss of 0.006361650182113338 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1409 with loss of 0.0068250084430194246 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1410 with loss of 0.008160621137134373 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1411 with loss of 0.006431252852612838 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1412 with loss of 0.006524183649162296 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1413 with loss of 0.007119939176146335 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1414 with loss of 0.0063002789580563195 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1415 with loss of 0.007195424065475191 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1416 with loss of 0.008142855766121872 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1417 with loss of 0.00952041921790134 training acc of 99.6923076923077 testing acc of 55.0\n",
      "Epoch 1418 with loss of 0.01019632451173563 training acc of 99.6923076923077 testing acc of 57.0\n",
      "Epoch 1419 with loss of 0.007394815143085837 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1420 with loss of 0.00648780296167108 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1421 with loss of 0.00660299133596709 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1422 with loss of 0.006372384071045627 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1423 with loss of 0.006436666515722978 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1424 with loss of 0.006533844535274861 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1425 with loss of 0.007094833189162623 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1426 with loss of 0.006231102369033929 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1427 with loss of 0.006861770435464747 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1428 with loss of 0.00621617548886906 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1429 with loss of 0.006326021173243554 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1430 with loss of 0.00620307799685039 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1431 with loss of 0.006689782516332343 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1432 with loss of 0.00631871500977118 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1433 with loss of 0.006251665211143868 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1434 with loss of 0.006124696364377786 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1435 with loss of 0.0064086042990451324 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1436 with loss of 0.006243163052623948 training acc of 99.76923076923077 testing acc of 56.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1437 with loss of 0.010591451537318509 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1438 with loss of 0.006292102297895606 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1439 with loss of 0.006446184845677076 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1440 with loss of 0.00628164613534374 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1441 with loss of 0.006252444355945604 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1442 with loss of 0.006341324836914562 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1443 with loss of 0.006413981602376085 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1444 with loss of 0.006403472802048782 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1445 with loss of 0.006278478257277479 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1446 with loss of 0.006353032757593051 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1447 with loss of 0.011399917888831204 training acc of 99.62637387789212 testing acc of 55.0\n",
      "Epoch 1448 with loss of 0.006624489824389457 training acc of 99.76923076923077 testing acc of 59.0\n",
      "Epoch 1449 with loss of 0.006164790386929571 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1450 with loss of 0.0062123714510212615 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1451 with loss of 0.006561779972309103 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1452 with loss of 0.006182907985091263 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1453 with loss of 0.006342570794004132 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1454 with loss of 0.006399486127730387 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1455 with loss of 0.007164767228599745 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1456 with loss of 0.0062806085173248835 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1457 with loss of 0.006297763998960503 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1458 with loss of 0.00809195797186336 training acc of 99.62637387789212 testing acc of 57.0\n",
      "Epoch 1459 with loss of 0.00681482073224078 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1460 with loss of 0.0065993885800708085 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1461 with loss of 0.006341177962335328 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1462 with loss of 0.006526495220896322 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1463 with loss of 0.0063825824091999 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1464 with loss of 0.006380825916914126 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1465 with loss of 0.0064970800387807405 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1466 with loss of 0.00751456519635842 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1467 with loss of 0.0061474506619561 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1468 with loss of 0.006428138105687909 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1469 with loss of 0.006507729949589702 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1470 with loss of 0.006151998407193997 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1471 with loss of 0.00698189527723186 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1472 with loss of 0.006618587376299099 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1473 with loss of 0.006170617600358897 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1474 with loss of 0.006161387444500668 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1475 with loss of 0.008928750294636792 training acc of 99.54945080096905 testing acc of 56.0\n",
      "Epoch 1476 with loss of 0.006553265540647912 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1477 with loss of 0.006520699542866519 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1478 with loss of 0.007069838614020578 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1479 with loss of 0.006131921350270904 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1480 with loss of 0.006339841044301955 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1481 with loss of 0.006248563691490115 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1482 with loss of 0.0068240280953222494 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1483 with loss of 0.006307962642145308 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1484 with loss of 0.006227792189747561 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1485 with loss of 0.006318894885715473 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1486 with loss of 0.006227868684688404 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Epoch 1487 with loss of 0.006481790359920034 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1488 with loss of 0.006082623059578159 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1489 with loss of 0.006621018779696897 training acc of 99.76923076923077 testing acc of 58.0\n",
      "Epoch 1490 with loss of 0.012007029256682681 training acc of 99.48351639967699 testing acc of 57.0\n",
      "Epoch 1491 with loss of 0.006299397416324399 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1492 with loss of 0.006593653248833457 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1493 with loss of 0.006457940919342666 training acc of 99.6923076923077 testing acc of 56.0\n",
      "Epoch 1494 with loss of 0.006070675998321699 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1495 with loss of 0.006547728367037892 training acc of 99.76923076923077 testing acc of 55.0\n",
      "Epoch 1496 with loss of 0.008149273462689823 training acc of 99.54945080096905 testing acc of 57.0\n",
      "Epoch 1497 with loss of 0.006590988747960252 training acc of 99.61538461538461 testing acc of 57.0\n",
      "Epoch 1498 with loss of 0.006166027077107663 training acc of 99.76923076923077 testing acc of 57.0\n",
      "Epoch 1499 with loss of 0.006146443769186869 training acc of 99.76923076923077 testing acc of 56.0\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_value = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    train_loss, train_acc = train_model(train_iter, epoch)\n",
    "    _, val_acc = eval_model(train_iter)\n",
    "    _, test_acc = eval_model(test_iter)\n",
    "    \n",
    "    loss_value.append(train_loss)\n",
    "    train_accuracy.append(train_acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print('Epoch', epoch, 'with loss of', train_loss, 'training acc of', train_acc, 'testing acc of', test_acc)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value:  0.006146443769186869\n",
      "Training accuracy:  99.76923076923077\n",
      "Testing accuracy:  56.0\n"
     ]
    }
   ],
   "source": [
    "print('Loss value: ', loss_value[-1])\n",
    "print('Training accuracy: ', train_accuracy[-1])\n",
    "print('Testing accuracy: ', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ffa2fb4acf8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_title('Learning Curve'.format('default'), color='C0')\n",
    "ax.set_ylabel('Cross Entrpy Loss')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.plot(list(i+1 for i in range(len(loss_value))), loss_value, 'b')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_title('Accuracy'.format('default'), color='C0')\n",
    "ax.set_ylabel('Accuracy Rate')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.plot(list(i+1 for i in range(len(train_accuracy))), train_accuracy, 'b', label='train')\n",
    "ax.plot(list(i+1 for i in range(len(test_accuracy))), test_accuracy, 'tab:orange', label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
